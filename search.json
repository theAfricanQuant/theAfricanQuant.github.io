[
  {
    "objectID": "projects/index.html",
    "href": "projects/index.html",
    "title": "My Projects",
    "section": "",
    "text": "üöß\nWelcome to my project page! Here you‚Äôll find an overview of some of the exciting projects I‚Äôve worked on and significantly shaped throughout their life cycles. These projects span algorithmic trading, financial data science, and machine learning, and they reflect my passion for innovation and collaboration.\n\n\nüßëüèΩ‚Äçüíª SisengAI Blog\nMy personal blog, where I write about algorithmic trading, machine learning, and data science topics using Quarto. It‚Äôs hosted on GitHub Pages.\nCheck out SisengAI Blog\n\n\n\nüåç Berom-English Corpus\nIn this project, I am developing a corpus of contemporary words and sentences for the Berom Language, focusing on preserving and expanding the use of this under-resourced language.\nExplore the Berom-English Corpus\n\n\n\nüìú Low-Resourced Machine Translation\nI co-authored this research paper on participatory research for low-resourced machine translation, with a case study on African languages. This study addresses the challenge of translating for languages with minimal resources.\nRead the Paper on ResearchGate\n\n\n\nüåç AfriMTE and AfriCOMET\nThis research explores empowering COMET, a machine translation evaluation framework, to embrace African languages, making machine translation more inclusive for under-resourced languages.\nLearn more about AfriMTE and AfriCOMET on Arxiv\n\n\n\nüéµ Spotify Dashboard\nAn interactive dashboard that curates hit songs on Spotify, giving a clear overview of trending music in various genres and regions.\nVisit the Spotify Dashboard\n\n\n\nüìä Donchian Channel Breakout Dashboard\nThis dashboard visualizes Donchian Breakouts on selected stocks. It uses the Alphavantage API to track stock performance and show meaningful insights for traders.\nCheck out the Donchian Channel Breakout Dashboard\n\n\nFeel free to reach out to me through my social handles for any inquiries about these projects or if you‚Äôre interested in collaborating!"
  },
  {
    "objectID": "blog/2024-June-19-Walrus-Operator/theWalrusOperator.html",
    "href": "blog/2024-June-19-Walrus-Operator/theWalrusOperator.html",
    "title": "Understanding The Assignment Expression aka Walrus Operator",
    "section": "",
    "text": "One of the world‚Äôs most widely-used programming languages is undoubtedly Python. Created by Guido van Rossum in 1991, Python‚Äôs inevitable growth necessitated a robust framework for governance to manage its expanding ecosystem.\nInitially, Python‚Äôs governance was straightforward. Guido van Rossum served as the Benevolent Dictator For Life (BDFL), a title that conferred both ultimate authority and the expectation of benevolence in decision-making. This model allowed for swift, unilateral decisions, essential during the nascent stages of Python‚Äôs development. However, as the language gained traction and its user base expanded, the limitations of a single-point decision-maker became evident.\nIn response to the growing complexity, the Python Enhancement Proposal (PEP) system was instituted. PEPs function as formalized proposals for changes to Python, analogous to legislative amendments in political systems. Each PEP undergoes a rigorous process of drafting, review, and acceptance. This system democratized the decision-making process, allowing for community input and expert evaluation.\nGuido van Rossum appointed BDFL Delegates who are entrusted with decision-making power in specific areas, thus decentralizing authority while maintaining overall coherence in the language‚Äôs evolution.\nOn October 14, 2019, Python 3.8 introduced the assignment expression or the walrus operator (:=) via PEP 572, allowing in-line assignment within expressions to enhance code efficiency and readability. Despite its benefits, the operator sparked significant controversy, highlighting the intrinsic tension between innovation and stability in programming language development. This debate led to Guido van Rossum‚Äôs resignation as BDFL, illustrating the personal toll of leadership and prompting a reevaluation of Python‚Äôs governance structure to create more sustainable, distributed decision-making models."
  },
  {
    "objectID": "blog/2024-June-19-Walrus-Operator/theWalrusOperator.html#introduction-the-evolution-of-decision-making-in-python",
    "href": "blog/2024-June-19-Walrus-Operator/theWalrusOperator.html#introduction-the-evolution-of-decision-making-in-python",
    "title": "Understanding The Assignment Expression aka Walrus Operator",
    "section": "",
    "text": "One of the world‚Äôs most widely-used programming languages is undoubtedly Python. Created by Guido van Rossum in 1991, Python‚Äôs inevitable growth necessitated a robust framework for governance to manage its expanding ecosystem.\nInitially, Python‚Äôs governance was straightforward. Guido van Rossum served as the Benevolent Dictator For Life (BDFL), a title that conferred both ultimate authority and the expectation of benevolence in decision-making. This model allowed for swift, unilateral decisions, essential during the nascent stages of Python‚Äôs development. However, as the language gained traction and its user base expanded, the limitations of a single-point decision-maker became evident.\nIn response to the growing complexity, the Python Enhancement Proposal (PEP) system was instituted. PEPs function as formalized proposals for changes to Python, analogous to legislative amendments in political systems. Each PEP undergoes a rigorous process of drafting, review, and acceptance. This system democratized the decision-making process, allowing for community input and expert evaluation.\nGuido van Rossum appointed BDFL Delegates who are entrusted with decision-making power in specific areas, thus decentralizing authority while maintaining overall coherence in the language‚Äôs evolution.\nOn October 14, 2019, Python 3.8 introduced the assignment expression or the walrus operator (:=) via PEP 572, allowing in-line assignment within expressions to enhance code efficiency and readability. Despite its benefits, the operator sparked significant controversy, highlighting the intrinsic tension between innovation and stability in programming language development. This debate led to Guido van Rossum‚Äôs resignation as BDFL, illustrating the personal toll of leadership and prompting a reevaluation of Python‚Äôs governance structure to create more sustainable, distributed decision-making models."
  },
  {
    "objectID": "blog/2024-June-19-Walrus-Operator/theWalrusOperator.html#understanding-the-walrus-operator",
    "href": "blog/2024-June-19-Walrus-Operator/theWalrusOperator.html#understanding-the-walrus-operator",
    "title": "Understanding The Assignment Expression aka Walrus Operator",
    "section": "Understanding the Walrus Operator",
    "text": "Understanding the Walrus Operator\nThe walrus operator is a syntactic innovation that allows for assignment expressions. This operator can streamline code by reducing redundancy and improving readability, particularly when dealing with computationally expensive operations. To elucidate its utility, let us examine various examples.\nImagine you have a value you want to use multiple times in a single line of code. Normally, you would have to do this in two steps:\nx = compute_value()  # Step 1: Assign the value to a variable\nif x &gt; 10:           # Step 2: Use the variable in an expression\n    print(x)\nIn the first line of code, we are performing an assignment operation. The function compute_value() is called, and it executes its defined task, which could involve computationally expensive calculations, data processing, or fetching information. The result of this function call is then stored in a variable named x.\n\nFunction Call: compute_value() is invoked, and it produces an output based on its implementation.\nAssignment: The = symbol assigns the output of compute_value() to the variable x.\n\nThis step is fundamental because it captures and stores the result of the function, making it available for future use within the program. By assigning this value to x, we ensure that we can reference and utilize the computed result in subsequent code.\nIn the second line, we evaluate a conditional expression and perform an action based on the result.\n\nThe if statement if x &gt; 10 : print(x) checks if the expression is true then prints the value of x\n\nNotice how any time either of the words assignment or expression is used, I italicized it. The assignment expression (or walrus operator because the symbol := looks like the teeth of a walrus lying down) performs the two operations of assignment and expresssion in one go. The Code above can be shortened to:\nif (x := compute_value()) &gt; 10: # Assign the value to a variable and use it in an expression\n    print(x)\n\nx := compute_value(): This part does two things at once. It calls the compute_value() function and assigns its result to the variable x.\nif (x := compute_value()) &gt; 10:: This is an if statement that checks if the result of compute_value() (which is now stored in x) is greater than 10. If it is, it executes the code inside the if block (in this case, print(x)).\n\nThis has made our code shorter by reducing the need to write the same expression twice and also kept related logic in one place, making the code hopefully easier to understand at a glance. in a nutshell, the walrus operator (:=) allows us to assign values within expressions, making our code shorter and often easier to read. It‚Äôs especially useful when we need to use the result of an expression multiple times in a single line of code.\n\nReducing Redundancy in Code\nConsider the scenario where we process sensor data and filter out invalid entries. Using traditional methods, the code appears verbose:\nvalid_data = []\nfor value in sensor_data:\n    result = compute_expensive_value(value)\n    if result is not None:\nWhenever you have a block of code that has an empty list sitting above a for loop, it usually implies that a list comprehension could be employed to do the exact same thing.\n\n  valid_data = [compute_expensive_value(value) for value in sensor_data \n  if compute_expensive_value(value) is not None]\n\n  # Compute statistics on valid_data\n  average = sum(valid_data) / len(valid_data)\nThis is a lot cleaner than the previous one; however, it is also quite inefficient. We can see that we are calling the computationally expensive function twice instead of once.\nWith the walrus operator, we can condense this logic into a more efficient list comprehension:\nvalid_data = [result for value in sensor_data if (result := compute_expensive_value(value))]\n\nExample: Handling Regular Expressions\nA common pattern involves checking for matches and then acting upon them:\nmatch = pattern.search(data)\nif match is not None:\n    # Do something with match\nThe walrus operator allows us to streamline this operation:\nif (match := pattern.search(data)) is not None:\n    # Do something with match\nThis approach reduces the scope of the match variable, enhancing code readability and maintainability."
  },
  {
    "objectID": "blog/2024-June-19-Walrus-Operator/theWalrusOperator.html#computational-efficiency-a-case-study",
    "href": "blog/2024-June-19-Walrus-Operator/theWalrusOperator.html#computational-efficiency-a-case-study",
    "title": "Understanding The Assignment Expression aka Walrus Operator",
    "section": "Computational Efficiency: A Case Study",
    "text": "Computational Efficiency: A Case Study\nLet us define a computationally expensive function called mega_transform that serves to illustrate the efficiency gains provided by the walrus operator:\n\nimport time\n\ndef mega_transform(x):\n    time.sleep(0.9)\n    sign = 1 if x % 2 == 0 else -1\n    return sign * x ** 2\n\nGiven a list of numbers, we can measure the time taken to filter and transform these numbers without using the walrus operator:\n\nnu_list = list(range(1, 20))\n\nstart = time.perf_counter()\nresult = [mega_transform(x) for x in nu_list if mega_transform(x) &gt; 0]\nend = time.perf_counter()\nprint(result)\nprint(f\"total time: {end - start:.2f} secs\")\n\n[4, 16, 36, 64, 100, 144, 196, 256, 324]\ntotal time: 25.35 secs\n\n\nWe repeat the process using the walrus operator to demonstrate its efficiency:\n\nstart = time.perf_counter()\nresult = [result for x in nu_list if (result := mega_transform(x)) &gt; 0]\nend = time.perf_counter()\nprint(result)\nprint(f\"total time: {end - start:.2f} secs\")\n\n[4, 16, 36, 64, 100, 144, 196, 256, 324]\ntotal time: 17.19 secs\n\n\nThe results clearly indicate that the walrus operator reduces the computational overhead by avoiding redundant function calls.\n\nTry this yourself\nYou can try this in your IDE like vscode, sublime or whatever you find fun:\nYou are to create a simple command-line interface in Python that allows you to execute shell commands until the user types ‚Äúexit‚Äù.\nimport subprocess\n\ncommand = input(\"$ \")\nwhile command != \"exit\":\n    subprocess.run(command, shell=True)\n    command = input(\"$ \")\n\n\n\nwithout the walrus operator\n\n\nimport subprocess\n\nwhile (command := input(\"$ \")) != \"exit\":\n    subprocess.run(command, shell=True)\n\n\n\nwith the walrus operator"
  },
  {
    "objectID": "blog/2024-June-19-Walrus-Operator/theWalrusOperator.html#conclusion",
    "href": "blog/2024-June-19-Walrus-Operator/theWalrusOperator.html#conclusion",
    "title": "Understanding The Assignment Expression aka Walrus Operator",
    "section": "Conclusion",
    "text": "Conclusion\nIn conclusion, the walrus operator is a powerful tool for optimizing Python code, particularly in scenarios involving computationally expensive operations. By allowing in-line assignments, it reduces redundancy, improves readability, and enhances overall efficiency. For developers dealing with complex data processing tasks, the walrus operator is a valuable addition to Python‚Äôs rich set of features.\nThese examples are but a few instances demonstrating the utility of the walrus operator. For further exploration and practical use-cases, the references below provide additional insights.\n\nReferences\n\nPEP 572 ‚Äì Assignment Expressions\nDustin Ingram‚Äôs Talk on the Walrus Operator"
  },
  {
    "objectID": "blog/2025-April-07-NamedTuples/tuples_namedtuples_dataclasses.html",
    "href": "blog/2025-April-07-NamedTuples/tuples_namedtuples_dataclasses.html",
    "title": "Python‚Äôs Data Structures: Tuples, namedTuples & DataClasses (Retired Legends Edition)",
    "section": "",
    "text": "Ever needed to store a fixed collection of related items, like coordinates (x, y) or basic info about a person (name, age)? How about details for legendary retired football players, many of whom famously wore the number 10 jersey? Think Pel√©, Zidane, Platini, and the skillful Jay-Jay Okocha! Python offers several ways to structure this data. We‚Äôll start with the basic tuple and see why sometimes we need more clarity. Then, we‚Äôll explore collections.namedtuple, its modern cousin typing.NamedTuple, and finally, the flexible dataclass, using these football icons as our examples! Let‚Äôs dive in and make sense of these useful structures!"
  },
  {
    "objectID": "blog/2025-April-07-NamedTuples/tuples_namedtuples_dataclasses.html#introduction",
    "href": "blog/2025-April-07-NamedTuples/tuples_namedtuples_dataclasses.html#introduction",
    "title": "Python‚Äôs Data Structures: Tuples, namedTuples & DataClasses (Retired Legends Edition)",
    "section": "",
    "text": "Ever needed to store a fixed collection of related items, like coordinates (x, y) or basic info about a person (name, age)? How about details for legendary retired football players, many of whom famously wore the number 10 jersey? Think Pel√©, Zidane, Platini, and the skillful Jay-Jay Okocha! Python offers several ways to structure this data. We‚Äôll start with the basic tuple and see why sometimes we need more clarity. Then, we‚Äôll explore collections.namedtuple, its modern cousin typing.NamedTuple, and finally, the flexible dataclass, using these football icons as our examples! Let‚Äôs dive in and make sense of these useful structures!"
  },
  {
    "objectID": "blog/2025-April-07-NamedTuples/tuples_namedtuples_dataclasses.html#whats-a-tuple",
    "href": "blog/2025-April-07-NamedTuples/tuples_namedtuples_dataclasses.html#whats-a-tuple",
    "title": "Python‚Äôs Data Structures: Tuples, namedTuples & DataClasses (Retired Legends Edition)",
    "section": "What‚Äôs a Tuple?",
    "text": "What‚Äôs a Tuple?\nThink of a tuple as a fixed, ordered list. Once you create it, you can‚Äôt change its contents ‚Äì it‚Äôs immutable. You define tuples using parentheses (). Let‚Äôs store some basic player info.\n\n# A tuple representing basic player info (Name, Year of Birth)\nplatini_basic = (\"Michel Platini\", 1955)\nprint(f\"Basic Platini tuple: {platini_basic}\")\n\n# A tuple representing more detailed player info (Name, YOB, Country, Active Years)\n# Note: Pel√©'s full name is Edson Arantes do Nascimento\npele_details = (\"Pel√©\", 1940, \"Brazil\", \"1956‚Äì1977\") \nprint(f\"Detailed Pel√© tuple: {pele_details}\")\n\nBasic Platini tuple: ('Michel Platini', 1955)\nDetailed Pel√© tuple: ('Pel√©', 1940, 'Brazil', '1956‚Äì1977')\n\n\nYou access items in a tuple using their position (index), starting from 0.\n\n# Continuing with the tuples from above\nplatini_basic = (\"Michel Platini\", 1955)\npele_details = (\"Pel√©\", 1940, \"Brazil\", \"1956‚Äì1977\")\n\n# Accessing elements by their index\nplatini_name = platini_basic[0]\nplatini_yob = platini_basic[1]\n\npele_name = pele_details[0]\npele_country = pele_details[2]\npele_active = pele_details[3]\n\n\nprint(f\"Platini: Name = {platini_name}, YOB = {platini_yob}\")\nprint(f\"Pel√©: Name = {pele_name}, Country = {pele_country}, Active = {pele_active}\")\n\nPlatini: Name = Michel Platini, YOB = 1955\nPel√©: Name = Pel√©, Country = Brazil, Active = 1956‚Äì1977\n\n\nThe Challenge with Simple Tuples:\nUsing indices like pele_details[2] works, but it isn‚Äôt very descriptive. What does index 2 represent? Country? Jersey number? Goals scored? This can get confusing, especially with tuples holding more items. We need a way to make accessing these elements more readable."
  },
  {
    "objectID": "blog/2025-April-07-NamedTuples/tuples_namedtuples_dataclasses.html#making-tuples-readable-collections.namedtuple",
    "href": "blog/2025-April-07-NamedTuples/tuples_namedtuples_dataclasses.html#making-tuples-readable-collections.namedtuple",
    "title": "Python‚Äôs Data Structures: Tuples, namedTuples & DataClasses (Retired Legends Edition)",
    "section": "Making Tuples Readable: collections.namedtuple",
    "text": "Making Tuples Readable: collections.namedtuple\nPython‚Äôs built-in collections module provides a fantastic solution: namedtuple. It lets you create tuple-like objects where you can access elements by a descriptive name as well as by their index! It acts like a factory that generates a new type of tuple tailored to our player data needs, featuring greats like Okocha and Pel√©.\n\nfrom collections import namedtuple\n\n# Create a 'blueprint' for a Player namedtuple\n# Fields: name, year_of_birth, country, years_active\nPlayer = namedtuple(\"Player\", \"name year_of_birth country years_active\")\n\n# Create instances (objects) of our Player namedtuple type\n# Data: Jay-Jay Okocha, Pel√©\nokocha = Player(\"Jay-Jay Okocha\", 1973, \"Nigeria\", \"1992‚Äì2008\")\npele = Player(name=\"Pel√©\", year_of_birth=1940, country=\"Brazil\", years_active=\"1956‚Äì1977\") # Keyword args work too\n\nprint(\"--- Player Instances ---\")\nprint(okocha)\nprint(pele)\n\n# Access by name (much clearer!)\nprint(\"\\n--- Access by Name ---\")\nprint(f\"Okocha's Country: {okocha.country}\")\nprint(f\"Pel√©'s Active Years: {pele.years_active}\")\n\n# Access by index (still works!)\nprint(\"\\n--- Access by Index ---\")\nprint(f\"Okocha's Name (Index 0): {okocha[0]}\")\nprint(f\"Pel√©'s YOB (Index 1): {pele[1]}\")\n\n--- Player Instances ---\nPlayer(name='Jay-Jay Okocha', year_of_birth=1973, country='Nigeria', years_active='1992‚Äì2008')\nPlayer(name='Pel√©', year_of_birth=1940, country='Brazil', years_active='1956‚Äì1977')\n\n--- Access by Name ---\nOkocha's Country: Nigeria\nPel√©'s Active Years: 1956‚Äì1977\n\n--- Access by Index ---\nOkocha's Name (Index 0): Jay-Jay Okocha\nPel√©'s YOB (Index 1): 1940\n\n\nSee how okocha.country and pele.years_active are much clearer than okocha[2] and pele[3]? This significantly improves code readability.\nnamedtuple also comes with some handy built-in helper methods:\n\nfrom collections import namedtuple\n\nPlayer = namedtuple(\"Player\", \"name year_of_birth country years_active\")\nokocha = Player(\"Jay-Jay Okocha\", 1973, \"Nigeria\", \"1992‚Äì2008\")\n\n# _asdict(): Get fields as an ordered dictionary\nprint(f\"Okocha as Dictionary: {okocha._asdict()}\")\n\n# _fields: Get the field names as a tuple\nprint(f\"Player Fields: {Player._fields}\") # Access fields via the type\n\n# _make(): Create a new instance from an iterable (like a list or tuple)\n# Data: Zinedine Zidane\nzidane_data = [\"Zinedine Zidane\", 1972, \"France\", \"1988‚Äì2006\"]\nzidane = Player._make(zidane_data)\nprint(f\"\\nMade from list (Zidane): {zidane}\")\n\n# _replace(): Create a *new* tuple with some fields replaced\n# Remember, tuples are immutable! This doesn't change okocha.\n# Let's imagine updating Okocha's active years hypothetically\nokocha_updated = okocha._replace(years_active=\"1992-2010\") # Creates a new instance\nprint(f\"\\nReplaced years (hypothetical): {okocha_updated}\")\nprint(f\"Original Okocha unchanged: {okocha}\")\n\nOkocha as Dictionary: {'name': 'Jay-Jay Okocha', 'year_of_birth': 1973, 'country': 'Nigeria', 'years_active': '1992‚Äì2008'}\nPlayer Fields: ('name', 'year_of_birth', 'country', 'years_active')\n\nMade from list (Zidane): Player(name='Zinedine Zidane', year_of_birth=1972, country='France', years_active='1988‚Äì2006')\n\nReplaced years (hypothetical): Player(name='Jay-Jay Okocha', year_of_birth=1973, country='Nigeria', years_active='1992-2010')\nOriginal Okocha unchanged: Player(name='Jay-Jay Okocha', year_of_birth=1973, country='Nigeria', years_active='1992‚Äì2008')"
  },
  {
    "objectID": "blog/2025-April-07-NamedTuples/tuples_namedtuples_dataclasses.html#the-modern-way-typing.namedtuple",
    "href": "blog/2025-April-07-NamedTuples/tuples_namedtuples_dataclasses.html#the-modern-way-typing.namedtuple",
    "title": "Python‚Äôs Data Structures: Tuples, namedTuples & DataClasses (Retired Legends Edition)",
    "section": "The Modern Way: typing.NamedTuple",
    "text": "The Modern Way: typing.NamedTuple\nWith the introduction of type hints in Python 3, a more modern way to create named tuples emerged: typing.NamedTuple. It integrates seamlessly with type checking tools.\nThere are two main ways to define a typing.NamedTuple:\n1. Functional Syntax: Similar to collections.namedtuple, but includes type information directly in the field definition.\n\nfrom typing import NamedTuple\n\n# Functional syntax: Define type name and a list of (field_name, type) tuples\nPlayerInfo = NamedTuple(\"PlayerInfo\", [\n    (\"name\", str),\n    (\"year_of_birth\", int),\n    (\"country\", str),\n    (\"years_active\", str)\n])\n\n# Create an instance using this type\nokocha_info = PlayerInfo(\"Jay-Jay Okocha\", 1973, \"Nigeria\", \"1992-2008\")\nzidane_info = PlayerInfo(name=\"Zinedine Zidane\", year_of_birth=1972, country=\"France\", years_active=\"1988‚Äì2006\")\n\nprint(\"--- Player Instances (typing.NamedTuple - Functional) ---\")\nprint(okocha_info)\nprint(zidane_info)\n\nprint(\"\\n--- Access by Name ---\")\nprint(f\"Okocha's Country: {okocha_info.country}\")\nprint(f\"Zidane's YOB: {zidane_info.year_of_birth}\")\n\nprint(\"\\n--- Access by Index ---\")\nprint(f\"Okocha's Name (Index 0): {okocha_info[0]}\")\n\n--- Player Instances (typing.NamedTuple - Functional) ---\nPlayerInfo(name='Jay-Jay Okocha', year_of_birth=1973, country='Nigeria', years_active='1992-2008')\nPlayerInfo(name='Zinedine Zidane', year_of_birth=1972, country='France', years_active='1988‚Äì2006')\n\n--- Access by Name ---\nOkocha's Country: Nigeria\nZidane's YOB: 1972\n\n--- Access by Index ---\nOkocha's Name (Index 0): Jay-Jay Okocha\n\n\n2. Class Syntax: This uses standard class definition syntax, which many find more intuitive and familiar, especially when coming from object-oriented programming.\n\nfrom typing import NamedTuple\nimport datetime # To calculate age\n\n# Define a Player using NamedTuple and type hints\n# This looks like defining a class!\nclass Player(NamedTuple):\n    name: str\n    year_of_birth: int\n    country: str\n    years_active: str\n    # NOTE: We are keeping this definition simple for clarity.\n    # You *can* add methods here if needed, but the core idea\n    # is an immutable, typed structure.\n\n# Create instances - works just like calling a class constructor\n# Data: Michel Platini, Zinedine Zidane, Jay-Jay Okocha\nplatini = Player(\"Michel Platini\", 1955, \"France\", \"1972‚Äì1987\")\nzidane = Player(name=\"Zinedine Zidane\", year_of_birth=1972, country=\"France\", years_active=\"1988‚Äì2006\")\nokocha = Player(name=\"Jay-Jay Okocha\", year_of_birth=1973, country=\"Nigeria\", years_active=\"1992‚Äì2008\")\n\nprint(\"--- Player Instances (typing.NamedTuple - Class Syntax) ---\")\nprint(platini)\nprint(zidane)\nprint(okocha)\n\n\nprint(\"\\n--- Access by Name ---\")\nprint(f\"Platini's YOB: {platini.year_of_birth}\")\nprint(f\"Zidane's Country: {zidane.country}\")\nprint(f\"Okocha's Active Years: {okocha.years_active}\")\n\n\n# Access by index still works too!\nprint(f\"\\n--- Access by Index ---\")\nprint(f\"Platini's Name (Index 0): {platini[0]}\")\nprint(f\"Zidane's Active Years (Index 3): {zidane[3]}\")\nprint(f\"Okocha's YOB (Index 1): {okocha[1]}\")\n\n\n# Still immutable! This would raise an AttributeError:\n# try:\n#    zidane.country = \"Algeria\" # Can't change fields\n# except AttributeError as e:\n#    print(f\"\\nError trying to modify: {e}\")\n\n--- Player Instances (typing.NamedTuple - Class Syntax) ---\nPlayer(name='Michel Platini', year_of_birth=1955, country='France', years_active='1972‚Äì1987')\nPlayer(name='Zinedine Zidane', year_of_birth=1972, country='France', years_active='1988‚Äì2006')\nPlayer(name='Jay-Jay Okocha', year_of_birth=1973, country='Nigeria', years_active='1992‚Äì2008')\n\n--- Access by Name ---\nPlatini's YOB: 1955\nZidane's Country: France\nOkocha's Active Years: 1992‚Äì2008\n\n--- Access by Index ---\nPlatini's Name (Index 0): Michel Platini\nZidane's Active Years (Index 3): 1988‚Äì2006\nOkocha's YOB (Index 1): 1973\n\n\nNotice how the class syntax looks much more like defining a regular Python class. It provides the same core benefits as collections.namedtuple (access by name, immutability, tuple behavior) but with the added advantages of:\n\nClear Type Annotations: Improves code understanding and helps catch errors.\nStandard Class Syntax: More familiar for object-oriented programming."
  },
  {
    "objectID": "blog/2025-April-07-NamedTuples/tuples_namedtuples_dataclasses.html#namedtuple-is-almost-a-class",
    "href": "blog/2025-April-07-NamedTuples/tuples_namedtuples_dataclasses.html#namedtuple-is-almost-a-class",
    "title": "Python‚Äôs Data Structures: Tuples, namedTuples & DataClasses (Retired Legends Edition)",
    "section": "NamedTuple is (almost) a Class",
    "text": "NamedTuple is (almost) a Class\nIt‚Äôs important to understand that both collections.namedtuple and typing.NamedTuple create actual Python classes for you behind the scenes. These generated classes inherit directly from the base tuple type. That‚Äôs why they behave like tuples (immutable, ordered, unpackable, indexed) but also gain named fields. typing.NamedTuple just makes this class-based nature much more explicit, especially with the class syntax."
  },
  {
    "objectID": "blog/2025-April-07-NamedTuples/tuples_namedtuples_dataclasses.html#what-about-dataclasses",
    "href": "blog/2025-April-07-NamedTuples/tuples_namedtuples_dataclasses.html#what-about-dataclasses",
    "title": "Python‚Äôs Data Structures: Tuples, namedTuples & DataClasses (Retired Legends Edition)",
    "section": "What About dataclasses?",
    "text": "What About dataclasses?\nPython 3.7 introduced another powerful tool for creating classes that primarily hold data: the @dataclass decorator from the dataclasses module. It automatically generates special methods like __init__, __repr__, __eq__, etc., saving you boilerplate code. Let‚Äôs model our retired legends using a dataclass.\n\nfrom dataclasses import dataclass, field # field might be needed for complex defaults\n\n@dataclass\nclass PlayerData:\n    name: str\n    year_of_birth: int\n    country: str\n    years_active: str\n    # Example: Add an optional field with a simple default\n    status: str = \"Retired Legend\" \n\n    # You can add methods here too\n    def get_info_string(self) -&gt; str:\n        return f\"{self.name} ({self.country}), Born: {self.year_of_birth}, Active: {self.years_active}, Status: {self.status}\"\n\n\n# Create instances - the __init__ is generated for us!\n# Data: Pel√©, Jay-Jay Okocha\npele_dc = PlayerData(\"Pel√©\", 1940, \"Brazil\", \"1956‚Äì1977\") \nokocha_dc = PlayerData(\"Jay-Jay Okocha\", 1973, \"Nigeria\", \"1992‚Äì2008\")\n\n\nprint(\"--- Player Instances (dataclass) ---\")\nprint(pele_dc) # Nice __repr__ is generated!\nprint(okocha_dc)\n\n# Dataclasses are MUTABLE by default\nprint(\"\\n--- Mutability ---\")\n# Let's hypothetically update Pel√©'s status if new info came to light\npele_dc.status = \"Global Ambassador\"\nprint(f\"Modified Pel√© data: {pele_dc}\")\n\n# Comparison works out of the box (__eq__ is generated)\npele_dc_copy = PlayerData(\"Pel√©\", 1940, \"Brazil\", \"1956‚Äì1977\", status=\"Global Ambassador\")\nokocha_dc_copy = PlayerData(\"Jay-Jay Okocha\", 1973, \"Nigeria\", \"1992‚Äì2008\")\n\nprint(f\"\\nIs pele_dc equal to its copy? {pele_dc == pele_dc_copy}\") # Compares field values\nprint(f\"Is okocha_dc equal to its copy? {okocha_dc == okocha_dc_copy}\")\nprint(f\"Is pele_dc equal to okocha_dc? {pele_dc == okocha_dc}\")\n\n# Use the custom method\nprint(\"\\n--- Custom Method ---\")\nprint(okocha_dc.get_info_string())\n\n--- Player Instances (dataclass) ---\nPlayerData(name='Pel√©', year_of_birth=1940, country='Brazil', years_active='1956‚Äì1977', status='Retired Legend')\nPlayerData(name='Jay-Jay Okocha', year_of_birth=1973, country='Nigeria', years_active='1992‚Äì2008', status='Retired Legend')\n\n--- Mutability ---\nModified Pel√© data: PlayerData(name='Pel√©', year_of_birth=1940, country='Brazil', years_active='1956‚Äì1977', status='Global Ambassador')\n\nIs pele_dc equal to its copy? True\nIs okocha_dc equal to its copy? True\nIs pele_dc equal to okocha_dc? False\n\n--- Custom Method ---\nJay-Jay Okocha (Nigeria), Born: 1973, Active: 1992‚Äì2008, Status: Retired Legend"
  },
  {
    "objectID": "blog/2025-April-07-NamedTuples/tuples_namedtuples_dataclasses.html#typing.namedtuple-vs.-dataclasses.dataclass-when-to-choose",
    "href": "blog/2025-April-07-NamedTuples/tuples_namedtuples_dataclasses.html#typing.namedtuple-vs.-dataclasses.dataclass-when-to-choose",
    "title": "Python‚Äôs Data Structures: Tuples, namedTuples & DataClasses (Retired Legends Edition)",
    "section": "typing.NamedTuple vs.¬†dataclasses.dataclass: When to Choose?",
    "text": "typing.NamedTuple vs.¬†dataclasses.dataclass: When to Choose?\nBoth typing.NamedTuple and dataclasses.dataclass are excellent for structured data, but they differ crucially:\n\nMutability:\n\nNamedTuple: Immutable. Cannot change values after creation. Create a new instance for changes (using _replace or creating anew).\nDataClass: Mutable by default. Can change values (pele_dc.status = ...). Use @dataclass(frozen=True) to make it immutable, mimicking NamedTuple behavior.\n\nInheritance & Type:\n\nNamedTuple: Inherits from tuple. Is a tuple.\nDataClass: Inherits from object (by default). A regular class instance, not a tuple.\n\nMemory & Performance:\n\nNamedTuple can be slightly more memory-efficient (no __dict__ usually needed).\nPerformance differences are often negligible in practice. Choose based on required features (mutability, tuple behavior) first.\n\nFlexibility & Features:\n\nDataClass: More flexible, more configuration options (frozen, order, __post_init__), easier handling of mutable defaults (field(default_factory=...)), feels like a standard class, easier to add complex methods.\nNamedTuple: Simpler, focused on immutable, tuple-like records with named fields and types.\n\n\nWhen to Use Which (Retired Player Example Context):\n\nChoose typing.NamedTuple for Players like Okocha, Pel√©, Zidane when:\n\nYou want to ensure player records (like historical stats, birth year, active years) cannot be accidentally changed after creation. This represents fixed historical data well.\nYou need to use the player record exactly like a tuple (e.g., as a dictionary key, unpacking into variables).\nYou‚Äôre returning fixed player info from a function and want named fields plus type safety for clarity without the overhead or mutability of a full class.\n\nChoose dataclasses.dataclass for Players when:\n\nYou need to modify player data after creation (e.g., updating status, adding recent accolades, potentially tracking temporary states).\nYou want the option of immutability (frozen=True) but prefer the general structure and features of dataclasses (like easier method addition, __post_init__).\nYou plan to add more complex methods related to player actions or state changes that go beyond simple data representation.\nYou prefer the standard Python class feel and don‚Äôt specifically need tuple-like behavior."
  },
  {
    "objectID": "blog/2025-April-07-NamedTuples/tuples_namedtuples_dataclasses.html#conclusion",
    "href": "blog/2025-April-07-NamedTuples/tuples_namedtuples_dataclasses.html#conclusion",
    "title": "Python‚Äôs Data Structures: Tuples, namedTuples & DataClasses (Retired Legends Edition)",
    "section": "Conclusion",
    "text": "Conclusion\nWe‚Äôve explored how to represent structured data in Python, moving from basic tuples to more descriptive and powerful options using retired football legends like Pel√©, Zidane, Platini, and the exceptionally skillful Jay-Jay Okocha (many of whom wore the iconic number 10) as our examples:\n\ntuple: Simple, immutable, indexed sequence. Good for very basic, fixed data where names aren‚Äôt crucial.\ncollections.namedtuple: The classic way to add names to tuple fields for readability, creating simple tuple subclasses.\ntyping.NamedTuple: The modern, type-hinted way (using functional or class syntax) for creating immutable named tuple classes. Excellent for fixed records.\ndataclasses.dataclass: A flexible decorator for creating data-centric classes (mutable by default, but can be frozen) with less boilerplate code. Great for general-purpose data holding.\n\nUnderstanding the immutability of tuples and NamedTuple versus the default mutability of dataclass is key. Choose NamedTuple for fixed, unchanging records like historical player details or trading strategy backtest results that should remain tamper-proof. Opt for dataclass when you need a more general-purpose container, perhaps for data that might evolve or require more complex class behaviors. Both help write cleaner, more understandable Python code! Happy coding!"
  },
  {
    "objectID": "blog/2024-September-14-Unpacking-Operator/unpacking_operator_in_python.html",
    "href": "blog/2024-September-14-Unpacking-Operator/unpacking_operator_in_python.html",
    "title": "Grokking the Unpacking Operator",
    "section": "",
    "text": "Imagine for a moment that it is your spouse‚Äôs birthday and you planned to buy them a gift - a box with some selected items: jewelry, a watch, perfume, and airpods. You gleefully look at her/him smile as s/he unboxes it, carefully revealing each item one by one.\nSimilarly, in Python, unpacking allows us to unbox the elements of an iterable such as a list, tuple, or dictionary into individual variables.\nIn this article, we will get into the weeds of how python unboxes items, properly known as unpacking.\n\n\nLet‚Äôs start simple. Unpacking is the process of extracting individual elements from a collection like a list or a tuple and assigning them to variables. Think of it like unboxing your wife‚Äôs birthday gift. Here‚Äôs how we do that in Python:\n\ngift_box = ['jewelry', 'watch', 'perfume', 'airpods']\nitem1, item2, item3, item4 = gift_box\n\nprint(item1)  # 'jewelry'\nprint(item2)  # 'watch'\nprint(item3)  # 'perfume'\nprint(item4)  # 'airpods'\n\njewelry\nwatch\nperfume\nairpods\n\n\nHere, we ‚Äúunbox‚Äù the list by assigning each item to a separate variable. Now, each variable contains one of the items from the gift_box.\n\n\nWhat happens if the gift box has more items than variables?\n\ngift_box = ['jewelry', 'watch', 'perfume', 'airpods', 'flowers']\nitem1, item2, item3 = gift_box  # Error: too many values to unpack\n\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nCell In[2], line 2\n      1 gift_box = ['jewelry', 'watch', 'perfume', 'airpods', 'flowers']\n----&gt; 2 item1, item2, item3 = gift_box  # Error: too many values to unpack\n\nValueError: too many values to unpack (expected 3)\n\n\n\nWe get an error! Python doesn‚Äôt know how to fit five items into just three variables. Likewise, if we had too few items, we‚Äôd also get an error:\n\ngift_box = ['jewelry', 'watch']\nitem1, item2, item3 = gift_box  # Error: not enough values to unpack\n\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nCell In[3], line 2\n      1 gift_box = ['jewelry', 'watch']\n----&gt; 2 item1, item2, item3 = gift_box  # Error: not enough values to unpack\n\nValueError: not enough values to unpack (expected 3, got 2)\n\n\n\n\n\n\n\nNow, imagine you want to only unpack the first and last items from the gift box, while ignoring the rest. This is where the * operator comes in handy:\n\ngift_box = ['jewelry', 'watch', 'perfume', 'airpods', 'flowers']\nfirst, *middle, last = gift_box\n\nprint(first)  # 'jewelry'\nprint(last)   # 'flowers'\nprint(middle)  # ['watch', 'perfume', 'airpods']\n\njewelry\nflowers\n['watch', 'perfume', 'airpods']\n\n\nThe * operator captures the remaining elements and packs them into a list. You can even discard them by using _:\n\nfirst, *_, last = gift_box\nprint(first, last)  # 'jewelry', 'flowers'\n\njewelry flowers\n\n\nThis method is incredibly useful when you only care about a few elements in a list or tuple.\n\n\n\nNow let‚Äôs move on to unpacking dictionaries. Imagine the gift contains a dictionary with item names and their respective values.\n\ngift_details = {'jewelry': 'gold', 'watch': 'rolex', 'perfume': 'Chanel'}\n\nYou can unpack dictionaries using **. This is especially useful when you want to merge dictionaries, like adding more gifts in your spouse‚Äôs box of gifts:\n\nextra_gift = {'airpods': 'pro'}\nmerged_gifts = {**gift_details, **extra_gift}\nprint(merged_gifts)\n# {'jewelry': 'gold', 'watch': 'rolex', 'perfume': 'Chanel', 'airpods': 'pro'}\n\n{'jewelry': 'gold', 'watch': 'rolex', 'perfume': 'Chanel', 'airpods': 'pro'}\n\n\nThe ** operator spreads out the dictionary and allows you to combine multiple dictionaries into one.\n\n\n\nYou might have seen the terms *args and **kwargs before in Python. These are used when defining functions to accept a variable number of arguments and keyword arguments.\n\n*args lets you pass a variable number of positional arguments to a function.\n**kwargs lets you pass a variable number of keyword arguments.\n\nLet‚Äôs start with *args:\n\ndef calculate_total(*args):\n    return sum(args)\n\nprint(calculate_total(10, 20, 30))  # 60\nprint(calculate_total(5, 15))  # 20\n\n60\n20\n\n\nHere, *args captures all the positional arguments into a tuple. You can pass as many arguments as you like.\nNext, let‚Äôs look at **kwargs:\n\ndef print_gift_details(**kwargs):\n    for key, value in kwargs.items():\n        print(f\"{key}: {value}\")\n\nprint_gift_details(jewelry=\"gold\", watch=\"rolex\", perfume=\"Chanel\")\n# jewelry: gold\n# watch: rolex\n# perfume: Chanel\n\njewelry: gold\nwatch: rolex\nperfume: Chanel\n\n\nWith **kwargs, all keyword arguments are captured into a dictionary, which you can iterate over.\n\n\nYou can combine both in a function to accept any number of positional and keyword arguments:\n\ndef gift_summary(*args, **kwargs):\n    print(\"Items:\", args)\n    print(\"Details:\", kwargs)\n\ngift_summary('jewelry', 'watch', jewelry=\"gold\", watch=\"rolex\")\n# Items: ('jewelry', 'watch')\n# Details: {'jewelry': 'gold', 'watch': 'rolex'}\n\nItems: ('jewelry', 'watch')\nDetails: {'jewelry': 'gold', 'watch': 'rolex'}\n\n\n\n\n\n\nMethod chaining in pandas is a technique where multiple methods are called sequentially on a DataFrame or Series in a single statement. Each method operates on the output of the previous one, making the code concise and readable. It helps avoid intermediate variables and enables efficient data processing.\nlet us create a dataframe from the numpy library:\n\nimport pandas as pd\nimport numpy as np \n# Setting up random number generator\nrng = np.random.default_rng(seed=42)\n\n# Generating random data for open, high, low, close prices\ndata = {\n    'Open': rng.uniform(low=100, high=200, size=10),\n    'High': rng.uniform(low=200, high=300, size=10),\n    'Low': rng.uniform(low=50, high=100, size=10),\n    'Close': rng.uniform(low=100, high=200, size=10)\n}\n\n# Creating the DataFrame\ndf = pd.DataFrame(data).round(4)\ndf1=df2=df.copy() # making copies to use for examples later. \ndf\n\n\n\n\n\n\n\n\nOpen\nHigh\nLow\nClose\n\n\n\n\n0\n177.3956\n237.0798\n87.9044\n174.4762\n\n\n1\n143.8878\n292.6765\n67.7263\n196.7510\n\n\n2\n185.8598\n264.3865\n98.5349\n132.5825\n\n\n3\n169.7368\n282.2762\n94.6561\n137.0460\n\n\n4\n109.4177\n244.3414\n88.9192\n146.9556\n\n\n5\n197.5622\n222.7239\n59.7319\n118.9471\n\n\n6\n176.1140\n255.4585\n73.3361\n112.9922\n\n\n7\n178.6064\n206.3817\n52.1902\n147.5705\n\n\n8\n112.8114\n282.7631\n57.7145\n122.6909\n\n\n9\n145.0386\n263.1664\n84.1524\n166.9814\n\n\n\n\n\n\n\nTo add a new feature like calculating the moving average of the Close price in a DataFrame, you can use the rolling() function in pandas. Here‚Äôs how to calculate the moving average (for example, a 3-period moving average) of the Close price and add it as a new column to the DataFrame.\n\ndf1['SMA'] = df1['Close'].rolling(window=3).mean()\ndf1\n\n\n\n\n\n\n\n\nOpen\nHigh\nLow\nClose\nSMA\n\n\n\n\n0\n177.3956\n237.0798\n87.9044\n174.4762\nNaN\n\n\n1\n143.8878\n292.6765\n67.7263\n196.7510\nNaN\n\n\n2\n185.8598\n264.3865\n98.5349\n132.5825\n167.936567\n\n\n3\n169.7368\n282.2762\n94.6561\n137.0460\n155.459833\n\n\n4\n109.4177\n244.3414\n88.9192\n146.9556\n138.861367\n\n\n5\n197.5622\n222.7239\n59.7319\n118.9471\n134.316233\n\n\n6\n176.1140\n255.4585\n73.3361\n112.9922\n126.298300\n\n\n7\n178.6064\n206.3817\n52.1902\n147.5705\n126.503267\n\n\n8\n112.8114\n282.7631\n57.7145\n122.6909\n127.751200\n\n\n9\n145.0386\n263.1664\n84.1524\n166.9814\n145.747600\n\n\n\n\n\n\n\nTo achieve the same using method chaining:\n\ndf2 = (df2\n.assign(SMA=lambda x: x['Close'].rolling(window=3)\n.mean())\n)\ndf2\n\n\n\n\n\n\n\n\nOpen\nHigh\nLow\nClose\nSMA\n\n\n\n\n0\n177.3956\n237.0798\n87.9044\n174.4762\nNaN\n\n\n1\n143.8878\n292.6765\n67.7263\n196.7510\nNaN\n\n\n2\n185.8598\n264.3865\n98.5349\n132.5825\n167.936567\n\n\n3\n169.7368\n282.2762\n94.6561\n137.0460\n155.459833\n\n\n4\n109.4177\n244.3414\n88.9192\n146.9556\n138.861367\n\n\n5\n197.5622\n222.7239\n59.7319\n118.9471\n134.316233\n\n\n6\n176.1140\n255.4585\n73.3361\n112.9922\n126.298300\n\n\n7\n178.6064\n206.3817\n52.1902\n147.5705\n126.503267\n\n\n8\n112.8114\n282.7631\n57.7145\n122.6909\n127.751200\n\n\n9\n145.0386\n263.1664\n84.1524\n166.9814\n145.747600\n\n\n\n\n\n\n\nThis seems to be working just fine. However, sometimes we might want to create a function that could insert multiple features at once based off of the users‚Äô choice. An example is given below:\n\ndef add_multiple_smas(df, col, *windows):\n    for window in windows:\n      df[f'SMA_{window}'] = df[col].rolling(window=window).mean()\n    return df\n\n# Example usage\nadd_multiple_smas(df1, 'Close', 2, 3, 4)\n\n\n\n\n\n\n\n\nOpen\nHigh\nLow\nClose\nSMA\nSMA_2\nSMA_3\nSMA_4\n\n\n\n\n0\n177.3956\n237.0798\n87.9044\n174.4762\nNaN\nNaN\nNaN\nNaN\n\n\n1\n143.8878\n292.6765\n67.7263\n196.7510\nNaN\n185.61360\nNaN\nNaN\n\n\n2\n185.8598\n264.3865\n98.5349\n132.5825\n167.936567\n164.66675\n167.936567\nNaN\n\n\n3\n169.7368\n282.2762\n94.6561\n137.0460\n155.459833\n134.81425\n155.459833\n160.213925\n\n\n4\n109.4177\n244.3414\n88.9192\n146.9556\n138.861367\n142.00080\n138.861367\n153.333775\n\n\n5\n197.5622\n222.7239\n59.7319\n118.9471\n134.316233\n132.95135\n134.316233\n133.882800\n\n\n6\n176.1140\n255.4585\n73.3361\n112.9922\n126.298300\n115.96965\n126.298300\n128.985225\n\n\n7\n178.6064\n206.3817\n52.1902\n147.5705\n126.503267\n130.28135\n126.503267\n131.616350\n\n\n8\n112.8114\n282.7631\n57.7145\n122.6909\n127.751200\n135.13070\n127.751200\n125.550175\n\n\n9\n145.0386\n263.1664\n84.1524\n166.9814\n145.747600\n144.83615\n145.747600\n137.558750\n\n\n\n\n\n\n\nAttempting to accomplish a similar feat using method chaining will result in an error because the .assign method expects keyword arguments. The best way is to use the unpacking operator to achieve this with the help of dictionary comprehension to loop through the different values for the SMAs.\n\ndef add_multiple_smas(df, col, *windows, **kwargs):\n    \n    # Use assign to add the columns in a chained fashion\n    return df.assign(**{f'SMA_{window}': df[col].rolling(window=window, **kwargs).mean() for window in windows})\n\n# Example usage with method chaining\nadd_multiple_smas(df2, 'Close', 2, 3, 4)\n\n\n\n\n\n\n\n\nOpen\nHigh\nLow\nClose\nSMA\nSMA_2\nSMA_3\nSMA_4\n\n\n\n\n0\n177.3956\n237.0798\n87.9044\n174.4762\nNaN\nNaN\nNaN\nNaN\n\n\n1\n143.8878\n292.6765\n67.7263\n196.7510\nNaN\n185.61360\nNaN\nNaN\n\n\n2\n185.8598\n264.3865\n98.5349\n132.5825\n167.936567\n164.66675\n167.936567\nNaN\n\n\n3\n169.7368\n282.2762\n94.6561\n137.0460\n155.459833\n134.81425\n155.459833\n160.213925\n\n\n4\n109.4177\n244.3414\n88.9192\n146.9556\n138.861367\n142.00080\n138.861367\n153.333775\n\n\n5\n197.5622\n222.7239\n59.7319\n118.9471\n134.316233\n132.95135\n134.316233\n133.882800\n\n\n6\n176.1140\n255.4585\n73.3361\n112.9922\n126.298300\n115.96965\n126.298300\n128.985225\n\n\n7\n178.6064\n206.3817\n52.1902\n147.5705\n126.503267\n130.28135\n126.503267\n131.616350\n\n\n8\n112.8114\n282.7631\n57.7145\n122.6909\n127.751200\n135.13070\n127.751200\n125.550175\n\n\n9\n145.0386\n263.1664\n84.1524\n166.9814\n145.747600\n144.83615\n145.747600\n137.558750\n\n\n\n\n\n\n\nMethod chaining in pandas allows for more concise, readable, and functional-style code by performing multiple transformations in a single statement without creating intermediate variables. This makes the code more compact and often easier to follow when handling complex data transformations. It can also help avoid side effects by keeping transformations within the same flow, making it easier to debug and maintain.\nHowever, it‚Äôs also a matter of preference. Some developers prefer method chaining for its elegance and simplicity, while others prefer using intermediate variables for clarity, especially when dealing with more complex logic, as it can be easier to inspect the data at different stages of transformation. The choice depends on the coding style that the person or team finds most understandable and maintainable.\n\n\n\nUnpacking is a powerful feature in Python that helps you write cleaner and more efficient code. Whether you‚Äôre unpacking lists, tuples, or dictionaries, or using *args and **kwargs in functions, this feature allows for flexible and dynamic code. Moreover, unpacking can be used with popular libraries like Pandas and NumPy to streamline data manipulation.\nSo, the next time you find yourself opening a gift box, remember: Python unpacking is just like unboxing‚Äîtaking out each item, one by one, and making it yours!"
  },
  {
    "objectID": "blog/2024-September-14-Unpacking-Operator/unpacking_operator_in_python.html#the-basics-of-unpacking",
    "href": "blog/2024-September-14-Unpacking-Operator/unpacking_operator_in_python.html#the-basics-of-unpacking",
    "title": "Grokking the Unpacking Operator",
    "section": "",
    "text": "Let‚Äôs start simple. Unpacking is the process of extracting individual elements from a collection like a list or a tuple and assigning them to variables. Think of it like unboxing your wife‚Äôs birthday gift. Here‚Äôs how we do that in Python:\n\ngift_box = ['jewelry', 'watch', 'perfume', 'airpods']\nitem1, item2, item3, item4 = gift_box\n\nprint(item1)  # 'jewelry'\nprint(item2)  # 'watch'\nprint(item3)  # 'perfume'\nprint(item4)  # 'airpods'\n\njewelry\nwatch\nperfume\nairpods\n\n\nHere, we ‚Äúunbox‚Äù the list by assigning each item to a separate variable. Now, each variable contains one of the items from the gift_box.\n\n\nWhat happens if the gift box has more items than variables?\n\ngift_box = ['jewelry', 'watch', 'perfume', 'airpods', 'flowers']\nitem1, item2, item3 = gift_box  # Error: too many values to unpack\n\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nCell In[2], line 2\n      1 gift_box = ['jewelry', 'watch', 'perfume', 'airpods', 'flowers']\n----&gt; 2 item1, item2, item3 = gift_box  # Error: too many values to unpack\n\nValueError: too many values to unpack (expected 3)\n\n\n\nWe get an error! Python doesn‚Äôt know how to fit five items into just three variables. Likewise, if we had too few items, we‚Äôd also get an error:\n\ngift_box = ['jewelry', 'watch']\nitem1, item2, item3 = gift_box  # Error: not enough values to unpack\n\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nCell In[3], line 2\n      1 gift_box = ['jewelry', 'watch']\n----&gt; 2 item1, item2, item3 = gift_box  # Error: not enough values to unpack\n\nValueError: not enough values to unpack (expected 3, got 2)"
  },
  {
    "objectID": "blog/2024-September-14-Unpacking-Operator/unpacking_operator_in_python.html#unpacking-with-the-operator",
    "href": "blog/2024-September-14-Unpacking-Operator/unpacking_operator_in_python.html#unpacking-with-the-operator",
    "title": "Grokking the Unpacking Operator",
    "section": "",
    "text": "Now, imagine you want to only unpack the first and last items from the gift box, while ignoring the rest. This is where the * operator comes in handy:\n\ngift_box = ['jewelry', 'watch', 'perfume', 'airpods', 'flowers']\nfirst, *middle, last = gift_box\n\nprint(first)  # 'jewelry'\nprint(last)   # 'flowers'\nprint(middle)  # ['watch', 'perfume', 'airpods']\n\njewelry\nflowers\n['watch', 'perfume', 'airpods']\n\n\nThe * operator captures the remaining elements and packs them into a list. You can even discard them by using _:\n\nfirst, *_, last = gift_box\nprint(first, last)  # 'jewelry', 'flowers'\n\njewelry flowers\n\n\nThis method is incredibly useful when you only care about a few elements in a list or tuple."
  },
  {
    "objectID": "blog/2024-September-14-Unpacking-Operator/unpacking_operator_in_python.html#unpacking-dictionaries-with",
    "href": "blog/2024-September-14-Unpacking-Operator/unpacking_operator_in_python.html#unpacking-dictionaries-with",
    "title": "Grokking the Unpacking Operator",
    "section": "",
    "text": "Now let‚Äôs move on to unpacking dictionaries. Imagine the gift contains a dictionary with item names and their respective values.\n\ngift_details = {'jewelry': 'gold', 'watch': 'rolex', 'perfume': 'Chanel'}\n\nYou can unpack dictionaries using **. This is especially useful when you want to merge dictionaries, like adding more gifts in your spouse‚Äôs box of gifts:\n\nextra_gift = {'airpods': 'pro'}\nmerged_gifts = {**gift_details, **extra_gift}\nprint(merged_gifts)\n# {'jewelry': 'gold', 'watch': 'rolex', 'perfume': 'Chanel', 'airpods': 'pro'}\n\n{'jewelry': 'gold', 'watch': 'rolex', 'perfume': 'Chanel', 'airpods': 'pro'}\n\n\nThe ** operator spreads out the dictionary and allows you to combine multiple dictionaries into one."
  },
  {
    "objectID": "blog/2024-September-14-Unpacking-Operator/unpacking_operator_in_python.html#using-args-and-kwargs-in-functions",
    "href": "blog/2024-September-14-Unpacking-Operator/unpacking_operator_in_python.html#using-args-and-kwargs-in-functions",
    "title": "Grokking the Unpacking Operator",
    "section": "",
    "text": "You might have seen the terms *args and **kwargs before in Python. These are used when defining functions to accept a variable number of arguments and keyword arguments.\n\n*args lets you pass a variable number of positional arguments to a function.\n**kwargs lets you pass a variable number of keyword arguments.\n\nLet‚Äôs start with *args:\n\ndef calculate_total(*args):\n    return sum(args)\n\nprint(calculate_total(10, 20, 30))  # 60\nprint(calculate_total(5, 15))  # 20\n\n60\n20\n\n\nHere, *args captures all the positional arguments into a tuple. You can pass as many arguments as you like.\nNext, let‚Äôs look at **kwargs:\n\ndef print_gift_details(**kwargs):\n    for key, value in kwargs.items():\n        print(f\"{key}: {value}\")\n\nprint_gift_details(jewelry=\"gold\", watch=\"rolex\", perfume=\"Chanel\")\n# jewelry: gold\n# watch: rolex\n# perfume: Chanel\n\njewelry: gold\nwatch: rolex\nperfume: Chanel\n\n\nWith **kwargs, all keyword arguments are captured into a dictionary, which you can iterate over.\n\n\nYou can combine both in a function to accept any number of positional and keyword arguments:\n\ndef gift_summary(*args, **kwargs):\n    print(\"Items:\", args)\n    print(\"Details:\", kwargs)\n\ngift_summary('jewelry', 'watch', jewelry=\"gold\", watch=\"rolex\")\n# Items: ('jewelry', 'watch')\n# Details: {'jewelry': 'gold', 'watch': 'rolex'}\n\nItems: ('jewelry', 'watch')\nDetails: {'jewelry': 'gold', 'watch': 'rolex'}"
  },
  {
    "objectID": "blog/2024-September-14-Unpacking-Operator/unpacking_operator_in_python.html#method-chaining",
    "href": "blog/2024-September-14-Unpacking-Operator/unpacking_operator_in_python.html#method-chaining",
    "title": "Grokking the Unpacking Operator",
    "section": "",
    "text": "Method chaining in pandas is a technique where multiple methods are called sequentially on a DataFrame or Series in a single statement. Each method operates on the output of the previous one, making the code concise and readable. It helps avoid intermediate variables and enables efficient data processing.\nlet us create a dataframe from the numpy library:\n\nimport pandas as pd\nimport numpy as np \n# Setting up random number generator\nrng = np.random.default_rng(seed=42)\n\n# Generating random data for open, high, low, close prices\ndata = {\n    'Open': rng.uniform(low=100, high=200, size=10),\n    'High': rng.uniform(low=200, high=300, size=10),\n    'Low': rng.uniform(low=50, high=100, size=10),\n    'Close': rng.uniform(low=100, high=200, size=10)\n}\n\n# Creating the DataFrame\ndf = pd.DataFrame(data).round(4)\ndf1=df2=df.copy() # making copies to use for examples later. \ndf\n\n\n\n\n\n\n\n\nOpen\nHigh\nLow\nClose\n\n\n\n\n0\n177.3956\n237.0798\n87.9044\n174.4762\n\n\n1\n143.8878\n292.6765\n67.7263\n196.7510\n\n\n2\n185.8598\n264.3865\n98.5349\n132.5825\n\n\n3\n169.7368\n282.2762\n94.6561\n137.0460\n\n\n4\n109.4177\n244.3414\n88.9192\n146.9556\n\n\n5\n197.5622\n222.7239\n59.7319\n118.9471\n\n\n6\n176.1140\n255.4585\n73.3361\n112.9922\n\n\n7\n178.6064\n206.3817\n52.1902\n147.5705\n\n\n8\n112.8114\n282.7631\n57.7145\n122.6909\n\n\n9\n145.0386\n263.1664\n84.1524\n166.9814\n\n\n\n\n\n\n\nTo add a new feature like calculating the moving average of the Close price in a DataFrame, you can use the rolling() function in pandas. Here‚Äôs how to calculate the moving average (for example, a 3-period moving average) of the Close price and add it as a new column to the DataFrame.\n\ndf1['SMA'] = df1['Close'].rolling(window=3).mean()\ndf1\n\n\n\n\n\n\n\n\nOpen\nHigh\nLow\nClose\nSMA\n\n\n\n\n0\n177.3956\n237.0798\n87.9044\n174.4762\nNaN\n\n\n1\n143.8878\n292.6765\n67.7263\n196.7510\nNaN\n\n\n2\n185.8598\n264.3865\n98.5349\n132.5825\n167.936567\n\n\n3\n169.7368\n282.2762\n94.6561\n137.0460\n155.459833\n\n\n4\n109.4177\n244.3414\n88.9192\n146.9556\n138.861367\n\n\n5\n197.5622\n222.7239\n59.7319\n118.9471\n134.316233\n\n\n6\n176.1140\n255.4585\n73.3361\n112.9922\n126.298300\n\n\n7\n178.6064\n206.3817\n52.1902\n147.5705\n126.503267\n\n\n8\n112.8114\n282.7631\n57.7145\n122.6909\n127.751200\n\n\n9\n145.0386\n263.1664\n84.1524\n166.9814\n145.747600\n\n\n\n\n\n\n\nTo achieve the same using method chaining:\n\ndf2 = (df2\n.assign(SMA=lambda x: x['Close'].rolling(window=3)\n.mean())\n)\ndf2\n\n\n\n\n\n\n\n\nOpen\nHigh\nLow\nClose\nSMA\n\n\n\n\n0\n177.3956\n237.0798\n87.9044\n174.4762\nNaN\n\n\n1\n143.8878\n292.6765\n67.7263\n196.7510\nNaN\n\n\n2\n185.8598\n264.3865\n98.5349\n132.5825\n167.936567\n\n\n3\n169.7368\n282.2762\n94.6561\n137.0460\n155.459833\n\n\n4\n109.4177\n244.3414\n88.9192\n146.9556\n138.861367\n\n\n5\n197.5622\n222.7239\n59.7319\n118.9471\n134.316233\n\n\n6\n176.1140\n255.4585\n73.3361\n112.9922\n126.298300\n\n\n7\n178.6064\n206.3817\n52.1902\n147.5705\n126.503267\n\n\n8\n112.8114\n282.7631\n57.7145\n122.6909\n127.751200\n\n\n9\n145.0386\n263.1664\n84.1524\n166.9814\n145.747600\n\n\n\n\n\n\n\nThis seems to be working just fine. However, sometimes we might want to create a function that could insert multiple features at once based off of the users‚Äô choice. An example is given below:\n\ndef add_multiple_smas(df, col, *windows):\n    for window in windows:\n      df[f'SMA_{window}'] = df[col].rolling(window=window).mean()\n    return df\n\n# Example usage\nadd_multiple_smas(df1, 'Close', 2, 3, 4)\n\n\n\n\n\n\n\n\nOpen\nHigh\nLow\nClose\nSMA\nSMA_2\nSMA_3\nSMA_4\n\n\n\n\n0\n177.3956\n237.0798\n87.9044\n174.4762\nNaN\nNaN\nNaN\nNaN\n\n\n1\n143.8878\n292.6765\n67.7263\n196.7510\nNaN\n185.61360\nNaN\nNaN\n\n\n2\n185.8598\n264.3865\n98.5349\n132.5825\n167.936567\n164.66675\n167.936567\nNaN\n\n\n3\n169.7368\n282.2762\n94.6561\n137.0460\n155.459833\n134.81425\n155.459833\n160.213925\n\n\n4\n109.4177\n244.3414\n88.9192\n146.9556\n138.861367\n142.00080\n138.861367\n153.333775\n\n\n5\n197.5622\n222.7239\n59.7319\n118.9471\n134.316233\n132.95135\n134.316233\n133.882800\n\n\n6\n176.1140\n255.4585\n73.3361\n112.9922\n126.298300\n115.96965\n126.298300\n128.985225\n\n\n7\n178.6064\n206.3817\n52.1902\n147.5705\n126.503267\n130.28135\n126.503267\n131.616350\n\n\n8\n112.8114\n282.7631\n57.7145\n122.6909\n127.751200\n135.13070\n127.751200\n125.550175\n\n\n9\n145.0386\n263.1664\n84.1524\n166.9814\n145.747600\n144.83615\n145.747600\n137.558750\n\n\n\n\n\n\n\nAttempting to accomplish a similar feat using method chaining will result in an error because the .assign method expects keyword arguments. The best way is to use the unpacking operator to achieve this with the help of dictionary comprehension to loop through the different values for the SMAs.\n\ndef add_multiple_smas(df, col, *windows, **kwargs):\n    \n    # Use assign to add the columns in a chained fashion\n    return df.assign(**{f'SMA_{window}': df[col].rolling(window=window, **kwargs).mean() for window in windows})\n\n# Example usage with method chaining\nadd_multiple_smas(df2, 'Close', 2, 3, 4)\n\n\n\n\n\n\n\n\nOpen\nHigh\nLow\nClose\nSMA\nSMA_2\nSMA_3\nSMA_4\n\n\n\n\n0\n177.3956\n237.0798\n87.9044\n174.4762\nNaN\nNaN\nNaN\nNaN\n\n\n1\n143.8878\n292.6765\n67.7263\n196.7510\nNaN\n185.61360\nNaN\nNaN\n\n\n2\n185.8598\n264.3865\n98.5349\n132.5825\n167.936567\n164.66675\n167.936567\nNaN\n\n\n3\n169.7368\n282.2762\n94.6561\n137.0460\n155.459833\n134.81425\n155.459833\n160.213925\n\n\n4\n109.4177\n244.3414\n88.9192\n146.9556\n138.861367\n142.00080\n138.861367\n153.333775\n\n\n5\n197.5622\n222.7239\n59.7319\n118.9471\n134.316233\n132.95135\n134.316233\n133.882800\n\n\n6\n176.1140\n255.4585\n73.3361\n112.9922\n126.298300\n115.96965\n126.298300\n128.985225\n\n\n7\n178.6064\n206.3817\n52.1902\n147.5705\n126.503267\n130.28135\n126.503267\n131.616350\n\n\n8\n112.8114\n282.7631\n57.7145\n122.6909\n127.751200\n135.13070\n127.751200\n125.550175\n\n\n9\n145.0386\n263.1664\n84.1524\n166.9814\n145.747600\n144.83615\n145.747600\n137.558750\n\n\n\n\n\n\n\nMethod chaining in pandas allows for more concise, readable, and functional-style code by performing multiple transformations in a single statement without creating intermediate variables. This makes the code more compact and often easier to follow when handling complex data transformations. It can also help avoid side effects by keeping transformations within the same flow, making it easier to debug and maintain.\nHowever, it‚Äôs also a matter of preference. Some developers prefer method chaining for its elegance and simplicity, while others prefer using intermediate variables for clarity, especially when dealing with more complex logic, as it can be easier to inspect the data at different stages of transformation. The choice depends on the coding style that the person or team finds most understandable and maintainable."
  },
  {
    "objectID": "blog/2024-September-14-Unpacking-Operator/unpacking_operator_in_python.html#conclusion",
    "href": "blog/2024-September-14-Unpacking-Operator/unpacking_operator_in_python.html#conclusion",
    "title": "Grokking the Unpacking Operator",
    "section": "",
    "text": "Unpacking is a powerful feature in Python that helps you write cleaner and more efficient code. Whether you‚Äôre unpacking lists, tuples, or dictionaries, or using *args and **kwargs in functions, this feature allows for flexible and dynamic code. Moreover, unpacking can be used with popular libraries like Pandas and NumPy to streamline data manipulation.\nSo, the next time you find yourself opening a gift box, remember: Python unpacking is just like unboxing‚Äîtaking out each item, one by one, and making it yours!"
  },
  {
    "objectID": "blog/2024-February-24-Introduction-to-Matplotlib/Introduction_to_matplotlib.html",
    "href": "blog/2024-February-24-Introduction-to-Matplotlib/Introduction_to_matplotlib.html",
    "title": "An introduction to Matplotlib",
    "section": "",
    "text": "Matplotlib, often a first encounter for Python enthusiasts diving into data visualization, has evolved into an indispensable tool in the Python data science stack. Initially, its complexity and style choices may have deterred users, but with updates like Matplotlib 2.0, it now offers a blend of power and aesthetics, rivalling R‚Äôs ggplot.\nMatplotlib‚Äôs Dual Interfaces: MATLAB-style vs.¬†Object-Oriented\nMatplotlib offers two main interfaces for plotting:\n\nMATLAB-style Interface: Inspired by MATLAB, this interface is state-based. It‚Äôs like using a traditional canvas, where each command alters the state of the canvas (e.g., adding a line, changing colors). It‚Äôs straightforward but can get confusing with complex plots.\nObject-Oriented (OO) Interface: This is Python‚Äôs way. Here, you deal with objects and their methods. You have more control and clarity, especially for complicated figures. It‚Äôs like having blueprints for different parts of your plot, which you can modify independently.\n\nWhy Object-Oriented is the Way to Go\nThe OO approach aligns with Python‚Äôs philosophy and provides clarity, especially for complex visualizations. It‚Äôs like being a director of a play where you have control over every actor (plot element) rather than just narrating a story.\nLet us look at some examples. We will start by explaining a few steps here before we start coding. We will be using made up data for now.\n\nImporting Libraries:\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nmatplotlib.pyplot: A collection of command-style functions that make Matplotlib work like MATLAB. This module is used for plotting graphs. It allows users to create figures, set up plotting areas within figures, plot lines or other graphical elements, add labels and decorations to plots, and more. The pyplot functions operate on the current figure and plotting area, making it easy to generate visualizations quickly.\nnumpy: A fundamental package for scientific computing in Python. It‚Äôs used here for numerical operations and random number generation.\n\nCreating a Random Number Generator (RNG):\nrng = np.random.default_rng()  # Using numpy's random number generator\n\nnp.random.default_rng(): This method creates an instance of a random number generator. The default_rng() is a part of NumPy‚Äôs new random number generation system that provides a variety of random number generation methods. It‚Äôs preferred over older functions like np.random.rand() or np.random.randn() for several reasons:\n\nImproved Randomness: It uses a more modern and statistically robust algorithm for random number generation.\nReproducibility: It offers better reproducibility across different platforms.\nFlexibility: It provides a wider range of random number generation functions and better control over the random state.\n\n\nGenerating Random Data:\nx = rng.random(100)\ny = rng.random(100)\n\nrng.random(100): Generates 100 random numbers between 0 and 1. These numbers are used as the x and y coordinates for the scatter plot.\n\nCreating a Scatter Plot:\nplt.scatter(x, y)  # Adding a scatter plot\n\nplt.scatter(x, y): This function creates a scatter plot with x and y as its coordinates. Scatter plots are used to observe relationships between variables.\n\nAdding a Title and Displaying the Plot:\nplt.title(\"Random Scatter Plot\")\nplt.show()\n\nplt.title(): Adds a title to the plot.\nplt.show(): Displays the plot. In a Jupyter notebook, this line is often optional as plots are displayed automatically.\n\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\nrng = np.random.default_rng()  # Using numpy's random number generator\nx = rng.random(100)\ny = rng.random(100)\n\nMATLAB-style Plotting in Matplotlib\nUsing the MATLAB-style interface, you might create a simple plot like this:\n\nplt.scatter(x, y)  # Adding a scatter plot\nplt.title(\"Random Scatter Plot\")\nplt.show()\n\n\n\n\n\n\n\n\nMatplotlib‚Äôs Object-Oriented Approach\nUsing the OO approach, the same plot would be:\n\nfig, ax = plt.subplots()  # Creating figure and axes objects\nax.scatter(x, y)  # Adding scatter plot to the axes\nax.set_title(\"Random Scatter Plot\")\nplt.show()\n\n\n\n\n\n\n\n\nLet us take a look at what we just did above.\n\nCreating Figure and Axes Objects:\n\nfig, ax = plt.subplots(): This line is the starting point for most Matplotlib plots in the OO style. Here, plt.subplots() is a function that creates two objects:\n\nfig (short for ‚Äòfigure‚Äô): This object represents the entire figure or plot, a container holding all elements of the plot.\nax (short for ‚Äòaxes‚Äô): This is an object representing a single plot within the figure. Despite its name, ‚Äòaxes‚Äô here doesn‚Äôt refer to the x-axis and y-axis; rather, it‚Äôs the space where data will be plotted.\n\n\nAdding a Scatter Plot to the Axes:\n\nax.scatter(x, y): Here, the scatter method of the ax object is used to create a scatter plot. The x and y are arrays or lists of values that represent the positions of points on the plot. This method plots each point on the axes, with x values determining the horizontal position and y values the vertical position.\n\nSetting the Title:\n\nax.set_title(\"Random Scatter Plot\"): This method sets the title of the plot. The string ‚ÄúRandom Scatter Plot‚Äù is displayed as the title of the axes. It‚Äôs a way to provide a descriptive label for what the plot represents.\n\nDisplaying the Plot:\n\nplt.show(): This command is used to display the figure. It tells Matplotlib to render the plot and show it to the user. In a Jupyter notebook, this might happen automatically, but in other environments (like a Python script), this command is essential to actually see the plot.\n\n\nWhy Use the OO Approach?\n\nClarity and Control: The OO approach provides a clear structure for the plot, making it easier to understand and modify. Each element of the plot is controlled by explicit commands.\nFlexibility: It allows for more complex layouts (like multiple subplots) and detailed customization, as each part of the plot can be individually controlled.\nConsistency with Python‚Äôs Style: Using objects and methods is more aligned with general Python programming practices.\n\nOverall, this code snippet is an example of creating a scatter plot using Matplotlib‚Äôs OO interface, which is recommended for its clarity, flexibility, and alignment with Python‚Äôs object-oriented programming style.\n\n\n\nUnderstanding the Essentials\n\nKey Concepts: Grasp the basic Matplotlib terminology, especially ‚ÄòFigure‚Äô and ‚ÄòAxes‚Äô.\n\nFigure: The entire plot, which can contain one or more ‚ÄòAxes‚Äô.\nAxes: A single plot within the ‚ÄòFigure‚Äô, where you draw your data.\n\nObject-Oriented Approach: Always utilize Matplotlib‚Äôs Object-Oriented (OO) interface. It aligns with Python‚Äôs philosophy, offering clarity and control over your plots.\nStart with Pandas: Begin your visualization journey with basic Pandas plotting for simplicity.\nLeverage Seaborn: For more complex statistical visualizations, use Seaborn, which builds on Matplotlib.\nCustomize with Matplotlib: Use Matplotlib to further tailor your Pandas or Seaborn visualizations.\n\nWhy This Matters: Understanding these fundamentals is crucial, as Matplotlib forms the foundation for many advanced Python packages like Seaborn and ggplot. A solid grasp of Matplotlib makes learning these packages easier.\n\n\nWe are going to get some datasets from a github repo, concatenate them and store them as a FEATHER file. Choosing Feather format over the more widely used CSV format can be beneficial for several reasons, particularly in the context of data analysis and scientific computing:\n\nPerformance: Feather is designed for efficiency. It reads and writes data much faster than CSV. This is especially noticeable with large datasets.\nStorage Efficiency: Feather often uses less disk space than CSV because it uses binary storage format. This can be significant when dealing with large datasets.\nData Integrity: Unlike CSV, which can sometimes lead to loss of information about data types (e.g., integers, floats, dates) when saving and loading, Feather preserves data types. This means that when you load a dataset, it will have the same types as when it was saved.\nCompatibility with Data Analysis Tools: Feather is designed to be used with data analysis tools like Pandas in Python and data.table or dplyr in R. This makes it a good choice for workflows that involve both Python and R.\nHandling of Large Data: Feather is more suitable for large datasets that can be problematic to handle in CSV due to size and performance issues.\nNo Need for Parsing: With CSV files, there‚Äôs often a need to parse the file and possibly convert strings to numerical values or dates. Feather files, however, are ready to be used as-is, which simplifies tof your project.\n\n\nimport pandas as pd\n\nA helper function has been created called download_and_combine_csv and stored in a download_and_combine_csv.py module to help with the download of the files from a github repo linked in the cell below. We won‚Äôt be bothered with the details of the helper function however.\n\nfrom download_and_combine_csv import download_and_combine_csv\n# Base URL for the CSV files\nbase_url = \"https://raw.githubusercontent.com/KeithGalli/Pandas-Data-Science-Tasks/master/SalesAnalysis/Sales_Data/\"\n\n# List of CSV file names\nfile_names = [\n    \"Sales_April_2019.csv\",\n    \"Sales_August_2019.csv\",\n    \"Sales_December_2019.csv\",\n    \"Sales_February_2019.csv\",\n    \"Sales_January_2019.csv\",\n    \"Sales_July_2019.csv\",\n    \"Sales_June_2019.csv\",\n    \"Sales_March_2019.csv\",\n    \"Sales_May_2019.csv\",\n    \"Sales_November_2019.csv\",\n    \"Sales_October_2019.csv\",\n    \"Sales_September_2019.csv\"\n]\n\ncombined_file_name = \"combined_sales_data\"\n\ndownload_and_combine_csv(base_url, file_names, combined_file_name)\n\nAll data saved in Feather file.\n\n\n\nfeather_file = 'combined_sales_data.feather'\n\n# Load the data from the Feather file\nsales_data = pd.read_feather(feather_file)\n\nsales_data\n\n\n\n\n\n\n\n\nOrder ID\nProduct\nQuantity Ordered\nPrice Each\nOrder Date\nPurchase Address\n\n\n\n\n0\n176558\nUSB-C Charging Cable\n2\n11.95\n04/19/19 08:46\n917 1st St, Dallas, TX 75001\n\n\n1\nNone\nNone\nNone\nNone\nNone\nNone\n\n\n2\n176559\nBose SoundSport Headphones\n1\n99.99\n04/07/19 22:30\n682 Chestnut St, Boston, MA 02215\n\n\n3\n176560\nGoogle Phone\n1\n600\n04/12/19 14:38\n669 Spruce St, Los Angeles, CA 90001\n\n\n4\n176560\nWired Headphones\n1\n11.99\n04/12/19 14:38\n669 Spruce St, Los Angeles, CA 90001\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n11681\n259353\nAAA Batteries (4-pack)\n3\n2.99\n09/17/19 20:56\n840 Highland St, Los Angeles, CA 90001\n\n\n11682\n259354\niPhone\n1\n700\n09/01/19 16:00\n216 Dogwood St, San Francisco, CA 94016\n\n\n11683\n259355\niPhone\n1\n700\n09/23/19 07:39\n220 12th St, San Francisco, CA 94016\n\n\n11684\n259356\n34in Ultrawide Monitor\n1\n379.99\n09/19/19 17:30\n511 Forest St, San Francisco, CA 94016\n\n\n11685\n259357\nUSB-C Charging Cable\n1\n11.95\n09/30/19 00:18\n250 Meadow St, San Francisco, CA 94016\n\n\n\n\n186850 rows √ó 6 columns\n\n\n\n\nsales_data.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 186850 entries, 0 to 11685\nData columns (total 6 columns):\n #   Column            Non-Null Count   Dtype \n---  ------            --------------   ----- \n 0   Order ID          186305 non-null  object\n 1   Product           186305 non-null  object\n 2   Quantity Ordered  186305 non-null  object\n 3   Price Each        186305 non-null  object\n 4   Order Date        186305 non-null  object\n 5   Purchase Address  186305 non-null  object\ndtypes: object(6)\nmemory usage: 10.0+ MB\n\n\nThe function tweak_data is designed to clean and format a DataFrame data that contains sales data. Here‚Äôs a concise explanation of each step in the function:\n\nRename Columns:\n\n.rename(columns=lambda x: x.strip().replace(' ', '_')): This renames the columns by removing any leading or trailing whitespace and replacing spaces with underscores. This step makes column names more consistent and easier to reference in code.\n\nConvert Columns to Numeric:\n\n.assign(Quantity_Ordered = lambda x: pd.to_numeric(x['Quantity_Ordered'], errors='coerce'), Price_Each = lambda x: pd.to_numeric(x['Price_Each'], errors='coerce')): This converts the ‚ÄòQuantity_Ordered‚Äô and ‚ÄòPrice_Each‚Äô columns to numeric types, handling any non-numeric values by turning them into NaNs (errors='coerce').\n\nDrop NaN Values:\n\n.dropna(): Removes any rows with NaN values, ensuring that the DataFrame only contains complete data.\n\nConvert ‚ÄòOrder_Date‚Äô to DateTime and Set as Index:\n\n.assign(Order_Date = lambda x: pd.to_datetime(x['Order_Date'], format='%m/%d/%y %H:%M'), errors='coerce'): Converts the ‚ÄòOrder_Date‚Äô column to a datetime object using the specified format. If any values cannot be converted, it coerces them to NaN.\n.set_index('Order_Date'): Sets the ‚ÄòOrder_Date‚Äô column as the index of the DataFrame.\n\nSort DataFrame by Index:\n\n.sort_index(): Sorts the DataFrame by the ‚ÄòOrder_Date‚Äô index in ascending order.\n\n\nBy calling tweaked_sales_data = tweak_data(sales_data), the function is applied to a DataFrame sales_data, performing all these cleaning and formatting steps, and the result is stored in tweaked_sales_data. This processed DataFrame is now more suitable for analysis, with cleaner column names, proper data types, and a sorted datetime index.\n\ndef tweak_data(data):\n    return (data\n            .rename(columns=lambda x: x.strip()\n            .replace(' ', '_'))\n            .assign(Quantity_Ordered = lambda x: pd.to_numeric(x['Quantity_Ordered'], errors='coerce'),\n                   Price_Each = lambda x: pd.to_numeric(x['Price_Each'], errors='coerce'))\n            .dropna()\n            .assign(Order_Date = lambda x: pd.to_datetime(x['Order_Date'], format='%m/%d/%y %H:%M'))\n            .set_index('Order_Date')\n            .sort_index()\n           )\n\ntweaked_sales_data = tweak_data(sales_data)\ntweaked_sales_data\n\n\n\n\n\n\n\n\nOrder_ID\nProduct\nQuantity_Ordered\nPrice_Each\nPurchase_Address\n\n\nOrder_Date\n\n\n\n\n\n\n\n\n\n2019-01-01 03:07:00\n147268\nWired Headphones\n1.0\n11.99\n9 Lake St, New York City, NY 10001\n\n\n2019-01-01 03:40:00\n148041\nUSB-C Charging Cable\n1.0\n11.95\n760 Church St, San Francisco, CA 94016\n\n\n2019-01-01 04:56:00\n149343\nApple Airpods Headphones\n1.0\n150.00\n735 5th St, New York City, NY 10001\n\n\n2019-01-01 05:53:00\n149964\nAAA Batteries (4-pack)\n1.0\n2.99\n75 Jackson St, Dallas, TX 75001\n\n\n2019-01-01 06:03:00\n149350\nUSB-C Charging Cable\n2.0\n11.95\n943 2nd St, Atlanta, GA 30301\n\n\n...\n...\n...\n...\n...\n...\n\n\n2020-01-01 04:13:00\n304165\nAAA Batteries (4-pack)\n1.0\n2.99\n825 Adams St, Portland, OR 97035\n\n\n2020-01-01 04:21:00\n299125\nUSB-C Charging Cable\n1.0\n11.95\n754 Hickory St, New York City, NY 10001\n\n\n2020-01-01 04:54:00\n305840\nBose SoundSport Headphones\n1.0\n99.99\n784 River St, San Francisco, CA 94016\n\n\n2020-01-01 05:13:00\n300519\nBose SoundSport Headphones\n1.0\n99.99\n657 Spruce St, New York City, NY 10001\n\n\n2020-01-01 05:13:00\n300519\nLightning Charging Cable\n1.0\n14.95\n657 Spruce St, New York City, NY 10001\n\n\n\n\n185950 rows √ó 5 columns\n\n\n\nEyeballing the output, we can discern that the data pertains to sales transactions for the year 2019. This quick visual inspection helps us grasp the overarching theme of the dataset, setting the stage for a more detailed analysis and summarization, particularly focusing on the highest-performing products.\n\ndef prep4plot(data):\n    return (data\n     .groupby('Product')\n     [['Price_Each', 'Quantity_Ordered']]\n     .agg({'Price_Each' : 'sum', 'Quantity_Ordered':'count'})\n     .sort_values(by='Price_Each', ascending=False)\n     .reset_index()\n     .rename(columns={'Price_Each': 'Sales', 'Quantity_Ordered': 'Purchases'})\n    )\n\ndata4plot = prep4plot(tweaked_sales_data)\ndata4plot.head()\n\n\n\n\n\n\n\n\nProduct\nSales\nPurchases\n\n\n\n\n0\nMacbook Pro Laptop\n8030800.00\n4724\n\n\n1\niPhone\n4789400.00\n6842\n\n\n2\nThinkPad Laptop\n4127958.72\n4128\n\n\n3\nGoogle Phone\n3315000.00\n5525\n\n\n4\n27in 4K Gaming Monitor\n2429637.70\n6230\n\n\n\n\n\n\n\nThis code snippet performs several data manipulation and aggregation tasks on the tweaked_sales_data DataFrame:\n\nGrouping by ‚ÄòProduct‚Äô: .groupby('Product') groups the DataFrame by unique values in the ‚ÄòProduct‚Äô column. Each group corresponds to a different product.\nSelecting Columns for Aggregation: [['Price_Each', 'Quantity_Ordered']] selects two columns for subsequent operations - Price_Each and Quantity_Ordered.\nAggregating Data:\n\n.agg({'Price_Each' : 'sum', 'Quantity_Ordered':'count'}): This aggregates the data within each product group. It calculates the sum of Price_Each and the count of Quantity_Ordered for each product. Essentially, it sums up total sales and counts the number of orders per product.\n\nSorting Results:\n\n.sort_values(by='Price_Each', ascending=False): This sorts the aggregated results in descending order based on the ‚ÄòPrice_Each‚Äô column, which, after aggregation, represents the total sales per product.\n\nResetting Index:\n\n.reset_index(): This resets the index of the DataFrame. After grouping and sorting, the index might be in a custom state (like product names). Resetting the index turns it back to a simple numerical range index.\n\nRenaming Columns:\n\n.rename(columns={'Price_Each': 'Sales', 'Quantity_Ordered': 'Purchases'}): This renames the columns for clarity. ‚ÄòPrice_Each‚Äô is renamed to ‚ÄòSales‚Äô, indicating total sales revenue per product, and ‚ÄòQuantity_Ordered‚Äô to ‚ÄòPurchases‚Äô, indicating the number of purchases of each product.\n\n\nThe final result of this snippet is a neatly organized DataFrame that provides a summarized view of sales data, showing the total sales revenue and the number of purchases for each product, sorted by the total sales revenue in descending order.\nThe first step is to plot the data using the standard pandas plotting function. We will choose a nice and simple plot style too.\n\nprint(plt.style.available)\n\n['Solarize_Light2', '_classic_test_patch', '_mpl-gallery', '_mpl-gallery-nogrid', 'bmh', 'classic', 'dark_background', 'fast', 'fivethirtyeight', 'ggplot', 'grayscale', 'seaborn-v0_8', 'seaborn-v0_8-bright', 'seaborn-v0_8-colorblind', 'seaborn-v0_8-dark', 'seaborn-v0_8-dark-palette', 'seaborn-v0_8-darkgrid', 'seaborn-v0_8-deep', 'seaborn-v0_8-muted', 'seaborn-v0_8-notebook', 'seaborn-v0_8-paper', 'seaborn-v0_8-pastel', 'seaborn-v0_8-poster', 'seaborn-v0_8-talk', 'seaborn-v0_8-ticks', 'seaborn-v0_8-white', 'seaborn-v0_8-whitegrid', 'tableau-colorblind10']\n\n\n\nplt.style.use('ggplot')\n\n\n_=data4plot.plot(kind='barh', y=\"Sales\", x=\"Product\")\n\n\n\n\n\n\n\n\nThe code snippet _=data4plot.plot(kind='barh', y=\"Sales\", x=\"Product\") is a Pandas plotting command with several components:\n\ndata4plot: This is a Pandas DataFrame containing the data we want to plot that has the two columns named Sales and Product.\n.plot(): This is a method associated with Pandas DataFrames used for generating various types of plots.\nkind='barh': This parameter specifies the kind of plot to generate. In this case, 'barh' stands for horizontal bar plot. This plot type is useful for comparing quantities across different categories, which are displayed as bars running horizontally.\ny=\"Sales\": This parameter sets the column in the DataFrame that will be used for the plot‚Äôs vertical axis. In this plot, it means that the lengths of the horizontal bars will represent the values in the ‚ÄòSales‚Äô column.\nx=\"Product\": This sets the column to be used for the horizontal axis labels. Here, the labels on the y-axis (since it‚Äôs a horizontal bar plot) will be taken from the ‚ÄòProduct‚Äô column. Each product will have a corresponding bar.\n_=: This is a common Python idiom for assigning the output of a function to a dummy variable (_) when the output is not needed. In this case, the plot function returns a matplotlib object. If you‚Äôre just displaying the plot and don‚Äôt need to further manipulate or access this object, it‚Äôs common to assign it to _ to indicate that it can be ignored. We use this basically to suppress some outputs that tend to come with matplotlib plots. The semi-colon ; also achieves the same as we can see later in our code.\n\nPandas‚Äô built-in plotting capabilities are used here mainly due to their simplicity and efficiency in creating preliminary visualizations. Given that Pandas is commonly used for preliminary data handling and analysis, it‚Äôs practical to leverage its straightforward plotting tools for initial graphical representations. This approach allows for a swift and effortless transition from data manipulation to visualization, making it an ideal starting point for most users.\nEnhancing our Visualization\nOnce we have grasped the basics of creating a plot using Pandas, the next logical step is to enhance and personalize it. Simple enhancements, such as adding titles and labels, are straightforward with Pandas‚Äô plotting features. However, as our visualization needs evolve, we may find ourselves seeking more advanced customization options. This is why we have to familiarize ourselves with more extensive customization techniques early on. Doing so will equip us with the skills to fully tailor our visualizations beyond the basic offerings of Pandas‚Äô plotting functions.\n\nfig, ax = plt.subplots()\n\n_=data4plot.plot(kind='barh', y=\"Sales\", x=\"Product\", ax=ax)\n\n\n\n\n\n\n\n\nUpon creating the initial plot with Pandas, we enhance it by integrating plt.subplots(), a Matplotlib function, to gain more customization control. This approach might produce a plot visually identical to the original Pandas plot, but the significant difference lies in the newfound access to Matplotlib‚Äôs ax and fig objects. This access is crucial for detailed customization and control over the plot‚Äôs aspects.\nThe integration of plt.subplots() allows us to tap into Matplotlib‚Äôs extensive capabilities while retaining the simplicity and speed of Pandas‚Äô plotting. This hybrid method is not only efficient but also versatile, making it easier to apply and adapt various customization techniques to suit specific requirements.\nFor instance, consider modifying the x-axis limits or altering axis labels. With the ax variable now encapsulating the plot‚Äôs axes, we unlock a greater degree of control over these elements. This method of combining Pandas‚Äô ease of use with Matplotlib‚Äôs detailed customization offers the best of both worlds in data visualization.\n\nfig, ax = plt.subplots()\ndata4plot.plot(kind='barh', y=\"Sales\", x=\"Product\", ax=ax)\nax.set_xlim([-10000, 9000000])\nax.set_xlabel('Total Sales')\nax.set_ylabel('Product')\nax.set_title('2019 Sales');\n\n\n\n\n\n\n\n\nOr we do it another way:\n\nfig, ax = plt.subplots()\ndata4plot.plot(kind='barh', y=\"Sales\", x=\"Product\", ax=ax)\nax.set_xlim([-10000, 9000000])\nax.set(title='2019 Sales', xlabel='Total Sales', ylabel='Product');\n\n\n\n\n\n\n\n\nThis method showcases the flexibility of the plt.subplots() function from Matplotlib in fine-tuning our visualization. Specifically, it allows for the customization of the plot‚Äôs size by setting the figsize parameter, which is defined in inches. This level of control over the dimensions of our plot ensures that it fits the intended context or presentation space perfectly. Additionally, the functionality to modify the plot extends to aspects like the legend. With the command ax.legend().set_visible(False), we can effortlessly hide the legend if it‚Äôs unnecessary or distracting, thus maintaining a focus on the most crucial elements of our visualization. This approach underscores the advantage of blending simplicity with the powerful customization capabilities of Matplotlib.\n\nfig, ax = plt.subplots(figsize=(5, 6))\ndata4plot.plot(kind='barh', y=\"Sales\", x=\"Product\", ax=ax)\nax.set_xlim([-10000, 9000000])\nax.set(title='2019 Sales', xlabel='Total Sales')\nax.legend().set_visible(False)\n\n\n\n\n\n\n\n\nTo enhance the visual appeal of our plot, one key area to focus on is the presentation of the ‚ÄúTotal Sales‚Äù figures. Matplotlib offers a powerful tool in the form of FuncFormatter, which allows for the application of custom functions to axis values, effectively enabling tailored formatting. This feature is particularly useful for rendering numerical data in a more readable and context-appropriate format.\nConsider, for example, a function designed to format currency, especially useful for figures in the higher ranges, such as several hundred thousand dollars. By applying such a function through FuncFormatter, ‚ÄúTotal Sales‚Äù values can be displayed in a format that is both aesthetically pleasing and easily comprehensible, ensuring that your plot communicates its data as effectively as possible. This approach illustrates the advantage of using Matplotlib‚Äôs advanced features to refine and customize the details of your data visualization.\n\nfrom matplotlib.ticker import FuncFormatter\n\n\ndef format_currency(value, position):\n    \"\"\"Format the currency based on its magnitude.\n\n    Args:\n    value (float): The numerical value to format.\n    position (int): The tick position (not used in this function).\n\n    Returns:\n    str: Formatted currency string as millions if value is &gt;= 1,000,000.\n    \"\"\"\n    if value &gt;= 1000000:\n        return f'${value*1e-6:1.1f}M'\n    \n    # No formatting provided for values under 1,000,000\n    return str(value)\n\n\nfig, ax = plt.subplots()\ndata4plot.plot(kind='barh', y=\"Sales\", x=\"Product\", ax=ax)\nax.set_xlim([-10000, 9000000])\nax.set(title='2019 Sales', xlabel='Total Sales', ylabel='Product')\nformatter = FuncFormatter(format_currency)\nax.xaxis.set_major_formatter(formatter)\nax.legend().set_visible(False);\n\n\n\n\n\n\n\n\nThis refined approach really illustrates the versatility and customizability at our disposal. The next feature to explore in customizing our plot is adding annotations, which can significantly enhance the information communicated by our visualization.\nTo illustrate this, consider using ax.axvline() to draw a vertical line on the plot. This can be particularly effective for highlighting average values or specific thresholds. Additionally, incorporating custom text annotations on the plot can be achieved with ax.text(). This allows for labeling specific points of interest, such as highlighting the introduction of new customers in our example.\nThe full code, complete with annotations and comments, demonstrates how these elements come together to create a more informative and visually appealing plot. This use of annotations and vertical lines not only enhances the visual clarity of the plot but also adds layers of context and meaning, making the data more accessible and understandable.\n\n# Create the figure and the axes\nfig, ax = plt.subplots()\n\n# Plot the data and get the averaged\ndata4plot.plot(kind='barh', y=\"Sales\", x=\"Product\", ax=ax)\navg = data4plot['Sales'].mean()\n\n# Set limits and labels\nax.set_xlim([-10000, 9000000])\nax.set(title='2019 Sales', xlabel='Total Sales', ylabel='Product')\n\n# Add a line for the average\nax.axvline(x=avg, color='burlywood', label='Average', linestyle='--', linewidth=2)\n\n# Annotate the average Sales\nfor cust in [6, 12, 16]:\n    ax.text(2_000_000, cust, \"Above average Sales\")\n\n# Format the currency\nformatter = FuncFormatter(format_currency)\nax.xaxis.set_major_formatter(formatter)\n\n# Hide the legend\nax.legend().set_visible(False)\n\n\n\n\n\n\n\n\n\n# Create the figure and the axes\nfig, ax = plt.subplots()\n\n# Plot the data and get the averaged\ndata4plot.plot(kind='barh', y=\"Sales\", x=\"Product\", ax=ax)\navg = data4plot['Sales'].mean()\n\n# Set limits and labels\nax.set_xlim([-10000, 9000000])\nax.set(title='2019 Sales', xlabel='Total Sales', ylabel='Product')\n\n# Add a line for the average\nax.axvline(x=avg, color='burlywood', label='Average', linestyle='--', linewidth=2)\n\n# Annotate with an arrow\nax.annotate('Average Sales', xy=(avg, 0), xytext=(avg + 2_000_000, 10),\n             arrowprops=dict(facecolor='black', shrink=0.05),\n             horizontalalignment='right', verticalalignment='top')\n\n\n# Format the currency\nformatter = FuncFormatter(format_currency)\nax.xaxis.set_major_formatter(formatter)\n\n# Hide the legend\nax.legend().set_visible(False)\n\n\n\n\n\n\n\n\n\nFunction Call:\n\nax.annotate(): This method is used to add an annotation to the plot.\n\nAnnotation Text:\n\n'Average Sales': This is the text that will appear in the annotation.\n\nAnnotation Position:\n\nxy=(avg, 0): The xy argument specifies the point (as a tuple) to be annotated. In this case, it‚Äôs the average sales value (avg) on the x-axis, and 0 on the y-axis.\nxytext=(avg + 2_000_000, 10): The xytext argument determines where the text part of the annotation will be placed. Here, it‚Äôs set to be 2,000,000 units right from the avg on the x-axis and 10 units up on the y-axis.\n\nArrow Properties:\n\narrowprops=dict(facecolor='black', shrink=0.05): This dictionary defines the appearance of the arrow. facecolor sets the color of the arrow, and shrink slightly shrinks the arrow from both the annotated point and the text for better visibility.\n\nText Alignment:\n\nhorizontalalignment='right', verticalalignment='top': These parameters control how the text is aligned relative to the xytext point.\n\n\nAdjusting the Annotation\n\nTo move the annotation text, adjust the xytext coordinates. Changing the first number moves the text left/right, and changing the second number moves it up/down.\nTo point the arrow to a different location, change the xy coordinates.\nYou can change the appearance of the arrow by modifying the arrowprops dictionary. For instance, changing facecolor will change the color of the arrow.\nThe alignment of the text can be adjusted with horizontalalignment and verticalalignment. Options include ‚Äòleft‚Äô, ‚Äòright‚Äô, ‚Äòtop‚Äô, ‚Äòbottom‚Äô, ‚Äòcenter‚Äô, etc.\n\nUp to this point, our modifications have been focused on individual plots. However, our capabilities extend to incorporating multiple plots within a single figure, as well as various options to save the entire figure. Let‚Äôs explore how we can achieve this.\nTo place two plots on a single figure, we‚Äôll start by creating the figure and its axes using plt.subplots():\nfig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, sharey=True, figsize=(7, 4))\nIn this instance, we‚Äôre specifying the layout of our figure with nrows and ncols, which determine the number of rows and columns of subplots, respectively. We‚Äôve chosen to create one row and two columns, thus positioning our two plots side by side. For clarity, especially for those new to Matplotlib, explicitly using these named parameters can make the code more intuitive and easier to understand when revisited.\nBy setting sharey=True, we ensure that both plots (ax1 and ax2) share the same y-axis labels, which is particularly useful for comparing plots with the same scale or data range.\nOnce we‚Äôve established our figure structure and shared axes, we can proceed to add our plots. Plotting on ax1 and ax2 allows us to place different visualizations side by side within the same figure, enhancing our ability to compare and contrast the data presented. This approach not only broadens our visualization toolkit but also encourages a more comprehensive analysis through side-by-side comparisons.\n\n# Get the figure and the axes\nfig, (ax1, ax2) = plt.subplots(nrows=1,ncols=2, sharey=True, figsize=(7, 4))\n\n# Plot the data and get the averaged\ndata4plot.plot(kind='barh', y=\"Sales\", x=\"Product\", ax=ax1)\n\nax1.set_xlim([-10000, 9000000])\nax1.set(title='2019 Sales', xlabel='Total Sales', ylabel='Product')\n\n# Plot the average as a vertical line\navg = data4plot['Sales'].mean()\nax1.axvline(x=avg, color='purple', label='Average', linestyle='--', linewidth=2)\n\n# Repeat for the unit plot\ndata4plot.plot(kind='barh', y=\"Purchases\", x=\"Product\", ax=ax2)\navg = data4plot['Purchases'].mean()\nax2.set(title='Units', xlabel='Total Units', ylabel='')\nax2.axvline(x=avg, color='purple', label='Average', linestyle='--', linewidth=2)\n\n# Title the figure\nfig.suptitle('2019 Sales Analysis', fontsize=14, fontweight='bold');\n\n# Hide the legends\nax1.legend().set_visible(False)\nax2.legend().set_visible(False)"
  },
  {
    "objectID": "blog/2024-February-24-Introduction-to-Matplotlib/Introduction_to_matplotlib.html#navigating-matplotlib",
    "href": "blog/2024-February-24-Introduction-to-Matplotlib/Introduction_to_matplotlib.html#navigating-matplotlib",
    "title": "An introduction to Matplotlib",
    "section": "",
    "text": "Matplotlib, often a first encounter for Python enthusiasts diving into data visualization, has evolved into an indispensable tool in the Python data science stack. Initially, its complexity and style choices may have deterred users, but with updates like Matplotlib 2.0, it now offers a blend of power and aesthetics, rivalling R‚Äôs ggplot.\nMatplotlib‚Äôs Dual Interfaces: MATLAB-style vs.¬†Object-Oriented\nMatplotlib offers two main interfaces for plotting:\n\nMATLAB-style Interface: Inspired by MATLAB, this interface is state-based. It‚Äôs like using a traditional canvas, where each command alters the state of the canvas (e.g., adding a line, changing colors). It‚Äôs straightforward but can get confusing with complex plots.\nObject-Oriented (OO) Interface: This is Python‚Äôs way. Here, you deal with objects and their methods. You have more control and clarity, especially for complicated figures. It‚Äôs like having blueprints for different parts of your plot, which you can modify independently.\n\nWhy Object-Oriented is the Way to Go\nThe OO approach aligns with Python‚Äôs philosophy and provides clarity, especially for complex visualizations. It‚Äôs like being a director of a play where you have control over every actor (plot element) rather than just narrating a story.\nLet us look at some examples. We will start by explaining a few steps here before we start coding. We will be using made up data for now.\n\nImporting Libraries:\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nmatplotlib.pyplot: A collection of command-style functions that make Matplotlib work like MATLAB. This module is used for plotting graphs. It allows users to create figures, set up plotting areas within figures, plot lines or other graphical elements, add labels and decorations to plots, and more. The pyplot functions operate on the current figure and plotting area, making it easy to generate visualizations quickly.\nnumpy: A fundamental package for scientific computing in Python. It‚Äôs used here for numerical operations and random number generation.\n\nCreating a Random Number Generator (RNG):\nrng = np.random.default_rng()  # Using numpy's random number generator\n\nnp.random.default_rng(): This method creates an instance of a random number generator. The default_rng() is a part of NumPy‚Äôs new random number generation system that provides a variety of random number generation methods. It‚Äôs preferred over older functions like np.random.rand() or np.random.randn() for several reasons:\n\nImproved Randomness: It uses a more modern and statistically robust algorithm for random number generation.\nReproducibility: It offers better reproducibility across different platforms.\nFlexibility: It provides a wider range of random number generation functions and better control over the random state.\n\n\nGenerating Random Data:\nx = rng.random(100)\ny = rng.random(100)\n\nrng.random(100): Generates 100 random numbers between 0 and 1. These numbers are used as the x and y coordinates for the scatter plot.\n\nCreating a Scatter Plot:\nplt.scatter(x, y)  # Adding a scatter plot\n\nplt.scatter(x, y): This function creates a scatter plot with x and y as its coordinates. Scatter plots are used to observe relationships between variables.\n\nAdding a Title and Displaying the Plot:\nplt.title(\"Random Scatter Plot\")\nplt.show()\n\nplt.title(): Adds a title to the plot.\nplt.show(): Displays the plot. In a Jupyter notebook, this line is often optional as plots are displayed automatically.\n\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\nrng = np.random.default_rng()  # Using numpy's random number generator\nx = rng.random(100)\ny = rng.random(100)\n\nMATLAB-style Plotting in Matplotlib\nUsing the MATLAB-style interface, you might create a simple plot like this:\n\nplt.scatter(x, y)  # Adding a scatter plot\nplt.title(\"Random Scatter Plot\")\nplt.show()\n\n\n\n\n\n\n\n\nMatplotlib‚Äôs Object-Oriented Approach\nUsing the OO approach, the same plot would be:\n\nfig, ax = plt.subplots()  # Creating figure and axes objects\nax.scatter(x, y)  # Adding scatter plot to the axes\nax.set_title(\"Random Scatter Plot\")\nplt.show()\n\n\n\n\n\n\n\n\nLet us take a look at what we just did above.\n\nCreating Figure and Axes Objects:\n\nfig, ax = plt.subplots(): This line is the starting point for most Matplotlib plots in the OO style. Here, plt.subplots() is a function that creates two objects:\n\nfig (short for ‚Äòfigure‚Äô): This object represents the entire figure or plot, a container holding all elements of the plot.\nax (short for ‚Äòaxes‚Äô): This is an object representing a single plot within the figure. Despite its name, ‚Äòaxes‚Äô here doesn‚Äôt refer to the x-axis and y-axis; rather, it‚Äôs the space where data will be plotted.\n\n\nAdding a Scatter Plot to the Axes:\n\nax.scatter(x, y): Here, the scatter method of the ax object is used to create a scatter plot. The x and y are arrays or lists of values that represent the positions of points on the plot. This method plots each point on the axes, with x values determining the horizontal position and y values the vertical position.\n\nSetting the Title:\n\nax.set_title(\"Random Scatter Plot\"): This method sets the title of the plot. The string ‚ÄúRandom Scatter Plot‚Äù is displayed as the title of the axes. It‚Äôs a way to provide a descriptive label for what the plot represents.\n\nDisplaying the Plot:\n\nplt.show(): This command is used to display the figure. It tells Matplotlib to render the plot and show it to the user. In a Jupyter notebook, this might happen automatically, but in other environments (like a Python script), this command is essential to actually see the plot.\n\n\nWhy Use the OO Approach?\n\nClarity and Control: The OO approach provides a clear structure for the plot, making it easier to understand and modify. Each element of the plot is controlled by explicit commands.\nFlexibility: It allows for more complex layouts (like multiple subplots) and detailed customization, as each part of the plot can be individually controlled.\nConsistency with Python‚Äôs Style: Using objects and methods is more aligned with general Python programming practices.\n\nOverall, this code snippet is an example of creating a scatter plot using Matplotlib‚Äôs OO interface, which is recommended for its clarity, flexibility, and alignment with Python‚Äôs object-oriented programming style.\n\n\n\nUnderstanding the Essentials\n\nKey Concepts: Grasp the basic Matplotlib terminology, especially ‚ÄòFigure‚Äô and ‚ÄòAxes‚Äô.\n\nFigure: The entire plot, which can contain one or more ‚ÄòAxes‚Äô.\nAxes: A single plot within the ‚ÄòFigure‚Äô, where you draw your data.\n\nObject-Oriented Approach: Always utilize Matplotlib‚Äôs Object-Oriented (OO) interface. It aligns with Python‚Äôs philosophy, offering clarity and control over your plots.\nStart with Pandas: Begin your visualization journey with basic Pandas plotting for simplicity.\nLeverage Seaborn: For more complex statistical visualizations, use Seaborn, which builds on Matplotlib.\nCustomize with Matplotlib: Use Matplotlib to further tailor your Pandas or Seaborn visualizations.\n\nWhy This Matters: Understanding these fundamentals is crucial, as Matplotlib forms the foundation for many advanced Python packages like Seaborn and ggplot. A solid grasp of Matplotlib makes learning these packages easier.\n\n\nWe are going to get some datasets from a github repo, concatenate them and store them as a FEATHER file. Choosing Feather format over the more widely used CSV format can be beneficial for several reasons, particularly in the context of data analysis and scientific computing:\n\nPerformance: Feather is designed for efficiency. It reads and writes data much faster than CSV. This is especially noticeable with large datasets.\nStorage Efficiency: Feather often uses less disk space than CSV because it uses binary storage format. This can be significant when dealing with large datasets.\nData Integrity: Unlike CSV, which can sometimes lead to loss of information about data types (e.g., integers, floats, dates) when saving and loading, Feather preserves data types. This means that when you load a dataset, it will have the same types as when it was saved.\nCompatibility with Data Analysis Tools: Feather is designed to be used with data analysis tools like Pandas in Python and data.table or dplyr in R. This makes it a good choice for workflows that involve both Python and R.\nHandling of Large Data: Feather is more suitable for large datasets that can be problematic to handle in CSV due to size and performance issues.\nNo Need for Parsing: With CSV files, there‚Äôs often a need to parse the file and possibly convert strings to numerical values or dates. Feather files, however, are ready to be used as-is, which simplifies tof your project.\n\n\nimport pandas as pd\n\nA helper function has been created called download_and_combine_csv and stored in a download_and_combine_csv.py module to help with the download of the files from a github repo linked in the cell below. We won‚Äôt be bothered with the details of the helper function however.\n\nfrom download_and_combine_csv import download_and_combine_csv\n# Base URL for the CSV files\nbase_url = \"https://raw.githubusercontent.com/KeithGalli/Pandas-Data-Science-Tasks/master/SalesAnalysis/Sales_Data/\"\n\n# List of CSV file names\nfile_names = [\n    \"Sales_April_2019.csv\",\n    \"Sales_August_2019.csv\",\n    \"Sales_December_2019.csv\",\n    \"Sales_February_2019.csv\",\n    \"Sales_January_2019.csv\",\n    \"Sales_July_2019.csv\",\n    \"Sales_June_2019.csv\",\n    \"Sales_March_2019.csv\",\n    \"Sales_May_2019.csv\",\n    \"Sales_November_2019.csv\",\n    \"Sales_October_2019.csv\",\n    \"Sales_September_2019.csv\"\n]\n\ncombined_file_name = \"combined_sales_data\"\n\ndownload_and_combine_csv(base_url, file_names, combined_file_name)\n\nAll data saved in Feather file.\n\n\n\nfeather_file = 'combined_sales_data.feather'\n\n# Load the data from the Feather file\nsales_data = pd.read_feather(feather_file)\n\nsales_data\n\n\n\n\n\n\n\n\nOrder ID\nProduct\nQuantity Ordered\nPrice Each\nOrder Date\nPurchase Address\n\n\n\n\n0\n176558\nUSB-C Charging Cable\n2\n11.95\n04/19/19 08:46\n917 1st St, Dallas, TX 75001\n\n\n1\nNone\nNone\nNone\nNone\nNone\nNone\n\n\n2\n176559\nBose SoundSport Headphones\n1\n99.99\n04/07/19 22:30\n682 Chestnut St, Boston, MA 02215\n\n\n3\n176560\nGoogle Phone\n1\n600\n04/12/19 14:38\n669 Spruce St, Los Angeles, CA 90001\n\n\n4\n176560\nWired Headphones\n1\n11.99\n04/12/19 14:38\n669 Spruce St, Los Angeles, CA 90001\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n11681\n259353\nAAA Batteries (4-pack)\n3\n2.99\n09/17/19 20:56\n840 Highland St, Los Angeles, CA 90001\n\n\n11682\n259354\niPhone\n1\n700\n09/01/19 16:00\n216 Dogwood St, San Francisco, CA 94016\n\n\n11683\n259355\niPhone\n1\n700\n09/23/19 07:39\n220 12th St, San Francisco, CA 94016\n\n\n11684\n259356\n34in Ultrawide Monitor\n1\n379.99\n09/19/19 17:30\n511 Forest St, San Francisco, CA 94016\n\n\n11685\n259357\nUSB-C Charging Cable\n1\n11.95\n09/30/19 00:18\n250 Meadow St, San Francisco, CA 94016\n\n\n\n\n186850 rows √ó 6 columns\n\n\n\n\nsales_data.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 186850 entries, 0 to 11685\nData columns (total 6 columns):\n #   Column            Non-Null Count   Dtype \n---  ------            --------------   ----- \n 0   Order ID          186305 non-null  object\n 1   Product           186305 non-null  object\n 2   Quantity Ordered  186305 non-null  object\n 3   Price Each        186305 non-null  object\n 4   Order Date        186305 non-null  object\n 5   Purchase Address  186305 non-null  object\ndtypes: object(6)\nmemory usage: 10.0+ MB\n\n\nThe function tweak_data is designed to clean and format a DataFrame data that contains sales data. Here‚Äôs a concise explanation of each step in the function:\n\nRename Columns:\n\n.rename(columns=lambda x: x.strip().replace(' ', '_')): This renames the columns by removing any leading or trailing whitespace and replacing spaces with underscores. This step makes column names more consistent and easier to reference in code.\n\nConvert Columns to Numeric:\n\n.assign(Quantity_Ordered = lambda x: pd.to_numeric(x['Quantity_Ordered'], errors='coerce'), Price_Each = lambda x: pd.to_numeric(x['Price_Each'], errors='coerce')): This converts the ‚ÄòQuantity_Ordered‚Äô and ‚ÄòPrice_Each‚Äô columns to numeric types, handling any non-numeric values by turning them into NaNs (errors='coerce').\n\nDrop NaN Values:\n\n.dropna(): Removes any rows with NaN values, ensuring that the DataFrame only contains complete data.\n\nConvert ‚ÄòOrder_Date‚Äô to DateTime and Set as Index:\n\n.assign(Order_Date = lambda x: pd.to_datetime(x['Order_Date'], format='%m/%d/%y %H:%M'), errors='coerce'): Converts the ‚ÄòOrder_Date‚Äô column to a datetime object using the specified format. If any values cannot be converted, it coerces them to NaN.\n.set_index('Order_Date'): Sets the ‚ÄòOrder_Date‚Äô column as the index of the DataFrame.\n\nSort DataFrame by Index:\n\n.sort_index(): Sorts the DataFrame by the ‚ÄòOrder_Date‚Äô index in ascending order.\n\n\nBy calling tweaked_sales_data = tweak_data(sales_data), the function is applied to a DataFrame sales_data, performing all these cleaning and formatting steps, and the result is stored in tweaked_sales_data. This processed DataFrame is now more suitable for analysis, with cleaner column names, proper data types, and a sorted datetime index.\n\ndef tweak_data(data):\n    return (data\n            .rename(columns=lambda x: x.strip()\n            .replace(' ', '_'))\n            .assign(Quantity_Ordered = lambda x: pd.to_numeric(x['Quantity_Ordered'], errors='coerce'),\n                   Price_Each = lambda x: pd.to_numeric(x['Price_Each'], errors='coerce'))\n            .dropna()\n            .assign(Order_Date = lambda x: pd.to_datetime(x['Order_Date'], format='%m/%d/%y %H:%M'))\n            .set_index('Order_Date')\n            .sort_index()\n           )\n\ntweaked_sales_data = tweak_data(sales_data)\ntweaked_sales_data\n\n\n\n\n\n\n\n\nOrder_ID\nProduct\nQuantity_Ordered\nPrice_Each\nPurchase_Address\n\n\nOrder_Date\n\n\n\n\n\n\n\n\n\n2019-01-01 03:07:00\n147268\nWired Headphones\n1.0\n11.99\n9 Lake St, New York City, NY 10001\n\n\n2019-01-01 03:40:00\n148041\nUSB-C Charging Cable\n1.0\n11.95\n760 Church St, San Francisco, CA 94016\n\n\n2019-01-01 04:56:00\n149343\nApple Airpods Headphones\n1.0\n150.00\n735 5th St, New York City, NY 10001\n\n\n2019-01-01 05:53:00\n149964\nAAA Batteries (4-pack)\n1.0\n2.99\n75 Jackson St, Dallas, TX 75001\n\n\n2019-01-01 06:03:00\n149350\nUSB-C Charging Cable\n2.0\n11.95\n943 2nd St, Atlanta, GA 30301\n\n\n...\n...\n...\n...\n...\n...\n\n\n2020-01-01 04:13:00\n304165\nAAA Batteries (4-pack)\n1.0\n2.99\n825 Adams St, Portland, OR 97035\n\n\n2020-01-01 04:21:00\n299125\nUSB-C Charging Cable\n1.0\n11.95\n754 Hickory St, New York City, NY 10001\n\n\n2020-01-01 04:54:00\n305840\nBose SoundSport Headphones\n1.0\n99.99\n784 River St, San Francisco, CA 94016\n\n\n2020-01-01 05:13:00\n300519\nBose SoundSport Headphones\n1.0\n99.99\n657 Spruce St, New York City, NY 10001\n\n\n2020-01-01 05:13:00\n300519\nLightning Charging Cable\n1.0\n14.95\n657 Spruce St, New York City, NY 10001\n\n\n\n\n185950 rows √ó 5 columns\n\n\n\nEyeballing the output, we can discern that the data pertains to sales transactions for the year 2019. This quick visual inspection helps us grasp the overarching theme of the dataset, setting the stage for a more detailed analysis and summarization, particularly focusing on the highest-performing products.\n\ndef prep4plot(data):\n    return (data\n     .groupby('Product')\n     [['Price_Each', 'Quantity_Ordered']]\n     .agg({'Price_Each' : 'sum', 'Quantity_Ordered':'count'})\n     .sort_values(by='Price_Each', ascending=False)\n     .reset_index()\n     .rename(columns={'Price_Each': 'Sales', 'Quantity_Ordered': 'Purchases'})\n    )\n\ndata4plot = prep4plot(tweaked_sales_data)\ndata4plot.head()\n\n\n\n\n\n\n\n\nProduct\nSales\nPurchases\n\n\n\n\n0\nMacbook Pro Laptop\n8030800.00\n4724\n\n\n1\niPhone\n4789400.00\n6842\n\n\n2\nThinkPad Laptop\n4127958.72\n4128\n\n\n3\nGoogle Phone\n3315000.00\n5525\n\n\n4\n27in 4K Gaming Monitor\n2429637.70\n6230\n\n\n\n\n\n\n\nThis code snippet performs several data manipulation and aggregation tasks on the tweaked_sales_data DataFrame:\n\nGrouping by ‚ÄòProduct‚Äô: .groupby('Product') groups the DataFrame by unique values in the ‚ÄòProduct‚Äô column. Each group corresponds to a different product.\nSelecting Columns for Aggregation: [['Price_Each', 'Quantity_Ordered']] selects two columns for subsequent operations - Price_Each and Quantity_Ordered.\nAggregating Data:\n\n.agg({'Price_Each' : 'sum', 'Quantity_Ordered':'count'}): This aggregates the data within each product group. It calculates the sum of Price_Each and the count of Quantity_Ordered for each product. Essentially, it sums up total sales and counts the number of orders per product.\n\nSorting Results:\n\n.sort_values(by='Price_Each', ascending=False): This sorts the aggregated results in descending order based on the ‚ÄòPrice_Each‚Äô column, which, after aggregation, represents the total sales per product.\n\nResetting Index:\n\n.reset_index(): This resets the index of the DataFrame. After grouping and sorting, the index might be in a custom state (like product names). Resetting the index turns it back to a simple numerical range index.\n\nRenaming Columns:\n\n.rename(columns={'Price_Each': 'Sales', 'Quantity_Ordered': 'Purchases'}): This renames the columns for clarity. ‚ÄòPrice_Each‚Äô is renamed to ‚ÄòSales‚Äô, indicating total sales revenue per product, and ‚ÄòQuantity_Ordered‚Äô to ‚ÄòPurchases‚Äô, indicating the number of purchases of each product.\n\n\nThe final result of this snippet is a neatly organized DataFrame that provides a summarized view of sales data, showing the total sales revenue and the number of purchases for each product, sorted by the total sales revenue in descending order.\nThe first step is to plot the data using the standard pandas plotting function. We will choose a nice and simple plot style too.\n\nprint(plt.style.available)\n\n['Solarize_Light2', '_classic_test_patch', '_mpl-gallery', '_mpl-gallery-nogrid', 'bmh', 'classic', 'dark_background', 'fast', 'fivethirtyeight', 'ggplot', 'grayscale', 'seaborn-v0_8', 'seaborn-v0_8-bright', 'seaborn-v0_8-colorblind', 'seaborn-v0_8-dark', 'seaborn-v0_8-dark-palette', 'seaborn-v0_8-darkgrid', 'seaborn-v0_8-deep', 'seaborn-v0_8-muted', 'seaborn-v0_8-notebook', 'seaborn-v0_8-paper', 'seaborn-v0_8-pastel', 'seaborn-v0_8-poster', 'seaborn-v0_8-talk', 'seaborn-v0_8-ticks', 'seaborn-v0_8-white', 'seaborn-v0_8-whitegrid', 'tableau-colorblind10']\n\n\n\nplt.style.use('ggplot')\n\n\n_=data4plot.plot(kind='barh', y=\"Sales\", x=\"Product\")\n\n\n\n\n\n\n\n\nThe code snippet _=data4plot.plot(kind='barh', y=\"Sales\", x=\"Product\") is a Pandas plotting command with several components:\n\ndata4plot: This is a Pandas DataFrame containing the data we want to plot that has the two columns named Sales and Product.\n.plot(): This is a method associated with Pandas DataFrames used for generating various types of plots.\nkind='barh': This parameter specifies the kind of plot to generate. In this case, 'barh' stands for horizontal bar plot. This plot type is useful for comparing quantities across different categories, which are displayed as bars running horizontally.\ny=\"Sales\": This parameter sets the column in the DataFrame that will be used for the plot‚Äôs vertical axis. In this plot, it means that the lengths of the horizontal bars will represent the values in the ‚ÄòSales‚Äô column.\nx=\"Product\": This sets the column to be used for the horizontal axis labels. Here, the labels on the y-axis (since it‚Äôs a horizontal bar plot) will be taken from the ‚ÄòProduct‚Äô column. Each product will have a corresponding bar.\n_=: This is a common Python idiom for assigning the output of a function to a dummy variable (_) when the output is not needed. In this case, the plot function returns a matplotlib object. If you‚Äôre just displaying the plot and don‚Äôt need to further manipulate or access this object, it‚Äôs common to assign it to _ to indicate that it can be ignored. We use this basically to suppress some outputs that tend to come with matplotlib plots. The semi-colon ; also achieves the same as we can see later in our code.\n\nPandas‚Äô built-in plotting capabilities are used here mainly due to their simplicity and efficiency in creating preliminary visualizations. Given that Pandas is commonly used for preliminary data handling and analysis, it‚Äôs practical to leverage its straightforward plotting tools for initial graphical representations. This approach allows for a swift and effortless transition from data manipulation to visualization, making it an ideal starting point for most users.\nEnhancing our Visualization\nOnce we have grasped the basics of creating a plot using Pandas, the next logical step is to enhance and personalize it. Simple enhancements, such as adding titles and labels, are straightforward with Pandas‚Äô plotting features. However, as our visualization needs evolve, we may find ourselves seeking more advanced customization options. This is why we have to familiarize ourselves with more extensive customization techniques early on. Doing so will equip us with the skills to fully tailor our visualizations beyond the basic offerings of Pandas‚Äô plotting functions.\n\nfig, ax = plt.subplots()\n\n_=data4plot.plot(kind='barh', y=\"Sales\", x=\"Product\", ax=ax)\n\n\n\n\n\n\n\n\nUpon creating the initial plot with Pandas, we enhance it by integrating plt.subplots(), a Matplotlib function, to gain more customization control. This approach might produce a plot visually identical to the original Pandas plot, but the significant difference lies in the newfound access to Matplotlib‚Äôs ax and fig objects. This access is crucial for detailed customization and control over the plot‚Äôs aspects.\nThe integration of plt.subplots() allows us to tap into Matplotlib‚Äôs extensive capabilities while retaining the simplicity and speed of Pandas‚Äô plotting. This hybrid method is not only efficient but also versatile, making it easier to apply and adapt various customization techniques to suit specific requirements.\nFor instance, consider modifying the x-axis limits or altering axis labels. With the ax variable now encapsulating the plot‚Äôs axes, we unlock a greater degree of control over these elements. This method of combining Pandas‚Äô ease of use with Matplotlib‚Äôs detailed customization offers the best of both worlds in data visualization.\n\nfig, ax = plt.subplots()\ndata4plot.plot(kind='barh', y=\"Sales\", x=\"Product\", ax=ax)\nax.set_xlim([-10000, 9000000])\nax.set_xlabel('Total Sales')\nax.set_ylabel('Product')\nax.set_title('2019 Sales');\n\n\n\n\n\n\n\n\nOr we do it another way:\n\nfig, ax = plt.subplots()\ndata4plot.plot(kind='barh', y=\"Sales\", x=\"Product\", ax=ax)\nax.set_xlim([-10000, 9000000])\nax.set(title='2019 Sales', xlabel='Total Sales', ylabel='Product');\n\n\n\n\n\n\n\n\nThis method showcases the flexibility of the plt.subplots() function from Matplotlib in fine-tuning our visualization. Specifically, it allows for the customization of the plot‚Äôs size by setting the figsize parameter, which is defined in inches. This level of control over the dimensions of our plot ensures that it fits the intended context or presentation space perfectly. Additionally, the functionality to modify the plot extends to aspects like the legend. With the command ax.legend().set_visible(False), we can effortlessly hide the legend if it‚Äôs unnecessary or distracting, thus maintaining a focus on the most crucial elements of our visualization. This approach underscores the advantage of blending simplicity with the powerful customization capabilities of Matplotlib.\n\nfig, ax = plt.subplots(figsize=(5, 6))\ndata4plot.plot(kind='barh', y=\"Sales\", x=\"Product\", ax=ax)\nax.set_xlim([-10000, 9000000])\nax.set(title='2019 Sales', xlabel='Total Sales')\nax.legend().set_visible(False)\n\n\n\n\n\n\n\n\nTo enhance the visual appeal of our plot, one key area to focus on is the presentation of the ‚ÄúTotal Sales‚Äù figures. Matplotlib offers a powerful tool in the form of FuncFormatter, which allows for the application of custom functions to axis values, effectively enabling tailored formatting. This feature is particularly useful for rendering numerical data in a more readable and context-appropriate format.\nConsider, for example, a function designed to format currency, especially useful for figures in the higher ranges, such as several hundred thousand dollars. By applying such a function through FuncFormatter, ‚ÄúTotal Sales‚Äù values can be displayed in a format that is both aesthetically pleasing and easily comprehensible, ensuring that your plot communicates its data as effectively as possible. This approach illustrates the advantage of using Matplotlib‚Äôs advanced features to refine and customize the details of your data visualization.\n\nfrom matplotlib.ticker import FuncFormatter\n\n\ndef format_currency(value, position):\n    \"\"\"Format the currency based on its magnitude.\n\n    Args:\n    value (float): The numerical value to format.\n    position (int): The tick position (not used in this function).\n\n    Returns:\n    str: Formatted currency string as millions if value is &gt;= 1,000,000.\n    \"\"\"\n    if value &gt;= 1000000:\n        return f'${value*1e-6:1.1f}M'\n    \n    # No formatting provided for values under 1,000,000\n    return str(value)\n\n\nfig, ax = plt.subplots()\ndata4plot.plot(kind='barh', y=\"Sales\", x=\"Product\", ax=ax)\nax.set_xlim([-10000, 9000000])\nax.set(title='2019 Sales', xlabel='Total Sales', ylabel='Product')\nformatter = FuncFormatter(format_currency)\nax.xaxis.set_major_formatter(formatter)\nax.legend().set_visible(False);\n\n\n\n\n\n\n\n\nThis refined approach really illustrates the versatility and customizability at our disposal. The next feature to explore in customizing our plot is adding annotations, which can significantly enhance the information communicated by our visualization.\nTo illustrate this, consider using ax.axvline() to draw a vertical line on the plot. This can be particularly effective for highlighting average values or specific thresholds. Additionally, incorporating custom text annotations on the plot can be achieved with ax.text(). This allows for labeling specific points of interest, such as highlighting the introduction of new customers in our example.\nThe full code, complete with annotations and comments, demonstrates how these elements come together to create a more informative and visually appealing plot. This use of annotations and vertical lines not only enhances the visual clarity of the plot but also adds layers of context and meaning, making the data more accessible and understandable.\n\n# Create the figure and the axes\nfig, ax = plt.subplots()\n\n# Plot the data and get the averaged\ndata4plot.plot(kind='barh', y=\"Sales\", x=\"Product\", ax=ax)\navg = data4plot['Sales'].mean()\n\n# Set limits and labels\nax.set_xlim([-10000, 9000000])\nax.set(title='2019 Sales', xlabel='Total Sales', ylabel='Product')\n\n# Add a line for the average\nax.axvline(x=avg, color='burlywood', label='Average', linestyle='--', linewidth=2)\n\n# Annotate the average Sales\nfor cust in [6, 12, 16]:\n    ax.text(2_000_000, cust, \"Above average Sales\")\n\n# Format the currency\nformatter = FuncFormatter(format_currency)\nax.xaxis.set_major_formatter(formatter)\n\n# Hide the legend\nax.legend().set_visible(False)\n\n\n\n\n\n\n\n\n\n# Create the figure and the axes\nfig, ax = plt.subplots()\n\n# Plot the data and get the averaged\ndata4plot.plot(kind='barh', y=\"Sales\", x=\"Product\", ax=ax)\navg = data4plot['Sales'].mean()\n\n# Set limits and labels\nax.set_xlim([-10000, 9000000])\nax.set(title='2019 Sales', xlabel='Total Sales', ylabel='Product')\n\n# Add a line for the average\nax.axvline(x=avg, color='burlywood', label='Average', linestyle='--', linewidth=2)\n\n# Annotate with an arrow\nax.annotate('Average Sales', xy=(avg, 0), xytext=(avg + 2_000_000, 10),\n             arrowprops=dict(facecolor='black', shrink=0.05),\n             horizontalalignment='right', verticalalignment='top')\n\n\n# Format the currency\nformatter = FuncFormatter(format_currency)\nax.xaxis.set_major_formatter(formatter)\n\n# Hide the legend\nax.legend().set_visible(False)\n\n\n\n\n\n\n\n\n\nFunction Call:\n\nax.annotate(): This method is used to add an annotation to the plot.\n\nAnnotation Text:\n\n'Average Sales': This is the text that will appear in the annotation.\n\nAnnotation Position:\n\nxy=(avg, 0): The xy argument specifies the point (as a tuple) to be annotated. In this case, it‚Äôs the average sales value (avg) on the x-axis, and 0 on the y-axis.\nxytext=(avg + 2_000_000, 10): The xytext argument determines where the text part of the annotation will be placed. Here, it‚Äôs set to be 2,000,000 units right from the avg on the x-axis and 10 units up on the y-axis.\n\nArrow Properties:\n\narrowprops=dict(facecolor='black', shrink=0.05): This dictionary defines the appearance of the arrow. facecolor sets the color of the arrow, and shrink slightly shrinks the arrow from both the annotated point and the text for better visibility.\n\nText Alignment:\n\nhorizontalalignment='right', verticalalignment='top': These parameters control how the text is aligned relative to the xytext point.\n\n\nAdjusting the Annotation\n\nTo move the annotation text, adjust the xytext coordinates. Changing the first number moves the text left/right, and changing the second number moves it up/down.\nTo point the arrow to a different location, change the xy coordinates.\nYou can change the appearance of the arrow by modifying the arrowprops dictionary. For instance, changing facecolor will change the color of the arrow.\nThe alignment of the text can be adjusted with horizontalalignment and verticalalignment. Options include ‚Äòleft‚Äô, ‚Äòright‚Äô, ‚Äòtop‚Äô, ‚Äòbottom‚Äô, ‚Äòcenter‚Äô, etc.\n\nUp to this point, our modifications have been focused on individual plots. However, our capabilities extend to incorporating multiple plots within a single figure, as well as various options to save the entire figure. Let‚Äôs explore how we can achieve this.\nTo place two plots on a single figure, we‚Äôll start by creating the figure and its axes using plt.subplots():\nfig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, sharey=True, figsize=(7, 4))\nIn this instance, we‚Äôre specifying the layout of our figure with nrows and ncols, which determine the number of rows and columns of subplots, respectively. We‚Äôve chosen to create one row and two columns, thus positioning our two plots side by side. For clarity, especially for those new to Matplotlib, explicitly using these named parameters can make the code more intuitive and easier to understand when revisited.\nBy setting sharey=True, we ensure that both plots (ax1 and ax2) share the same y-axis labels, which is particularly useful for comparing plots with the same scale or data range.\nOnce we‚Äôve established our figure structure and shared axes, we can proceed to add our plots. Plotting on ax1 and ax2 allows us to place different visualizations side by side within the same figure, enhancing our ability to compare and contrast the data presented. This approach not only broadens our visualization toolkit but also encourages a more comprehensive analysis through side-by-side comparisons.\n\n# Get the figure and the axes\nfig, (ax1, ax2) = plt.subplots(nrows=1,ncols=2, sharey=True, figsize=(7, 4))\n\n# Plot the data and get the averaged\ndata4plot.plot(kind='barh', y=\"Sales\", x=\"Product\", ax=ax1)\n\nax1.set_xlim([-10000, 9000000])\nax1.set(title='2019 Sales', xlabel='Total Sales', ylabel='Product')\n\n# Plot the average as a vertical line\navg = data4plot['Sales'].mean()\nax1.axvline(x=avg, color='purple', label='Average', linestyle='--', linewidth=2)\n\n# Repeat for the unit plot\ndata4plot.plot(kind='barh', y=\"Purchases\", x=\"Product\", ax=ax2)\navg = data4plot['Purchases'].mean()\nax2.set(title='Units', xlabel='Total Units', ylabel='')\nax2.axvline(x=avg, color='purple', label='Average', linestyle='--', linewidth=2)\n\n# Title the figure\nfig.suptitle('2019 Sales Analysis', fontsize=14, fontweight='bold');\n\n# Hide the legends\nax1.legend().set_visible(False)\nax2.legend().set_visible(False)"
  },
  {
    "objectID": "blog/2024-February-24-Introduction-to-Matplotlib/Introduction_to_matplotlib.html#conclusion",
    "href": "blog/2024-February-24-Introduction-to-Matplotlib/Introduction_to_matplotlib.html#conclusion",
    "title": "An introduction to Matplotlib",
    "section": "Conclusion",
    "text": "Conclusion\nThis guide aims to enhance your proficiency in employing Matplotlib for daily data analysis tasks. By adopting this methodical approach in your analytical endeavors, you‚Äôll likely gain the agility and know-how necessary for tailoring your visualizations to your specific requirements. Writing this piece was a deeply fulfilling journey for me, born from a longstanding aspiration. It‚Äôs my hope that you find as much enjoyment and value in reading and applying these insights as I did in composing and illustrating them."
  },
  {
    "objectID": "blog/2024-January-04-Karachi-Property-Prices/KarachiPropertyPrices.html",
    "href": "blog/2024-January-04-Karachi-Property-Prices/KarachiPropertyPrices.html",
    "title": "Data Wrangling: Karachi Property Prices",
    "section": "",
    "text": "This dataset encompasses a collection of 8,414 housing advertisements sourced from Zameen.com, specifically pertaining to properties located in Karachi. The dataset is available on Kaggle, a popular platform for data science and machine learning enthusiasts. It can be accessed through the following link: Karachi, Pakistan Property Prices 2023 .\nIt‚Äôs important to note that this dataset represents only a fraction of the comprehensive data available. It was meticulously scraped and compiled by Faiq Ali, who, at the time, was a student at the University of Malaya. This dataset, listed on his Kaggle page, provides valuable insights into the real estate market of Karachi as of the year 2023. We will try to wrangle the data and prepare it for machine learning.\n\nimport pandas as pd\n\n\nfile_path = \"karachi-pakistan-property-prices-2023.csv\" \n\n\ndef prep_karachi_data(file_path):\n    return (pd.read_csv(file_path)\n           )\n\ndf = prep_karachi_data(file_path=file_path)\nprint(df.info())\ndf.head()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 8414 entries, 0 to 8413\nData columns (total 12 columns):\n #   Column             Non-Null Count  Dtype \n---  ------             --------------  ----- \n 0   title              8414 non-null   object\n 1   price              8413 non-null   object\n 2   date added         8047 non-null   object\n 3   type               8413 non-null   object\n 4   bedrooms           8414 non-null   int64 \n 5   bathrooms          8414 non-null   int64 \n 6   area               7322 non-null   object\n 7   location           8047 non-null   object\n 8   complete location  8413 non-null   object\n 9   description        8413 non-null   object\n 10  keywords           7551 non-null   object\n 11  url                8414 non-null   object\ndtypes: int64(2), object(10)\nmemory usage: 788.9+ KB\nNone\n\n\n\n\n\n\n\n\n\ntitle\nprice\ndate added\ntype\nbedrooms\nbathrooms\narea\nlocation\ncomplete location\ndescription\nkeywords\nurl\n\n\n\n\n0\n600 Yard Bungalow For Sale In DHA Phase 6\n11.5 Crore\n14 hours ago\nHouse\n5\n6\n600 Sq. Yd.\nDHA Defence, Karachi, Sindh\nDHA Phase 6, DHA Defence, Karachi, Sindh\nChance Deal 600 Yard Bungalow For Sale\nBuilt in year: 1,Parking Spaces: 5,Flooring,Ot...\nhttps://www.zameen.com/Property/d_h_a_dha_phas...\n\n\n1\n3 BEDS LUXURY 125 SQ YARDS VILLA FOR SALE LOCA...\n1.45 Crore\n5 hours ago\nHouse\n0\n0\nNaN\nBahria Town Karachi, Karachi, Sindh\nBahria Town - Ali Block, Bahria Town - Precinc...\n3 BEDS LUXURY 125 SQ YARDS VILLA FOR SALE LOCA...\nBedrooms: 3,Bathrooms: 3,Kitchens: 2\nhttps://www.zameen.com/Property/bahria_town_pr...\n\n\n2\n4 BEDS LUXURY SPORTS CITY VILLA FOR RENT BAHRI...\n2.12 Crore\n5 hours ago\nHouse\n0\n0\nNaN\nBahria Town Karachi, Karachi, Sindh\nBahria Sports City, Bahria Town Karachi, Karac...\n4 BEDS LUXURY SPORTS CITY VILLA FOR RENT BAHRI...\nBedrooms: 4,Bathrooms: 4,Kitchens: 2\nhttps://www.zameen.com/Property/bahria_town_ka...\n\n\n3\n3 BEDS LUXURY 235 SQ YARDS VILLA FOR SALE LOCA...\n1.5 Crore\n5 hours ago\nHouse\n0\n0\nNaN\nBahria Town Karachi, Karachi, Sindh\nBahria Town - Precinct 31, Bahria Town Karachi...\n3 BEDS LUXURY 235 SQ YARDS VILLA FOR SALE LOCA...\nBedrooms: 3,Bathrooms: 3,Kitchens: 2\nhttps://www.zameen.com/Property/bahria_town_ka...\n\n\n4\nBuying A Flat In Clifton - Block 9?\n4 Crore\n6 hours ago\nFlat\n3\n3\n200 Sq. Yd.\nClifton, Karachi, Sindh\nClifton - Block 9, Clifton, Karachi, Sindh\nApartment for sale\nFlooring,Electricity Backup,Broadband Internet...\nhttps://www.zameen.com/Property/clifton_clifto...\n\n\n\n\n\n\n\nOne of the first things we are going to do is to change the name of some of the columns. I want to get rid of spaces.\n\ndef prep_karachi_data(file_path):\n    return (pd.read_csv(file_path)\n            .rename(columns=lambda x: x.replace(\" \", \"_\")) #no spaces in column names\n           )\n\ndf = prep_karachi_data(file_path=file_path)\nprint(df.info())\ndf.head()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 8414 entries, 0 to 8413\nData columns (total 12 columns):\n #   Column             Non-Null Count  Dtype \n---  ------             --------------  ----- \n 0   title              8414 non-null   object\n 1   price              8413 non-null   object\n 2   date_added         8047 non-null   object\n 3   type               8413 non-null   object\n 4   bedrooms           8414 non-null   int64 \n 5   bathrooms          8414 non-null   int64 \n 6   area               7322 non-null   object\n 7   location           8047 non-null   object\n 8   complete_location  8413 non-null   object\n 9   description        8413 non-null   object\n 10  keywords           7551 non-null   object\n 11  url                8414 non-null   object\ndtypes: int64(2), object(10)\nmemory usage: 788.9+ KB\nNone\n\n\n\n\n\n\n\n\n\ntitle\nprice\ndate_added\ntype\nbedrooms\nbathrooms\narea\nlocation\ncomplete_location\ndescription\nkeywords\nurl\n\n\n\n\n0\n600 Yard Bungalow For Sale In DHA Phase 6\n11.5 Crore\n14 hours ago\nHouse\n5\n6\n600 Sq. Yd.\nDHA Defence, Karachi, Sindh\nDHA Phase 6, DHA Defence, Karachi, Sindh\nChance Deal 600 Yard Bungalow For Sale\nBuilt in year: 1,Parking Spaces: 5,Flooring,Ot...\nhttps://www.zameen.com/Property/d_h_a_dha_phas...\n\n\n1\n3 BEDS LUXURY 125 SQ YARDS VILLA FOR SALE LOCA...\n1.45 Crore\n5 hours ago\nHouse\n0\n0\nNaN\nBahria Town Karachi, Karachi, Sindh\nBahria Town - Ali Block, Bahria Town - Precinc...\n3 BEDS LUXURY 125 SQ YARDS VILLA FOR SALE LOCA...\nBedrooms: 3,Bathrooms: 3,Kitchens: 2\nhttps://www.zameen.com/Property/bahria_town_pr...\n\n\n2\n4 BEDS LUXURY SPORTS CITY VILLA FOR RENT BAHRI...\n2.12 Crore\n5 hours ago\nHouse\n0\n0\nNaN\nBahria Town Karachi, Karachi, Sindh\nBahria Sports City, Bahria Town Karachi, Karac...\n4 BEDS LUXURY SPORTS CITY VILLA FOR RENT BAHRI...\nBedrooms: 4,Bathrooms: 4,Kitchens: 2\nhttps://www.zameen.com/Property/bahria_town_ka...\n\n\n3\n3 BEDS LUXURY 235 SQ YARDS VILLA FOR SALE LOCA...\n1.5 Crore\n5 hours ago\nHouse\n0\n0\nNaN\nBahria Town Karachi, Karachi, Sindh\nBahria Town - Precinct 31, Bahria Town Karachi...\n3 BEDS LUXURY 235 SQ YARDS VILLA FOR SALE LOCA...\nBedrooms: 3,Bathrooms: 3,Kitchens: 2\nhttps://www.zameen.com/Property/bahria_town_ka...\n\n\n4\nBuying A Flat In Clifton - Block 9?\n4 Crore\n6 hours ago\nFlat\n3\n3\n200 Sq. Yd.\nClifton, Karachi, Sindh\nClifton - Block 9, Clifton, Karachi, Sindh\nApartment for sale\nFlooring,Electricity Backup,Broadband Internet...\nhttps://www.zameen.com/Property/clifton_clifto...\n\n\n\n\n\n\n\nWe will bring out the second column price for further investigation. We will want it to be a number but we can see that it is an object.\n\ndf['price']\n\n0       11.5 Crore\n1       1.45 Crore\n2       2.12 Crore\n3        1.5 Crore\n4          4 Crore\n           ...    \n8409       5 Crore\n8410     1.2 Crore\n8411    1.55 Crore\n8412       70 Lakh\n8413       1 Crore\nName: price, Length: 8414, dtype: object\n\n\nThis seems to have a combination of units.\n‚ÄúLakh‚Äù and ‚ÄúCrore‚Äù are units of numerical value commonly used in the Indian subcontinent, including countries like India, Pakistan, Bangladesh, and Nepal. They are part of the South Asian numbering system and are widely used in these regions for financial transactions, population counts, and more.\n\nLakh:\n\nOne lakh is equal to 100,000 (10^5).\nFor example, in international notation, 5 lakh would be written as 500,000.\n\nCrore:\n\nOne crore is equal to 10 million, or 100 lakh (10^7).\nIn international notation, 1 crore would be expressed as 10,000,000.\n\n\nThese terms provide a more convenient way to express large numbers, particularly in the context of financial transactions and population statistics in the Indian subcontinent. For instance, it‚Äôs more common to hear about a budget of 5 crore rupees rather than 50 million rupees.\nWe will split that column into two, one for the figures and the other for the units. The values and the units are separated by a space.\n\ndf['price'].str.split(\" \", expand=True)\n\n\n\n\n\n\n\n\n0\n1\n\n\n\n\n0\n11.5\nCrore\n\n\n1\n1.45\nCrore\n\n\n2\n2.12\nCrore\n\n\n3\n1.5\nCrore\n\n\n4\n4\nCrore\n\n\n...\n...\n...\n\n\n8409\n5\nCrore\n\n\n8410\n1.2\nCrore\n\n\n8411\n1.55\nCrore\n\n\n8412\n70\nLakh\n\n\n8413\n1\nCrore\n\n\n\n\n8414 rows √ó 2 columns\n\n\n\nWe will now include that in our function that we are slowly building. We will also convert the values in our new column to a float.\n\ndef prep_karachi_data(file_path):\n    return (pd.read_csv(file_path)\n            .rename(columns=lambda x: x.replace(\" \", \"_\")) #no spaces in column names\n            .assign(price_ = lambda x: x[\"price\"].str.split(\" \", expand=True)[0],\n                  currency_name = lambda x: x[\"price\"].str.split(\" \", expand=True)[1])\n            .astype({\"price_\":float})\n           )\n\ndf = prep_karachi_data(file_path=file_path)\nprint(df.info())\ndf.head()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 8414 entries, 0 to 8413\nData columns (total 14 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   title              8414 non-null   object \n 1   price              8413 non-null   object \n 2   date_added         8047 non-null   object \n 3   type               8413 non-null   object \n 4   bedrooms           8414 non-null   int64  \n 5   bathrooms          8414 non-null   int64  \n 6   area               7322 non-null   object \n 7   location           8047 non-null   object \n 8   complete_location  8413 non-null   object \n 9   description        8413 non-null   object \n 10  keywords           7551 non-null   object \n 11  url                8414 non-null   object \n 12  price_             8413 non-null   float64\n 13  currency_name      8413 non-null   object \ndtypes: float64(1), int64(2), object(11)\nmemory usage: 920.4+ KB\nNone\n\n\n\n\n\n\n\n\n\ntitle\nprice\ndate_added\ntype\nbedrooms\nbathrooms\narea\nlocation\ncomplete_location\ndescription\nkeywords\nurl\nprice_\ncurrency_name\n\n\n\n\n0\n600 Yard Bungalow For Sale In DHA Phase 6\n11.5 Crore\n14 hours ago\nHouse\n5\n6\n600 Sq. Yd.\nDHA Defence, Karachi, Sindh\nDHA Phase 6, DHA Defence, Karachi, Sindh\nChance Deal 600 Yard Bungalow For Sale\nBuilt in year: 1,Parking Spaces: 5,Flooring,Ot...\nhttps://www.zameen.com/Property/d_h_a_dha_phas...\n11.50\nCrore\n\n\n1\n3 BEDS LUXURY 125 SQ YARDS VILLA FOR SALE LOCA...\n1.45 Crore\n5 hours ago\nHouse\n0\n0\nNaN\nBahria Town Karachi, Karachi, Sindh\nBahria Town - Ali Block, Bahria Town - Precinc...\n3 BEDS LUXURY 125 SQ YARDS VILLA FOR SALE LOCA...\nBedrooms: 3,Bathrooms: 3,Kitchens: 2\nhttps://www.zameen.com/Property/bahria_town_pr...\n1.45\nCrore\n\n\n2\n4 BEDS LUXURY SPORTS CITY VILLA FOR RENT BAHRI...\n2.12 Crore\n5 hours ago\nHouse\n0\n0\nNaN\nBahria Town Karachi, Karachi, Sindh\nBahria Sports City, Bahria Town Karachi, Karac...\n4 BEDS LUXURY SPORTS CITY VILLA FOR RENT BAHRI...\nBedrooms: 4,Bathrooms: 4,Kitchens: 2\nhttps://www.zameen.com/Property/bahria_town_ka...\n2.12\nCrore\n\n\n3\n3 BEDS LUXURY 235 SQ YARDS VILLA FOR SALE LOCA...\n1.5 Crore\n5 hours ago\nHouse\n0\n0\nNaN\nBahria Town Karachi, Karachi, Sindh\nBahria Town - Precinct 31, Bahria Town Karachi...\n3 BEDS LUXURY 235 SQ YARDS VILLA FOR SALE LOCA...\nBedrooms: 3,Bathrooms: 3,Kitchens: 2\nhttps://www.zameen.com/Property/bahria_town_ka...\n1.50\nCrore\n\n\n4\nBuying A Flat In Clifton - Block 9?\n4 Crore\n6 hours ago\nFlat\n3\n3\n200 Sq. Yd.\nClifton, Karachi, Sindh\nClifton - Block 9, Clifton, Karachi, Sindh\nApartment for sale\nFlooring,Electricity Backup,Broadband Internet...\nhttps://www.zameen.com/Property/clifton_clifto...\n4.00\nCrore\n\n\n\n\n\n\n\nWhat we are going to do next is to multiply all the Crores by 100 to convert them to Lakhs. Then we drop off the columns we just created. Out code will check the currency_name column to see if the value is Crore before making the conversion. Next it will replace the column price with the result. We will use the mask method for that.\nThe mask method is used to replace values in a DataFrame or Series under certain conditions.\n.assign(price = lambda x: x[\"price_\"].mask(x[\"currency_name\"] == \"Crore\", x[\"price_\"] * 100))\n\nUsing mask inside .assign:\n\nThe code is creating or modifying the ‚Äòprice‚Äô column in the DataFrame.\nx[\"price_\"].mask(...): This applies the mask method on the price_ column of the DataFrame.\n\nCondition in mask:\n\nThe first argument of mask is a condition: x[\"currency_name\"] == \"Crore\". This checks each row in the currency_name column to see if it equals ‚ÄúCrore‚Äù.\n\nReplacement in mask:\n\nThe second argument of mask is x[\"price_\"] * 100. This is what the mask method will replace the original value with, but only where the condition is met (i.e., where ‚Äòcurrency_name‚Äô is ‚ÄúCrore‚Äù).\n\nHow It Works in our Example:\n\nFor each row in the DataFrame, the code checks if the currency_name for that row is ‚ÄúCrore‚Äù.\nIf it is ‚ÄúCrore‚Äù, the corresponding value in the ‚Äòprice_‚Äô column is multiplied by 100 and this new value replaces the original value in the price column.\nIf it is not ‚ÄúCrore‚Äù, the value in the price column remains as it is in the ‚Äòprice_‚Äô column.\n\n\nWe will now insert that code in the function we are building and also delete the new columns we created since we no longer need them.\n\ndef prep_karachi_data(file_path):\n    return (pd.read_csv(file_path)\n            .rename(columns=lambda x: x.replace(\" \", \"_\")) #no spaces in column names\n            .assign(price_ = lambda x: x[\"price\"].str.split(\" \", expand=True)[0],\n                  currency_name = lambda x: x[\"price\"].str.split(\" \", expand=True)[1])\n            .astype({\"price_\":float})\n            .assign(price = lambda x: x[\"price_\"].mask(x[\"currency_name\"] == \"Crore\", x[\"price_\"] * 100))\n            .drop(columns=['price_', 'currency_name'])\n           )\n\ndf = prep_karachi_data(file_path=file_path)\nprint(df.info())\ndf.head()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 8414 entries, 0 to 8413\nData columns (total 12 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   title              8414 non-null   object \n 1   price              8413 non-null   float64\n 2   date_added         8047 non-null   object \n 3   type               8413 non-null   object \n 4   bedrooms           8414 non-null   int64  \n 5   bathrooms          8414 non-null   int64  \n 6   area               7322 non-null   object \n 7   location           8047 non-null   object \n 8   complete_location  8413 non-null   object \n 9   description        8413 non-null   object \n 10  keywords           7551 non-null   object \n 11  url                8414 non-null   object \ndtypes: float64(1), int64(2), object(9)\nmemory usage: 788.9+ KB\nNone\n\n\n\n\n\n\n\n\n\ntitle\nprice\ndate_added\ntype\nbedrooms\nbathrooms\narea\nlocation\ncomplete_location\ndescription\nkeywords\nurl\n\n\n\n\n0\n600 Yard Bungalow For Sale In DHA Phase 6\n1150.0\n14 hours ago\nHouse\n5\n6\n600 Sq. Yd.\nDHA Defence, Karachi, Sindh\nDHA Phase 6, DHA Defence, Karachi, Sindh\nChance Deal 600 Yard Bungalow For Sale\nBuilt in year: 1,Parking Spaces: 5,Flooring,Ot...\nhttps://www.zameen.com/Property/d_h_a_dha_phas...\n\n\n1\n3 BEDS LUXURY 125 SQ YARDS VILLA FOR SALE LOCA...\n145.0\n5 hours ago\nHouse\n0\n0\nNaN\nBahria Town Karachi, Karachi, Sindh\nBahria Town - Ali Block, Bahria Town - Precinc...\n3 BEDS LUXURY 125 SQ YARDS VILLA FOR SALE LOCA...\nBedrooms: 3,Bathrooms: 3,Kitchens: 2\nhttps://www.zameen.com/Property/bahria_town_pr...\n\n\n2\n4 BEDS LUXURY SPORTS CITY VILLA FOR RENT BAHRI...\n212.0\n5 hours ago\nHouse\n0\n0\nNaN\nBahria Town Karachi, Karachi, Sindh\nBahria Sports City, Bahria Town Karachi, Karac...\n4 BEDS LUXURY SPORTS CITY VILLA FOR RENT BAHRI...\nBedrooms: 4,Bathrooms: 4,Kitchens: 2\nhttps://www.zameen.com/Property/bahria_town_ka...\n\n\n3\n3 BEDS LUXURY 235 SQ YARDS VILLA FOR SALE LOCA...\n150.0\n5 hours ago\nHouse\n0\n0\nNaN\nBahria Town Karachi, Karachi, Sindh\nBahria Town - Precinct 31, Bahria Town Karachi...\n3 BEDS LUXURY 235 SQ YARDS VILLA FOR SALE LOCA...\nBedrooms: 3,Bathrooms: 3,Kitchens: 2\nhttps://www.zameen.com/Property/bahria_town_ka...\n\n\n4\nBuying A Flat In Clifton - Block 9?\n400.0\n6 hours ago\nFlat\n3\n3\n200 Sq. Yd.\nClifton, Karachi, Sindh\nClifton - Block 9, Clifton, Karachi, Sindh\nApartment for sale\nFlooring,Electricity Backup,Broadband Internet...\nhttps://www.zameen.com/Property/clifton_clifto...\n\n\n\n\n\n\n\nWe may have other columns to drop later on and we would add them to the .drop method. But this is all we are going to do with this right now. We have used the method of chaining to clean our data. We are yet to drop null values from the price column.\nThis will be all for today. Thanks for reading."
  },
  {
    "objectID": "blog/2024-January-04-Karachi-Property-Prices/KarachiPropertyPrices.html#introduction",
    "href": "blog/2024-January-04-Karachi-Property-Prices/KarachiPropertyPrices.html#introduction",
    "title": "Data Wrangling: Karachi Property Prices",
    "section": "",
    "text": "This dataset encompasses a collection of 8,414 housing advertisements sourced from Zameen.com, specifically pertaining to properties located in Karachi. The dataset is available on Kaggle, a popular platform for data science and machine learning enthusiasts. It can be accessed through the following link: Karachi, Pakistan Property Prices 2023 .\nIt‚Äôs important to note that this dataset represents only a fraction of the comprehensive data available. It was meticulously scraped and compiled by Faiq Ali, who, at the time, was a student at the University of Malaya. This dataset, listed on his Kaggle page, provides valuable insights into the real estate market of Karachi as of the year 2023. We will try to wrangle the data and prepare it for machine learning.\n\nimport pandas as pd\n\n\nfile_path = \"karachi-pakistan-property-prices-2023.csv\" \n\n\ndef prep_karachi_data(file_path):\n    return (pd.read_csv(file_path)\n           )\n\ndf = prep_karachi_data(file_path=file_path)\nprint(df.info())\ndf.head()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 8414 entries, 0 to 8413\nData columns (total 12 columns):\n #   Column             Non-Null Count  Dtype \n---  ------             --------------  ----- \n 0   title              8414 non-null   object\n 1   price              8413 non-null   object\n 2   date added         8047 non-null   object\n 3   type               8413 non-null   object\n 4   bedrooms           8414 non-null   int64 \n 5   bathrooms          8414 non-null   int64 \n 6   area               7322 non-null   object\n 7   location           8047 non-null   object\n 8   complete location  8413 non-null   object\n 9   description        8413 non-null   object\n 10  keywords           7551 non-null   object\n 11  url                8414 non-null   object\ndtypes: int64(2), object(10)\nmemory usage: 788.9+ KB\nNone\n\n\n\n\n\n\n\n\n\ntitle\nprice\ndate added\ntype\nbedrooms\nbathrooms\narea\nlocation\ncomplete location\ndescription\nkeywords\nurl\n\n\n\n\n0\n600 Yard Bungalow For Sale In DHA Phase 6\n11.5 Crore\n14 hours ago\nHouse\n5\n6\n600 Sq. Yd.\nDHA Defence, Karachi, Sindh\nDHA Phase 6, DHA Defence, Karachi, Sindh\nChance Deal 600 Yard Bungalow For Sale\nBuilt in year: 1,Parking Spaces: 5,Flooring,Ot...\nhttps://www.zameen.com/Property/d_h_a_dha_phas...\n\n\n1\n3 BEDS LUXURY 125 SQ YARDS VILLA FOR SALE LOCA...\n1.45 Crore\n5 hours ago\nHouse\n0\n0\nNaN\nBahria Town Karachi, Karachi, Sindh\nBahria Town - Ali Block, Bahria Town - Precinc...\n3 BEDS LUXURY 125 SQ YARDS VILLA FOR SALE LOCA...\nBedrooms: 3,Bathrooms: 3,Kitchens: 2\nhttps://www.zameen.com/Property/bahria_town_pr...\n\n\n2\n4 BEDS LUXURY SPORTS CITY VILLA FOR RENT BAHRI...\n2.12 Crore\n5 hours ago\nHouse\n0\n0\nNaN\nBahria Town Karachi, Karachi, Sindh\nBahria Sports City, Bahria Town Karachi, Karac...\n4 BEDS LUXURY SPORTS CITY VILLA FOR RENT BAHRI...\nBedrooms: 4,Bathrooms: 4,Kitchens: 2\nhttps://www.zameen.com/Property/bahria_town_ka...\n\n\n3\n3 BEDS LUXURY 235 SQ YARDS VILLA FOR SALE LOCA...\n1.5 Crore\n5 hours ago\nHouse\n0\n0\nNaN\nBahria Town Karachi, Karachi, Sindh\nBahria Town - Precinct 31, Bahria Town Karachi...\n3 BEDS LUXURY 235 SQ YARDS VILLA FOR SALE LOCA...\nBedrooms: 3,Bathrooms: 3,Kitchens: 2\nhttps://www.zameen.com/Property/bahria_town_ka...\n\n\n4\nBuying A Flat In Clifton - Block 9?\n4 Crore\n6 hours ago\nFlat\n3\n3\n200 Sq. Yd.\nClifton, Karachi, Sindh\nClifton - Block 9, Clifton, Karachi, Sindh\nApartment for sale\nFlooring,Electricity Backup,Broadband Internet...\nhttps://www.zameen.com/Property/clifton_clifto...\n\n\n\n\n\n\n\nOne of the first things we are going to do is to change the name of some of the columns. I want to get rid of spaces.\n\ndef prep_karachi_data(file_path):\n    return (pd.read_csv(file_path)\n            .rename(columns=lambda x: x.replace(\" \", \"_\")) #no spaces in column names\n           )\n\ndf = prep_karachi_data(file_path=file_path)\nprint(df.info())\ndf.head()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 8414 entries, 0 to 8413\nData columns (total 12 columns):\n #   Column             Non-Null Count  Dtype \n---  ------             --------------  ----- \n 0   title              8414 non-null   object\n 1   price              8413 non-null   object\n 2   date_added         8047 non-null   object\n 3   type               8413 non-null   object\n 4   bedrooms           8414 non-null   int64 \n 5   bathrooms          8414 non-null   int64 \n 6   area               7322 non-null   object\n 7   location           8047 non-null   object\n 8   complete_location  8413 non-null   object\n 9   description        8413 non-null   object\n 10  keywords           7551 non-null   object\n 11  url                8414 non-null   object\ndtypes: int64(2), object(10)\nmemory usage: 788.9+ KB\nNone\n\n\n\n\n\n\n\n\n\ntitle\nprice\ndate_added\ntype\nbedrooms\nbathrooms\narea\nlocation\ncomplete_location\ndescription\nkeywords\nurl\n\n\n\n\n0\n600 Yard Bungalow For Sale In DHA Phase 6\n11.5 Crore\n14 hours ago\nHouse\n5\n6\n600 Sq. Yd.\nDHA Defence, Karachi, Sindh\nDHA Phase 6, DHA Defence, Karachi, Sindh\nChance Deal 600 Yard Bungalow For Sale\nBuilt in year: 1,Parking Spaces: 5,Flooring,Ot...\nhttps://www.zameen.com/Property/d_h_a_dha_phas...\n\n\n1\n3 BEDS LUXURY 125 SQ YARDS VILLA FOR SALE LOCA...\n1.45 Crore\n5 hours ago\nHouse\n0\n0\nNaN\nBahria Town Karachi, Karachi, Sindh\nBahria Town - Ali Block, Bahria Town - Precinc...\n3 BEDS LUXURY 125 SQ YARDS VILLA FOR SALE LOCA...\nBedrooms: 3,Bathrooms: 3,Kitchens: 2\nhttps://www.zameen.com/Property/bahria_town_pr...\n\n\n2\n4 BEDS LUXURY SPORTS CITY VILLA FOR RENT BAHRI...\n2.12 Crore\n5 hours ago\nHouse\n0\n0\nNaN\nBahria Town Karachi, Karachi, Sindh\nBahria Sports City, Bahria Town Karachi, Karac...\n4 BEDS LUXURY SPORTS CITY VILLA FOR RENT BAHRI...\nBedrooms: 4,Bathrooms: 4,Kitchens: 2\nhttps://www.zameen.com/Property/bahria_town_ka...\n\n\n3\n3 BEDS LUXURY 235 SQ YARDS VILLA FOR SALE LOCA...\n1.5 Crore\n5 hours ago\nHouse\n0\n0\nNaN\nBahria Town Karachi, Karachi, Sindh\nBahria Town - Precinct 31, Bahria Town Karachi...\n3 BEDS LUXURY 235 SQ YARDS VILLA FOR SALE LOCA...\nBedrooms: 3,Bathrooms: 3,Kitchens: 2\nhttps://www.zameen.com/Property/bahria_town_ka...\n\n\n4\nBuying A Flat In Clifton - Block 9?\n4 Crore\n6 hours ago\nFlat\n3\n3\n200 Sq. Yd.\nClifton, Karachi, Sindh\nClifton - Block 9, Clifton, Karachi, Sindh\nApartment for sale\nFlooring,Electricity Backup,Broadband Internet...\nhttps://www.zameen.com/Property/clifton_clifto...\n\n\n\n\n\n\n\nWe will bring out the second column price for further investigation. We will want it to be a number but we can see that it is an object.\n\ndf['price']\n\n0       11.5 Crore\n1       1.45 Crore\n2       2.12 Crore\n3        1.5 Crore\n4          4 Crore\n           ...    \n8409       5 Crore\n8410     1.2 Crore\n8411    1.55 Crore\n8412       70 Lakh\n8413       1 Crore\nName: price, Length: 8414, dtype: object\n\n\nThis seems to have a combination of units.\n‚ÄúLakh‚Äù and ‚ÄúCrore‚Äù are units of numerical value commonly used in the Indian subcontinent, including countries like India, Pakistan, Bangladesh, and Nepal. They are part of the South Asian numbering system and are widely used in these regions for financial transactions, population counts, and more.\n\nLakh:\n\nOne lakh is equal to 100,000 (10^5).\nFor example, in international notation, 5 lakh would be written as 500,000.\n\nCrore:\n\nOne crore is equal to 10 million, or 100 lakh (10^7).\nIn international notation, 1 crore would be expressed as 10,000,000.\n\n\nThese terms provide a more convenient way to express large numbers, particularly in the context of financial transactions and population statistics in the Indian subcontinent. For instance, it‚Äôs more common to hear about a budget of 5 crore rupees rather than 50 million rupees.\nWe will split that column into two, one for the figures and the other for the units. The values and the units are separated by a space.\n\ndf['price'].str.split(\" \", expand=True)\n\n\n\n\n\n\n\n\n0\n1\n\n\n\n\n0\n11.5\nCrore\n\n\n1\n1.45\nCrore\n\n\n2\n2.12\nCrore\n\n\n3\n1.5\nCrore\n\n\n4\n4\nCrore\n\n\n...\n...\n...\n\n\n8409\n5\nCrore\n\n\n8410\n1.2\nCrore\n\n\n8411\n1.55\nCrore\n\n\n8412\n70\nLakh\n\n\n8413\n1\nCrore\n\n\n\n\n8414 rows √ó 2 columns\n\n\n\nWe will now include that in our function that we are slowly building. We will also convert the values in our new column to a float.\n\ndef prep_karachi_data(file_path):\n    return (pd.read_csv(file_path)\n            .rename(columns=lambda x: x.replace(\" \", \"_\")) #no spaces in column names\n            .assign(price_ = lambda x: x[\"price\"].str.split(\" \", expand=True)[0],\n                  currency_name = lambda x: x[\"price\"].str.split(\" \", expand=True)[1])\n            .astype({\"price_\":float})\n           )\n\ndf = prep_karachi_data(file_path=file_path)\nprint(df.info())\ndf.head()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 8414 entries, 0 to 8413\nData columns (total 14 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   title              8414 non-null   object \n 1   price              8413 non-null   object \n 2   date_added         8047 non-null   object \n 3   type               8413 non-null   object \n 4   bedrooms           8414 non-null   int64  \n 5   bathrooms          8414 non-null   int64  \n 6   area               7322 non-null   object \n 7   location           8047 non-null   object \n 8   complete_location  8413 non-null   object \n 9   description        8413 non-null   object \n 10  keywords           7551 non-null   object \n 11  url                8414 non-null   object \n 12  price_             8413 non-null   float64\n 13  currency_name      8413 non-null   object \ndtypes: float64(1), int64(2), object(11)\nmemory usage: 920.4+ KB\nNone\n\n\n\n\n\n\n\n\n\ntitle\nprice\ndate_added\ntype\nbedrooms\nbathrooms\narea\nlocation\ncomplete_location\ndescription\nkeywords\nurl\nprice_\ncurrency_name\n\n\n\n\n0\n600 Yard Bungalow For Sale In DHA Phase 6\n11.5 Crore\n14 hours ago\nHouse\n5\n6\n600 Sq. Yd.\nDHA Defence, Karachi, Sindh\nDHA Phase 6, DHA Defence, Karachi, Sindh\nChance Deal 600 Yard Bungalow For Sale\nBuilt in year: 1,Parking Spaces: 5,Flooring,Ot...\nhttps://www.zameen.com/Property/d_h_a_dha_phas...\n11.50\nCrore\n\n\n1\n3 BEDS LUXURY 125 SQ YARDS VILLA FOR SALE LOCA...\n1.45 Crore\n5 hours ago\nHouse\n0\n0\nNaN\nBahria Town Karachi, Karachi, Sindh\nBahria Town - Ali Block, Bahria Town - Precinc...\n3 BEDS LUXURY 125 SQ YARDS VILLA FOR SALE LOCA...\nBedrooms: 3,Bathrooms: 3,Kitchens: 2\nhttps://www.zameen.com/Property/bahria_town_pr...\n1.45\nCrore\n\n\n2\n4 BEDS LUXURY SPORTS CITY VILLA FOR RENT BAHRI...\n2.12 Crore\n5 hours ago\nHouse\n0\n0\nNaN\nBahria Town Karachi, Karachi, Sindh\nBahria Sports City, Bahria Town Karachi, Karac...\n4 BEDS LUXURY SPORTS CITY VILLA FOR RENT BAHRI...\nBedrooms: 4,Bathrooms: 4,Kitchens: 2\nhttps://www.zameen.com/Property/bahria_town_ka...\n2.12\nCrore\n\n\n3\n3 BEDS LUXURY 235 SQ YARDS VILLA FOR SALE LOCA...\n1.5 Crore\n5 hours ago\nHouse\n0\n0\nNaN\nBahria Town Karachi, Karachi, Sindh\nBahria Town - Precinct 31, Bahria Town Karachi...\n3 BEDS LUXURY 235 SQ YARDS VILLA FOR SALE LOCA...\nBedrooms: 3,Bathrooms: 3,Kitchens: 2\nhttps://www.zameen.com/Property/bahria_town_ka...\n1.50\nCrore\n\n\n4\nBuying A Flat In Clifton - Block 9?\n4 Crore\n6 hours ago\nFlat\n3\n3\n200 Sq. Yd.\nClifton, Karachi, Sindh\nClifton - Block 9, Clifton, Karachi, Sindh\nApartment for sale\nFlooring,Electricity Backup,Broadband Internet...\nhttps://www.zameen.com/Property/clifton_clifto...\n4.00\nCrore\n\n\n\n\n\n\n\nWhat we are going to do next is to multiply all the Crores by 100 to convert them to Lakhs. Then we drop off the columns we just created. Out code will check the currency_name column to see if the value is Crore before making the conversion. Next it will replace the column price with the result. We will use the mask method for that.\nThe mask method is used to replace values in a DataFrame or Series under certain conditions.\n.assign(price = lambda x: x[\"price_\"].mask(x[\"currency_name\"] == \"Crore\", x[\"price_\"] * 100))\n\nUsing mask inside .assign:\n\nThe code is creating or modifying the ‚Äòprice‚Äô column in the DataFrame.\nx[\"price_\"].mask(...): This applies the mask method on the price_ column of the DataFrame.\n\nCondition in mask:\n\nThe first argument of mask is a condition: x[\"currency_name\"] == \"Crore\". This checks each row in the currency_name column to see if it equals ‚ÄúCrore‚Äù.\n\nReplacement in mask:\n\nThe second argument of mask is x[\"price_\"] * 100. This is what the mask method will replace the original value with, but only where the condition is met (i.e., where ‚Äòcurrency_name‚Äô is ‚ÄúCrore‚Äù).\n\nHow It Works in our Example:\n\nFor each row in the DataFrame, the code checks if the currency_name for that row is ‚ÄúCrore‚Äù.\nIf it is ‚ÄúCrore‚Äù, the corresponding value in the ‚Äòprice_‚Äô column is multiplied by 100 and this new value replaces the original value in the price column.\nIf it is not ‚ÄúCrore‚Äù, the value in the price column remains as it is in the ‚Äòprice_‚Äô column.\n\n\nWe will now insert that code in the function we are building and also delete the new columns we created since we no longer need them.\n\ndef prep_karachi_data(file_path):\n    return (pd.read_csv(file_path)\n            .rename(columns=lambda x: x.replace(\" \", \"_\")) #no spaces in column names\n            .assign(price_ = lambda x: x[\"price\"].str.split(\" \", expand=True)[0],\n                  currency_name = lambda x: x[\"price\"].str.split(\" \", expand=True)[1])\n            .astype({\"price_\":float})\n            .assign(price = lambda x: x[\"price_\"].mask(x[\"currency_name\"] == \"Crore\", x[\"price_\"] * 100))\n            .drop(columns=['price_', 'currency_name'])\n           )\n\ndf = prep_karachi_data(file_path=file_path)\nprint(df.info())\ndf.head()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 8414 entries, 0 to 8413\nData columns (total 12 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   title              8414 non-null   object \n 1   price              8413 non-null   float64\n 2   date_added         8047 non-null   object \n 3   type               8413 non-null   object \n 4   bedrooms           8414 non-null   int64  \n 5   bathrooms          8414 non-null   int64  \n 6   area               7322 non-null   object \n 7   location           8047 non-null   object \n 8   complete_location  8413 non-null   object \n 9   description        8413 non-null   object \n 10  keywords           7551 non-null   object \n 11  url                8414 non-null   object \ndtypes: float64(1), int64(2), object(9)\nmemory usage: 788.9+ KB\nNone\n\n\n\n\n\n\n\n\n\ntitle\nprice\ndate_added\ntype\nbedrooms\nbathrooms\narea\nlocation\ncomplete_location\ndescription\nkeywords\nurl\n\n\n\n\n0\n600 Yard Bungalow For Sale In DHA Phase 6\n1150.0\n14 hours ago\nHouse\n5\n6\n600 Sq. Yd.\nDHA Defence, Karachi, Sindh\nDHA Phase 6, DHA Defence, Karachi, Sindh\nChance Deal 600 Yard Bungalow For Sale\nBuilt in year: 1,Parking Spaces: 5,Flooring,Ot...\nhttps://www.zameen.com/Property/d_h_a_dha_phas...\n\n\n1\n3 BEDS LUXURY 125 SQ YARDS VILLA FOR SALE LOCA...\n145.0\n5 hours ago\nHouse\n0\n0\nNaN\nBahria Town Karachi, Karachi, Sindh\nBahria Town - Ali Block, Bahria Town - Precinc...\n3 BEDS LUXURY 125 SQ YARDS VILLA FOR SALE LOCA...\nBedrooms: 3,Bathrooms: 3,Kitchens: 2\nhttps://www.zameen.com/Property/bahria_town_pr...\n\n\n2\n4 BEDS LUXURY SPORTS CITY VILLA FOR RENT BAHRI...\n212.0\n5 hours ago\nHouse\n0\n0\nNaN\nBahria Town Karachi, Karachi, Sindh\nBahria Sports City, Bahria Town Karachi, Karac...\n4 BEDS LUXURY SPORTS CITY VILLA FOR RENT BAHRI...\nBedrooms: 4,Bathrooms: 4,Kitchens: 2\nhttps://www.zameen.com/Property/bahria_town_ka...\n\n\n3\n3 BEDS LUXURY 235 SQ YARDS VILLA FOR SALE LOCA...\n150.0\n5 hours ago\nHouse\n0\n0\nNaN\nBahria Town Karachi, Karachi, Sindh\nBahria Town - Precinct 31, Bahria Town Karachi...\n3 BEDS LUXURY 235 SQ YARDS VILLA FOR SALE LOCA...\nBedrooms: 3,Bathrooms: 3,Kitchens: 2\nhttps://www.zameen.com/Property/bahria_town_ka...\n\n\n4\nBuying A Flat In Clifton - Block 9?\n400.0\n6 hours ago\nFlat\n3\n3\n200 Sq. Yd.\nClifton, Karachi, Sindh\nClifton - Block 9, Clifton, Karachi, Sindh\nApartment for sale\nFlooring,Electricity Backup,Broadband Internet...\nhttps://www.zameen.com/Property/clifton_clifto...\n\n\n\n\n\n\n\nWe may have other columns to drop later on and we would add them to the .drop method. But this is all we are going to do with this right now. We have used the method of chaining to clean our data. We are yet to drop null values from the price column.\nThis will be all for today. Thanks for reading."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "SisengAI",
    "section": "",
    "text": "LinkedIn\n  \n  \n    \n     Twitter\n  \n  \n    \n     GitHub\n  \n  \n    \n     e-mail\n  \n\n\n\n\nHello, World!\n\nHey there! I‚Äôm Ricky, the brain behind SisengAI. It‚Äôs more than just a project ‚Äì it‚Äôs my playground in the ever-exciting world of Data Science, Machine Learning, Financial Engineering, and NLP. My adventure in this realm kicked off in 2017, when I was 43 and stumbled upon the wonders of Python & pandas. It was a game-changer for me, shifting gears from my days as a maintenance engineer in Nigeria and dabbling in renewable energy consultancy across Nigeria and Ethiopia.\nWhen the world paused during the pandemic, I turned to YouTube, sharing Python tricks and tips on my channel (check it out here!). These days, you‚Äôll find me at WorldQuant University in New Orleans, LA, where I teach data science. It‚Äôs a cool place, and you can learn more about it here.\nNot just a man behind the screen, I‚Äôve also been out and about, consulting for companies and people, spreading the data science cheer through seminars, and teaching both online and in-person. On this very website, you‚Äôll discover a treasure trove of my published works, thought-provoking blog posts, and the latest buzz on my research projects. Plus, you‚Äôll get the scoop on all the events and conferences where I‚Äôll be making an appearance. Dive into my world and explore to your heart‚Äôs content!\nGot questions? Ideas bubbling up? Or maybe a project you think we can collaborate on? Don‚Äôt hesitate to contact me. I‚Äôm all ears and ready to chat!\n\n\n\neducation\n\n\nMSc in Financial Engineering, 2021\n\n\nWorldQuant University, New Orleans, LA, USA.\n\n\nBS in Electrical & Electronics Engineering, 2000\n\n\nAbubakar Tafawa Balewa University, Bauchi, Nigeria.\n\n\n\n\nexperience\n\n\nData Science Instructor, 2022-present\n\n\nWorldQuant University\n\n\nCo-author and member of Masakhane, since 2020\n\n\n\nParticipatory Research for Low-resourced Machine Translation: A Case Study in African Languages\n\n\n\n\nAfriMTE and AfriCOMET: Empowering COMET to Embrace Under-resourced African Languages\n\n\n\n\nView the template for this on my github repo"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Blog Posts",
    "section": "",
    "text": "Python‚Äôs Data Structures: Tuples, namedTuples & DataClasses (Retired Legends Edition)\n\n\n\n\n\n\nPython\n\n\ncollections\n\n\nDataClasses\n\n\nNamedTuple\n\n\n\n\n\n\n\n\n\nApr 7, 2025\n\n\nRicky Macharm\n\n\n\n\n\n\n\n\n\n\n\n\nGrokking the Unpacking Operator\n\n\n\n\n\n\nPython\n\n\nUnpacking Operator\n\n\nMethod Chaining\n\n\n\n\n\n\n\n\n\nSep 14, 2024\n\n\nRicky Macharm\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding The Assignment Expression aka Walrus Operator\n\n\n\n\n\n\nPython\n\n\nwalrus operator\n\n\nassignment expression\n\n\nPEP 572\n\n\n\n\n\n\n\n\n\nJun 19, 2024\n\n\nRicky Macharm\n\n\n\n\n\n\n\n\n\n\n\n\nLambda Functions: How I tend to use them\n\n\n\n\n\n\nFunctions\n\n\nLambda Functions\n\n\npandas\n\n\nMethods\n\n\nMethod Chaining\n\n\nGroupby\n\n\nAggregation\n\n\n\n\n\n\n\n\n\nMay 25, 2024\n\n\nRicky Macharm\n\n\n\n\n\n\n\n\n\n\n\n\nWorking with Dictionaries and DataFrames\n\n\n\n\n\n\nDictionary\n\n\nDataFrame\n\n\npandas\n\n\nCorrelation\n\n\nFaker\n\n\nRandom Number Generator\n\n\n\n\n\n\n\n\n\nMay 15, 2024\n\n\nRicky Macharm\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to DuckDB: Working with User Defined Functions (UDF)\n\n\n\n\n\n\nDuckDB\n\n\nDatabase\n\n\nSQL\n\n\nUser-Defined Functions\n\n\nQueries\n\n\nIn-Process\n\n\nPython\n\n\n\n\n\n\n\n\n\nApr 3, 2024\n\n\nRicky Macharm\n\n\n\n\n\n\n\n\n\n\n\n\nAn introduction to Matplotlib\n\n\n\n\n\n\nMatplotlib\n\n\nMatlab\n\n\npandas\n\n\nOOP\n\n\n\n\n\n\n\n\n\nFeb 24, 2024\n\n\nRicky Macharm\n\n\n\n\n\n\n\n\n\n\n\n\nData Wrangling: Karachi Property Prices\n\n\n\n\n\n\nMethod Chaining\n\n\nData Wrangling\n\n\npandas\n\n\nKarachi Property Prices\n\n\n\n\n\n\n\n\n\nJan 4, 2024\n\n\nRicky Macharm\n\n\n\n\n\n\n\n\n\n\n\n\nData Wrangling\n\n\n\n\n\n\nMethod Chaining\n\n\nData Wrangling\n\n\npandas\n\n\nUCI Machine Learning Repository\n\n\nbike rentals\n\n\n\n\n\n\n\n\n\nDec 27, 2023\n\n\nRicky Macharm\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/2024-April-03-User-Defined-Functions-In-DuckDB/GenerateDummyDatawithUserDefinedFunctions_(UDFs).html",
    "href": "blog/2024-April-03-User-Defined-Functions-In-DuckDB/GenerateDummyDatawithUserDefinedFunctions_(UDFs).html",
    "title": "Introduction to DuckDB: Working with User Defined Functions (UDF)",
    "section": "",
    "text": "DuckDB, a relatively new entrant in the world of databases, is akin to finding a swiss army knife when you‚Äôre used to dealing with specialized tools. It‚Äôs an in-process SQL database, which, in layman‚Äôs terms, means it operates within your application‚Äôs process. This is a contrast to more conventional databases that run as separate server processes.\nPicture a library. Traditional databases like MySQL or PostgreSQL are like separate library buildings you visit. Each time you need a book (or data), you go there, get what you need, and come back. DuckDB, on the other hand, is like having a personal bookshelf in your room, integrated into your own space for easy and immediate access. This structure lends DuckDB an edge in efficiency and simplicity, particularly for data analysis tasks.\nDuckDB shines in scenarios where you need to analyze large amounts of data rapidly. Its columnar storage format, which organizes data by columns rather than rows, is optimized for analytical queries. Imagine a spreadsheet; most databases read it row by row. DuckDB reads it column by column, which is much faster for certain types of data analysis.\nBut DuckDB isn‚Äôt just about speed and proximity. It‚Äôs also incredibly versatile. Its support for standard SQL makes it instantly familiar to anyone who‚Äôs worked with traditional databases. It‚Äôs like speaking the same language but finding out it has some neat new expressions and idioms that make certain tasks much easier.\nAnother area where DuckDB stands out is its minimal setup and maintenance. Unlike more heavyweight databases, which require careful installation, configuration, and maintenance, getting DuckDB up and running is as simple as installing a Python library. It‚Äôs like the difference between setting up a tent and building a full-fledged house.\nHowever, DuckDB is not without its limitations. Its in-process nature means it‚Äôs not designed for scenarios where you need a separate, centralized database server accessible to multiple applications or users simultaneously ‚Äì a common scenario in web applications.\nFor data scientists working primarily in Python, DuckDB offers seamless integration, allowing them to stay within their familiar Python environment while performing complex SQL queries on large datasets. This is a significant productivity boost, as it eliminates the need to switch between different tools and interfaces.\nIn comparison, traditional databases like MySQL or PostgreSQL might offer more in terms of advanced features, security, and scalability for application development, but they also come with more overhead in terms of setup and maintenance. They are more like trucks, suitable for heavy lifting and long journeys, whereas DuckDB is more like a nimble bike, perfect for quick trips within the city.\nIn summary, DuckDB is a compact, efficient, and easy-to-use database that excels in data analysis within an application process. It‚Äôs a perfect fit for data analysts and scientists who need speed and efficiency without the overhead of traditional database servers. But for applications requiring a standalone database server, especially in production environments, traditional databases still hold the fort.\nExploring the depths of DuckDB‚Äôs Python package, particularly its latest feature that allows the addition of custom functions, is like opening a box of infinite possibilities. Let‚Äôs get into the weeds a little bit, step by step, aimed at newbies like me who are eager to learn each detail.\n\n\nImagine a toolbox; DuckDB is like a new, sophisticated tool inside it. DuckDB is an in-process SQL (Structured Query Language) database. This means it runs within your program, eliminating the need for a separate database server. Think of it as a helper living inside your Python script, managing data efficiently.\nAs of the time of this writing, it is in its 0.10 realease; however, in its 0.8 release, DuckDB introduced a game-changing feature: allowing users to create their own functions using Python. It‚Äôs like giving you the freedom to create a personalized tool for this toolbox.\nTo better understand this, we introduce another library called ‚ÄòFaker‚Äô. Faker is like a magician in the world of data. It generates fake data ‚Äì names, addresses, emails, etc. This is especially handy for testing and developing, as you often need data that isn‚Äôt.\n\n\n\nIf you do not have either duckdb or faker installed, the following line will help you to do so.\nmamba install duckdb faker -y\n(or use pip without the -y flag, which might be easier).\nThis line is like telling your computer, ‚ÄúHey, please add these two new tools to our kit.‚Äù Mamba or pip are like assistants fetching these tools for us.\nThen, we dive into Python code:\n\nimport faker\nfake = faker.Faker()\n\nHere, we‚Äôre simply saying, ‚ÄúLet‚Äôs start using Faker and create a new Faker generator.‚Äù It‚Äôs like turning on our data-creating magician.\nNext, we define a new function, generate_person:\n\ndef generate_person():\n    person = {\n        'name': fake.name(),\n        'city': fake.city(),\n        'state': fake.state(),\n        'zip_code': fake.zipcode(),\n        'country': fake.country(),\n        'email': fake.email(),\n        'job': fake.job(),\n        'company': fake.company(),\n        'ssn': fake.ssn(),\n        'birthdate': fake.date_of_birth(),\n        'phone_number': fake.phone_number()\n    }\n    return person\n\nThis function is our custom tool. When called, it creates a fake person with details like name, city, and job. Each fake.xxx() calls on Faker to conjure up a piece of data.\n\n\n\nNow, the exciting part: integrating this with DuckDB.\n\nimport duckdb\nfrom duckdb.typing import *\nprint(f\"version: {duckdb.__version__}\")\n\nversion: 0.10.0\n\n\n\ncon = duckdb.connect('udf.duck.db')\n\nHere, we‚Äôre setting up a connection to DuckDB, like opening a line of communication with our database tool.\n\ncon.create_function(\n    'generate_person',\n    generate_person,\n    [],\n    duckdb.struct_type({\n        'name': 'VARCHAR',\n        'city': 'VARCHAR',\n        'state': 'VARCHAR',\n        'zip_code': 'VARCHAR',\n        'country': 'VARCHAR',\n        'email': 'VARCHAR',\n        'job': 'VARCHAR',\n        'company': 'VARCHAR',\n        'ssn': 'VARCHAR',\n        'birthdate': 'DATE',\n        'phone_number': 'VARCHAR'\n    })\n)\n\n&lt;duckdb.duckdb.DuckDBPyConnection at 0x7fd49caca2f0&gt;\n\n\nIn this snippet, we tell DuckDB, ‚ÄúHey, I‚Äôve got this cool function called generate_person. Please use it as if it‚Äôs one of your native functions.‚Äù duckdb.struct_type specifies the structure of data our function returns, much like defining what kind of output our tool will produce.\n\n\n\nFinally, we use our function in a SQL query:\n\nrows = con.sql(\"\"\"\nSELECT generate_person() AS person\nFROM generate_series(1, 10)\n\"\"\").fetchall()\nfor row in rows:\n    print(row)\n\n({'name': 'Jason Johnston Jr.', 'city': 'Pattonshire', 'state': 'Utah', 'zip_code': '83591', 'country': 'Mayotte', 'email': 'brookemorse@example.com', 'job': 'Communications engineer', 'company': 'Brown Group', 'ssn': '566-85-9938', 'birthdate': datetime.date(1921, 12, 13), 'phone_number': '+1-906-990-7965x959'},)\n({'name': 'Jason Johnston Jr.', 'city': 'Pattonshire', 'state': 'Utah', 'zip_code': '83591', 'country': 'Mayotte', 'email': 'brookemorse@example.com', 'job': 'Communications engineer', 'company': 'Brown Group', 'ssn': '566-85-9938', 'birthdate': datetime.date(1921, 12, 13), 'phone_number': '+1-906-990-7965x959'},)\n({'name': 'Jason Johnston Jr.', 'city': 'Pattonshire', 'state': 'Utah', 'zip_code': '83591', 'country': 'Mayotte', 'email': 'brookemorse@example.com', 'job': 'Communications engineer', 'company': 'Brown Group', 'ssn': '566-85-9938', 'birthdate': datetime.date(1921, 12, 13), 'phone_number': '+1-906-990-7965x959'},)\n({'name': 'Jason Johnston Jr.', 'city': 'Pattonshire', 'state': 'Utah', 'zip_code': '83591', 'country': 'Mayotte', 'email': 'brookemorse@example.com', 'job': 'Communications engineer', 'company': 'Brown Group', 'ssn': '566-85-9938', 'birthdate': datetime.date(1921, 12, 13), 'phone_number': '+1-906-990-7965x959'},)\n({'name': 'Jason Johnston Jr.', 'city': 'Pattonshire', 'state': 'Utah', 'zip_code': '83591', 'country': 'Mayotte', 'email': 'brookemorse@example.com', 'job': 'Communications engineer', 'company': 'Brown Group', 'ssn': '566-85-9938', 'birthdate': datetime.date(1921, 12, 13), 'phone_number': '+1-906-990-7965x959'},)\n({'name': 'Jason Johnston Jr.', 'city': 'Pattonshire', 'state': 'Utah', 'zip_code': '83591', 'country': 'Mayotte', 'email': 'brookemorse@example.com', 'job': 'Communications engineer', 'company': 'Brown Group', 'ssn': '566-85-9938', 'birthdate': datetime.date(1921, 12, 13), 'phone_number': '+1-906-990-7965x959'},)\n({'name': 'Jason Johnston Jr.', 'city': 'Pattonshire', 'state': 'Utah', 'zip_code': '83591', 'country': 'Mayotte', 'email': 'brookemorse@example.com', 'job': 'Communications engineer', 'company': 'Brown Group', 'ssn': '566-85-9938', 'birthdate': datetime.date(1921, 12, 13), 'phone_number': '+1-906-990-7965x959'},)\n({'name': 'Jason Johnston Jr.', 'city': 'Pattonshire', 'state': 'Utah', 'zip_code': '83591', 'country': 'Mayotte', 'email': 'brookemorse@example.com', 'job': 'Communications engineer', 'company': 'Brown Group', 'ssn': '566-85-9938', 'birthdate': datetime.date(1921, 12, 13), 'phone_number': '+1-906-990-7965x959'},)\n({'name': 'Jason Johnston Jr.', 'city': 'Pattonshire', 'state': 'Utah', 'zip_code': '83591', 'country': 'Mayotte', 'email': 'brookemorse@example.com', 'job': 'Communications engineer', 'company': 'Brown Group', 'ssn': '566-85-9938', 'birthdate': datetime.date(1921, 12, 13), 'phone_number': '+1-906-990-7965x959'},)\n({'name': 'Jason Johnston Jr.', 'city': 'Pattonshire', 'state': 'Utah', 'zip_code': '83591', 'country': 'Mayotte', 'email': 'brookemorse@example.com', 'job': 'Communications engineer', 'company': 'Brown Group', 'ssn': '566-85-9938', 'birthdate': datetime.date(1921, 12, 13), 'phone_number': '+1-906-990-7965x959'},)\n\n\nThis is where magic happens. We ask DuckDB to create a new table, people, the code is executing a query in DuckDB to generate data for 10 fake people, by our generate_person function, fetching those data rows into Python, and then iterating over these rows to print them out. This is a common pattern for generating and inspecting data in database-driven Python applications, especially in data analysis and testing scenarios. It‚Äôs like telling DuckDB to build a house and fill it with residents, each created by our custom tool.\n\n\n\nBut, there was a twist. This script kept returning the same person over and over. Why? Because DuckDB, trying to be efficient, was reusing the result of the function. It‚Äôs like using a template instead of creating a new person each time.\nThe solution? We modify generate_person to accept a seed:\n\ndef generate_person(seed):\n    person = {\n        'name': fake.name(),\n        'city': fake.city(),\n        'state': fake.state(),\n        'zip_code': fake.zipcode(),\n        'country': fake.country(),\n        'email': fake.email(),\n        'job': fake.job(),\n        'company': fake.company(),\n        'ssn': fake.ssn(),\n        'birthdate': fake.date_of_birth(),\n        'phone_number': fake.phone_number()\n    }\n    return person\n\n\ncon = duckdb.connect('udf.duck.db')\n\ncon.create_function(\n    'generate_person',\n    generate_person,\n    [DOUBLE],\n    duckdb.struct_type({\n        'name': 'VARCHAR',\n        'city': 'VARCHAR',\n        'state': 'VARCHAR',\n        'zip_code': 'VARCHAR',\n        'country': 'VARCHAR',\n        'email': 'VARCHAR',\n        'job': 'VARCHAR',\n        'company': 'VARCHAR',\n        'ssn': 'VARCHAR',\n        'birthdate': 'DATE',\n        'phone_number': 'VARCHAR'\n    })\n)\n\nrows = con.sql(\"\"\"\nSELECT generate_person(random()) AS person\nFROM generate_series(1, 10)\n\"\"\").fetchall()\nfor row in rows:\n    print(row)\n\n({'name': 'Andrea Jordan', 'city': 'Port Lisashire', 'state': 'Connecticut', 'zip_code': '98788', 'country': 'Eritrea', 'email': 'dawngonzalez@example.net', 'job': 'Cabin crew', 'company': 'Barton-Fernandez', 'ssn': '894-92-7787', 'birthdate': datetime.date(2003, 10, 21), 'phone_number': '836-318-0192'},)\n({'name': 'Cynthia Welch', 'city': 'South Amandaton', 'state': 'Kansas', 'zip_code': '40133', 'country': 'Uruguay', 'email': 'daniel79@example.org', 'job': 'Education administrator', 'company': 'Lee, Poole and Perez', 'ssn': '352-89-7570', 'birthdate': datetime.date(1932, 4, 29), 'phone_number': '550-697-9011x72175'},)\n({'name': 'Hannah Glass', 'city': 'Port Steven', 'state': 'New Hampshire', 'zip_code': '31498', 'country': 'Thailand', 'email': 'joshuasolis@example.com', 'job': 'Engineer, biomedical', 'company': 'Wagner-Chan', 'ssn': '157-07-7981', 'birthdate': datetime.date(1994, 4, 6), 'phone_number': '714.364.1179'},)\n({'name': 'April Gonzalez', 'city': 'North Robert', 'state': 'Wyoming', 'zip_code': '35103', 'country': 'Guinea', 'email': 'hallpaul@example.com', 'job': 'Sub', 'company': 'Santos, Fitzgerald and Fritz', 'ssn': '346-33-8946', 'birthdate': datetime.date(2007, 11, 3), 'phone_number': '635-712-9242'},)\n({'name': 'Timothy Smith Jr.', 'city': 'New Gregory', 'state': 'South Carolina', 'zip_code': '83948', 'country': 'Israel', 'email': 'newmancarol@example.net', 'job': 'Research scientist (life sciences)', 'company': 'Hunter LLC', 'ssn': '717-29-2637', 'birthdate': datetime.date(1938, 11, 1), 'phone_number': '454-311-6022x036'},)\n({'name': 'Kathleen Watson', 'city': 'New Monique', 'state': 'Missouri', 'zip_code': '45261', 'country': 'Bolivia', 'email': 'cohenrachel@example.com', 'job': 'Community development worker', 'company': 'Hansen, Hurley and Gibson', 'ssn': '353-51-7806', 'birthdate': datetime.date(1991, 5, 15), 'phone_number': '693.901.5298'},)\n({'name': 'Thomas Snyder', 'city': 'South Karen', 'state': 'South Dakota', 'zip_code': '72363', 'country': 'Moldova', 'email': 'jessicawu@example.com', 'job': 'Medical secretary', 'company': 'Calderon and Sons', 'ssn': '347-11-7234', 'birthdate': datetime.date(1982, 9, 14), 'phone_number': '+1-503-484-5672x006'},)\n({'name': 'Sheena Parks', 'city': 'Port Donald', 'state': 'Vermont', 'zip_code': '94876', 'country': 'Spain', 'email': 'gfrank@example.org', 'job': 'Minerals surveyor', 'company': 'Mills Ltd', 'ssn': '020-14-2155', 'birthdate': datetime.date(1913, 10, 5), 'phone_number': '400.459.3822'},)\n({'name': 'Melissa Mason', 'city': 'West Alexander', 'state': 'Kansas', 'zip_code': '62074', 'country': 'Bahamas', 'email': 'holmesmark@example.org', 'job': 'Surveyor, building control', 'company': 'Jimenez-Mccall', 'ssn': '615-81-9873', 'birthdate': datetime.date(1957, 6, 14), 'phone_number': '(628)905-5737x84834'},)\n({'name': 'Anna Olsen', 'city': 'Deanhaven', 'state': 'South Carolina', 'zip_code': '88465', 'country': 'Bangladesh', 'email': 'wcastaneda@example.com', 'job': 'Environmental manager', 'company': 'Herman and Sons', 'ssn': '503-96-4916', 'birthdate': datetime.date(1951, 1, 4), 'phone_number': '9925956906'},)\n\n\nNow, each call to generate_person is unique, thanks to the random() function in our SQL query. It‚Äôs like telling the function, ‚ÄúPlease create a new, unique person each time.‚Äù\nWith this approach, we now have a powerful and flexible way to generate test data using DuckDB and Python, tailored to our needs. For beginners, it‚Äôs like learning to create and use customized tools in a workshop, opening up a world of possibilities in data handling and manipulation.\n\ncon.sql(\"\"\"\nCREATE OR REPLACE TABLE people AS\nSELECT person.* FROM (\n    SELECT generate_person(random()) AS person  \n    FROM generate_series(1, 10000)\n    )\n\"\"\")\n\n\n\n\nThis code snippet is creating a table and populating it with generated data. Let‚Äôs break down what this code is doing:\n\nCREATE OR REPLACE TABLE people AS:\n\nThis command creates a new table named people in the DuckDB database. If a table with this name already exists, it will be replaced (essentially, it‚Äôs deleted and recreated). This is useful for refreshing the data or structure of the table without having to manually drop it first.\n\nThe Inner SELECT Statement:\n\nSELECT generate_person(random()) AS person:\n\nThis part of the query generates a row of data using the generate_person function.\nThe generate_person function is assumed to be a user-defined function (UDF) in DuckDB that generates a structured record of a person with various attributes like name, city, etc.\nrandom() is a function that generates a random number. It seems to be passed as an argument to generate_person, likely to ensure that each call to generate_person results in a unique record by using a different random seed.\nAS person aliases the result of generate_person(random()) as person. This means that in the context of this query, the output of generate_person(random()) is referred to as person.\n\n\nFROM generate_series(1, 10000):\n\nThis part generates a series of numbers from 1 to 10,000. For each number in this series, the SELECT generate_person(random()) is executed. This means that the generate_person function is called 10,000 times, creating 10,000 unique records.\n\nThe Outer SELECT Statement:\n\nSELECT person.* FROM (...):\n\nThis outer SELECT statement selects all columns from each record generated by generate_person(random()) and arranges them into rows of the people table. The use of person.* indicates that all attributes of the person record should be included in the table.\n\n\n\nIn summary, this code is used to create a table named people in a DuckDB database and populate it with 10,000 unique rows of data, each row representing a person with various attributes. The data for each person is generated by the generate_person function, which appears to create diverse and random data for each person. This technique is especially useful for generating large sets of test data.\n\n\n\nWe will now employ various methods to verify and examine the data we have generated and stored in our database.\n\nquery = \"SELECT * FROM people ORDER BY RANDOM() LIMIT 5;\"\n\n\nresult = con.execute(query)\n\n\nrows = result.fetchall()\n\n\nfor row in rows:\n    print(row)\n\n('Brian Wolf', 'Waynehaven', 'Wisconsin', '08333', 'Cape Verde', 'dana74@example.net', 'Financial adviser', 'Russell LLC', '040-05-7683', datetime.date(2002, 8, 22), '240-447-8269')\n('Jerry Smith', 'Port Michael', 'Washington', '84743', 'Pakistan', 'melissa67@example.com', 'Programmer, systems', 'Moore, Foster and Mitchell', '374-58-8406', datetime.date(2011, 4, 5), '657-346-6548x310')\n('Tony Gonzales', 'New Heather', 'Illinois', '84194', 'Heard Island and McDonald Islands', 'ann77@example.com', 'Producer, television/film/video', 'Miller Group', '438-59-1081', datetime.date(1987, 9, 11), '+1-916-914-5582x8441')\n('David Hickman', 'North Eric', 'Louisiana', '55622', 'Argentina', 'matalarry@example.net', 'Dance movement psychotherapist', 'Rivera, Pittman and Orozco', '540-74-0081', datetime.date(2015, 12, 3), '313-341-7514x72204')\n('Nancy Clements', 'Stephaniemouth', 'Delaware', '00939', 'Gambia', 'sanderssarah@example.org', 'Arts administrator', 'Williams-Brown', '670-26-5114', datetime.date(1959, 8, 1), '759.858.5832x66035')\n\n\n\n#  Execute the First Query\nquery = \"SELECT COUNT(*) FROM people;\"\nrow_count = con.execute(query).fetchone()\n\n# Display the Result\nprint(\"Number of rows in 'people':\", row_count[0])\n\nNumber of rows in 'people': 10000\n\n\n\n# Execute the Second Query\nquery = \"SELECT * FROM people LIMIT 5;\"\nrows = con.execute(query).fetchall()\n\n# Display the Results\nfor row in rows:\n    print(row)\n\n('Stacey Pratt', 'Port Daniel', 'Arkansas', '85990', 'Syrian Arab Republic', 'uwarner@example.net', 'Therapist, speech and language', 'Sullivan-Thomas', '515-58-6703', datetime.date(2024, 2, 9), '+1-966-832-1511x417')\n('David Watkins', 'Skinnerborough', 'Maryland', '44041', 'Macao', 'sedwards@example.org', 'Engineer, manufacturing', 'Weaver LLC', '035-51-1158', datetime.date(1971, 3, 10), '827-546-2880')\n('Jonathan Sampson', 'Hannahtown', 'Nebraska', '67966', 'Tunisia', 'andreamcgee@example.com', 'Trade mark attorney', 'Fields-Henderson', '123-98-2248', datetime.date(2010, 7, 4), '3097651941')\n('Thomas Coleman', 'Sabrinashire', 'Oklahoma', '28417', 'Saudi Arabia', 'marieyang@example.org', 'Film/video editor', 'Tran, Anderson and Smith', '651-45-3497', datetime.date(1957, 5, 31), '001-565-609-3892')\n('Suzanne Jackson', 'Jessicaville', 'Minnesota', '95997', 'Antarctica (the territory South of 60 deg S)', 'kmadden@example.com', 'Air cabin crew', 'Hart-Wagner', '828-72-9535', datetime.date(2008, 12, 1), '875.786.6969x5960')\n\n\nAll appears to have gone smoothly.\nThat concludes this post. I trust you found as much pleasure in reading it as I did in writing it.\n\nThis insightful exploration into the functionalities of DuckDB, particularly the creation of dummy data using user-defined functions, is inspired by the work of Mark Needham. Mark is a prolific content creator in the data field, currently focusing on short-form content at ClickHouse. His expertise is evident in his engaging YouTube channel, LearnDataWithMark, where he simplifies complex data problems into concise, 5-minute tutorials. His previous experience at Neo4j, especially in graph analytics, further solidifies his stature in the community, as does his co-authorship of the O‚ÄôReilly Graph Algorithms Book with Amy Hodler. The original post that sparked this discussion can be found on Mark‚Äôs blog here. Mark‚Äôs clear and practical approach to data is not only enlightening but also instrumental in making advanced data concepts accessible to a wider audience."
  },
  {
    "objectID": "blog/2024-April-03-User-Defined-Functions-In-DuckDB/GenerateDummyDatawithUserDefinedFunctions_(UDFs).html#introduction",
    "href": "blog/2024-April-03-User-Defined-Functions-In-DuckDB/GenerateDummyDatawithUserDefinedFunctions_(UDFs).html#introduction",
    "title": "Introduction to DuckDB: Working with User Defined Functions (UDF)",
    "section": "",
    "text": "DuckDB, a relatively new entrant in the world of databases, is akin to finding a swiss army knife when you‚Äôre used to dealing with specialized tools. It‚Äôs an in-process SQL database, which, in layman‚Äôs terms, means it operates within your application‚Äôs process. This is a contrast to more conventional databases that run as separate server processes.\nPicture a library. Traditional databases like MySQL or PostgreSQL are like separate library buildings you visit. Each time you need a book (or data), you go there, get what you need, and come back. DuckDB, on the other hand, is like having a personal bookshelf in your room, integrated into your own space for easy and immediate access. This structure lends DuckDB an edge in efficiency and simplicity, particularly for data analysis tasks.\nDuckDB shines in scenarios where you need to analyze large amounts of data rapidly. Its columnar storage format, which organizes data by columns rather than rows, is optimized for analytical queries. Imagine a spreadsheet; most databases read it row by row. DuckDB reads it column by column, which is much faster for certain types of data analysis.\nBut DuckDB isn‚Äôt just about speed and proximity. It‚Äôs also incredibly versatile. Its support for standard SQL makes it instantly familiar to anyone who‚Äôs worked with traditional databases. It‚Äôs like speaking the same language but finding out it has some neat new expressions and idioms that make certain tasks much easier.\nAnother area where DuckDB stands out is its minimal setup and maintenance. Unlike more heavyweight databases, which require careful installation, configuration, and maintenance, getting DuckDB up and running is as simple as installing a Python library. It‚Äôs like the difference between setting up a tent and building a full-fledged house.\nHowever, DuckDB is not without its limitations. Its in-process nature means it‚Äôs not designed for scenarios where you need a separate, centralized database server accessible to multiple applications or users simultaneously ‚Äì a common scenario in web applications.\nFor data scientists working primarily in Python, DuckDB offers seamless integration, allowing them to stay within their familiar Python environment while performing complex SQL queries on large datasets. This is a significant productivity boost, as it eliminates the need to switch between different tools and interfaces.\nIn comparison, traditional databases like MySQL or PostgreSQL might offer more in terms of advanced features, security, and scalability for application development, but they also come with more overhead in terms of setup and maintenance. They are more like trucks, suitable for heavy lifting and long journeys, whereas DuckDB is more like a nimble bike, perfect for quick trips within the city.\nIn summary, DuckDB is a compact, efficient, and easy-to-use database that excels in data analysis within an application process. It‚Äôs a perfect fit for data analysts and scientists who need speed and efficiency without the overhead of traditional database servers. But for applications requiring a standalone database server, especially in production environments, traditional databases still hold the fort.\nExploring the depths of DuckDB‚Äôs Python package, particularly its latest feature that allows the addition of custom functions, is like opening a box of infinite possibilities. Let‚Äôs get into the weeds a little bit, step by step, aimed at newbies like me who are eager to learn each detail.\n\n\nImagine a toolbox; DuckDB is like a new, sophisticated tool inside it. DuckDB is an in-process SQL (Structured Query Language) database. This means it runs within your program, eliminating the need for a separate database server. Think of it as a helper living inside your Python script, managing data efficiently.\nAs of the time of this writing, it is in its 0.10 realease; however, in its 0.8 release, DuckDB introduced a game-changing feature: allowing users to create their own functions using Python. It‚Äôs like giving you the freedom to create a personalized tool for this toolbox.\nTo better understand this, we introduce another library called ‚ÄòFaker‚Äô. Faker is like a magician in the world of data. It generates fake data ‚Äì names, addresses, emails, etc. This is especially handy for testing and developing, as you often need data that isn‚Äôt.\n\n\n\nIf you do not have either duckdb or faker installed, the following line will help you to do so.\nmamba install duckdb faker -y\n(or use pip without the -y flag, which might be easier).\nThis line is like telling your computer, ‚ÄúHey, please add these two new tools to our kit.‚Äù Mamba or pip are like assistants fetching these tools for us.\nThen, we dive into Python code:\n\nimport faker\nfake = faker.Faker()\n\nHere, we‚Äôre simply saying, ‚ÄúLet‚Äôs start using Faker and create a new Faker generator.‚Äù It‚Äôs like turning on our data-creating magician.\nNext, we define a new function, generate_person:\n\ndef generate_person():\n    person = {\n        'name': fake.name(),\n        'city': fake.city(),\n        'state': fake.state(),\n        'zip_code': fake.zipcode(),\n        'country': fake.country(),\n        'email': fake.email(),\n        'job': fake.job(),\n        'company': fake.company(),\n        'ssn': fake.ssn(),\n        'birthdate': fake.date_of_birth(),\n        'phone_number': fake.phone_number()\n    }\n    return person\n\nThis function is our custom tool. When called, it creates a fake person with details like name, city, and job. Each fake.xxx() calls on Faker to conjure up a piece of data.\n\n\n\nNow, the exciting part: integrating this with DuckDB.\n\nimport duckdb\nfrom duckdb.typing import *\nprint(f\"version: {duckdb.__version__}\")\n\nversion: 0.10.0\n\n\n\ncon = duckdb.connect('udf.duck.db')\n\nHere, we‚Äôre setting up a connection to DuckDB, like opening a line of communication with our database tool.\n\ncon.create_function(\n    'generate_person',\n    generate_person,\n    [],\n    duckdb.struct_type({\n        'name': 'VARCHAR',\n        'city': 'VARCHAR',\n        'state': 'VARCHAR',\n        'zip_code': 'VARCHAR',\n        'country': 'VARCHAR',\n        'email': 'VARCHAR',\n        'job': 'VARCHAR',\n        'company': 'VARCHAR',\n        'ssn': 'VARCHAR',\n        'birthdate': 'DATE',\n        'phone_number': 'VARCHAR'\n    })\n)\n\n&lt;duckdb.duckdb.DuckDBPyConnection at 0x7fd49caca2f0&gt;\n\n\nIn this snippet, we tell DuckDB, ‚ÄúHey, I‚Äôve got this cool function called generate_person. Please use it as if it‚Äôs one of your native functions.‚Äù duckdb.struct_type specifies the structure of data our function returns, much like defining what kind of output our tool will produce.\n\n\n\nFinally, we use our function in a SQL query:\n\nrows = con.sql(\"\"\"\nSELECT generate_person() AS person\nFROM generate_series(1, 10)\n\"\"\").fetchall()\nfor row in rows:\n    print(row)\n\n({'name': 'Jason Johnston Jr.', 'city': 'Pattonshire', 'state': 'Utah', 'zip_code': '83591', 'country': 'Mayotte', 'email': 'brookemorse@example.com', 'job': 'Communications engineer', 'company': 'Brown Group', 'ssn': '566-85-9938', 'birthdate': datetime.date(1921, 12, 13), 'phone_number': '+1-906-990-7965x959'},)\n({'name': 'Jason Johnston Jr.', 'city': 'Pattonshire', 'state': 'Utah', 'zip_code': '83591', 'country': 'Mayotte', 'email': 'brookemorse@example.com', 'job': 'Communications engineer', 'company': 'Brown Group', 'ssn': '566-85-9938', 'birthdate': datetime.date(1921, 12, 13), 'phone_number': '+1-906-990-7965x959'},)\n({'name': 'Jason Johnston Jr.', 'city': 'Pattonshire', 'state': 'Utah', 'zip_code': '83591', 'country': 'Mayotte', 'email': 'brookemorse@example.com', 'job': 'Communications engineer', 'company': 'Brown Group', 'ssn': '566-85-9938', 'birthdate': datetime.date(1921, 12, 13), 'phone_number': '+1-906-990-7965x959'},)\n({'name': 'Jason Johnston Jr.', 'city': 'Pattonshire', 'state': 'Utah', 'zip_code': '83591', 'country': 'Mayotte', 'email': 'brookemorse@example.com', 'job': 'Communications engineer', 'company': 'Brown Group', 'ssn': '566-85-9938', 'birthdate': datetime.date(1921, 12, 13), 'phone_number': '+1-906-990-7965x959'},)\n({'name': 'Jason Johnston Jr.', 'city': 'Pattonshire', 'state': 'Utah', 'zip_code': '83591', 'country': 'Mayotte', 'email': 'brookemorse@example.com', 'job': 'Communications engineer', 'company': 'Brown Group', 'ssn': '566-85-9938', 'birthdate': datetime.date(1921, 12, 13), 'phone_number': '+1-906-990-7965x959'},)\n({'name': 'Jason Johnston Jr.', 'city': 'Pattonshire', 'state': 'Utah', 'zip_code': '83591', 'country': 'Mayotte', 'email': 'brookemorse@example.com', 'job': 'Communications engineer', 'company': 'Brown Group', 'ssn': '566-85-9938', 'birthdate': datetime.date(1921, 12, 13), 'phone_number': '+1-906-990-7965x959'},)\n({'name': 'Jason Johnston Jr.', 'city': 'Pattonshire', 'state': 'Utah', 'zip_code': '83591', 'country': 'Mayotte', 'email': 'brookemorse@example.com', 'job': 'Communications engineer', 'company': 'Brown Group', 'ssn': '566-85-9938', 'birthdate': datetime.date(1921, 12, 13), 'phone_number': '+1-906-990-7965x959'},)\n({'name': 'Jason Johnston Jr.', 'city': 'Pattonshire', 'state': 'Utah', 'zip_code': '83591', 'country': 'Mayotte', 'email': 'brookemorse@example.com', 'job': 'Communications engineer', 'company': 'Brown Group', 'ssn': '566-85-9938', 'birthdate': datetime.date(1921, 12, 13), 'phone_number': '+1-906-990-7965x959'},)\n({'name': 'Jason Johnston Jr.', 'city': 'Pattonshire', 'state': 'Utah', 'zip_code': '83591', 'country': 'Mayotte', 'email': 'brookemorse@example.com', 'job': 'Communications engineer', 'company': 'Brown Group', 'ssn': '566-85-9938', 'birthdate': datetime.date(1921, 12, 13), 'phone_number': '+1-906-990-7965x959'},)\n({'name': 'Jason Johnston Jr.', 'city': 'Pattonshire', 'state': 'Utah', 'zip_code': '83591', 'country': 'Mayotte', 'email': 'brookemorse@example.com', 'job': 'Communications engineer', 'company': 'Brown Group', 'ssn': '566-85-9938', 'birthdate': datetime.date(1921, 12, 13), 'phone_number': '+1-906-990-7965x959'},)\n\n\nThis is where magic happens. We ask DuckDB to create a new table, people, the code is executing a query in DuckDB to generate data for 10 fake people, by our generate_person function, fetching those data rows into Python, and then iterating over these rows to print them out. This is a common pattern for generating and inspecting data in database-driven Python applications, especially in data analysis and testing scenarios. It‚Äôs like telling DuckDB to build a house and fill it with residents, each created by our custom tool.\n\n\n\nBut, there was a twist. This script kept returning the same person over and over. Why? Because DuckDB, trying to be efficient, was reusing the result of the function. It‚Äôs like using a template instead of creating a new person each time.\nThe solution? We modify generate_person to accept a seed:\n\ndef generate_person(seed):\n    person = {\n        'name': fake.name(),\n        'city': fake.city(),\n        'state': fake.state(),\n        'zip_code': fake.zipcode(),\n        'country': fake.country(),\n        'email': fake.email(),\n        'job': fake.job(),\n        'company': fake.company(),\n        'ssn': fake.ssn(),\n        'birthdate': fake.date_of_birth(),\n        'phone_number': fake.phone_number()\n    }\n    return person\n\n\ncon = duckdb.connect('udf.duck.db')\n\ncon.create_function(\n    'generate_person',\n    generate_person,\n    [DOUBLE],\n    duckdb.struct_type({\n        'name': 'VARCHAR',\n        'city': 'VARCHAR',\n        'state': 'VARCHAR',\n        'zip_code': 'VARCHAR',\n        'country': 'VARCHAR',\n        'email': 'VARCHAR',\n        'job': 'VARCHAR',\n        'company': 'VARCHAR',\n        'ssn': 'VARCHAR',\n        'birthdate': 'DATE',\n        'phone_number': 'VARCHAR'\n    })\n)\n\nrows = con.sql(\"\"\"\nSELECT generate_person(random()) AS person\nFROM generate_series(1, 10)\n\"\"\").fetchall()\nfor row in rows:\n    print(row)\n\n({'name': 'Andrea Jordan', 'city': 'Port Lisashire', 'state': 'Connecticut', 'zip_code': '98788', 'country': 'Eritrea', 'email': 'dawngonzalez@example.net', 'job': 'Cabin crew', 'company': 'Barton-Fernandez', 'ssn': '894-92-7787', 'birthdate': datetime.date(2003, 10, 21), 'phone_number': '836-318-0192'},)\n({'name': 'Cynthia Welch', 'city': 'South Amandaton', 'state': 'Kansas', 'zip_code': '40133', 'country': 'Uruguay', 'email': 'daniel79@example.org', 'job': 'Education administrator', 'company': 'Lee, Poole and Perez', 'ssn': '352-89-7570', 'birthdate': datetime.date(1932, 4, 29), 'phone_number': '550-697-9011x72175'},)\n({'name': 'Hannah Glass', 'city': 'Port Steven', 'state': 'New Hampshire', 'zip_code': '31498', 'country': 'Thailand', 'email': 'joshuasolis@example.com', 'job': 'Engineer, biomedical', 'company': 'Wagner-Chan', 'ssn': '157-07-7981', 'birthdate': datetime.date(1994, 4, 6), 'phone_number': '714.364.1179'},)\n({'name': 'April Gonzalez', 'city': 'North Robert', 'state': 'Wyoming', 'zip_code': '35103', 'country': 'Guinea', 'email': 'hallpaul@example.com', 'job': 'Sub', 'company': 'Santos, Fitzgerald and Fritz', 'ssn': '346-33-8946', 'birthdate': datetime.date(2007, 11, 3), 'phone_number': '635-712-9242'},)\n({'name': 'Timothy Smith Jr.', 'city': 'New Gregory', 'state': 'South Carolina', 'zip_code': '83948', 'country': 'Israel', 'email': 'newmancarol@example.net', 'job': 'Research scientist (life sciences)', 'company': 'Hunter LLC', 'ssn': '717-29-2637', 'birthdate': datetime.date(1938, 11, 1), 'phone_number': '454-311-6022x036'},)\n({'name': 'Kathleen Watson', 'city': 'New Monique', 'state': 'Missouri', 'zip_code': '45261', 'country': 'Bolivia', 'email': 'cohenrachel@example.com', 'job': 'Community development worker', 'company': 'Hansen, Hurley and Gibson', 'ssn': '353-51-7806', 'birthdate': datetime.date(1991, 5, 15), 'phone_number': '693.901.5298'},)\n({'name': 'Thomas Snyder', 'city': 'South Karen', 'state': 'South Dakota', 'zip_code': '72363', 'country': 'Moldova', 'email': 'jessicawu@example.com', 'job': 'Medical secretary', 'company': 'Calderon and Sons', 'ssn': '347-11-7234', 'birthdate': datetime.date(1982, 9, 14), 'phone_number': '+1-503-484-5672x006'},)\n({'name': 'Sheena Parks', 'city': 'Port Donald', 'state': 'Vermont', 'zip_code': '94876', 'country': 'Spain', 'email': 'gfrank@example.org', 'job': 'Minerals surveyor', 'company': 'Mills Ltd', 'ssn': '020-14-2155', 'birthdate': datetime.date(1913, 10, 5), 'phone_number': '400.459.3822'},)\n({'name': 'Melissa Mason', 'city': 'West Alexander', 'state': 'Kansas', 'zip_code': '62074', 'country': 'Bahamas', 'email': 'holmesmark@example.org', 'job': 'Surveyor, building control', 'company': 'Jimenez-Mccall', 'ssn': '615-81-9873', 'birthdate': datetime.date(1957, 6, 14), 'phone_number': '(628)905-5737x84834'},)\n({'name': 'Anna Olsen', 'city': 'Deanhaven', 'state': 'South Carolina', 'zip_code': '88465', 'country': 'Bangladesh', 'email': 'wcastaneda@example.com', 'job': 'Environmental manager', 'company': 'Herman and Sons', 'ssn': '503-96-4916', 'birthdate': datetime.date(1951, 1, 4), 'phone_number': '9925956906'},)\n\n\nNow, each call to generate_person is unique, thanks to the random() function in our SQL query. It‚Äôs like telling the function, ‚ÄúPlease create a new, unique person each time.‚Äù\nWith this approach, we now have a powerful and flexible way to generate test data using DuckDB and Python, tailored to our needs. For beginners, it‚Äôs like learning to create and use customized tools in a workshop, opening up a world of possibilities in data handling and manipulation.\n\ncon.sql(\"\"\"\nCREATE OR REPLACE TABLE people AS\nSELECT person.* FROM (\n    SELECT generate_person(random()) AS person  \n    FROM generate_series(1, 10000)\n    )\n\"\"\")\n\n\n\n\nThis code snippet is creating a table and populating it with generated data. Let‚Äôs break down what this code is doing:\n\nCREATE OR REPLACE TABLE people AS:\n\nThis command creates a new table named people in the DuckDB database. If a table with this name already exists, it will be replaced (essentially, it‚Äôs deleted and recreated). This is useful for refreshing the data or structure of the table without having to manually drop it first.\n\nThe Inner SELECT Statement:\n\nSELECT generate_person(random()) AS person:\n\nThis part of the query generates a row of data using the generate_person function.\nThe generate_person function is assumed to be a user-defined function (UDF) in DuckDB that generates a structured record of a person with various attributes like name, city, etc.\nrandom() is a function that generates a random number. It seems to be passed as an argument to generate_person, likely to ensure that each call to generate_person results in a unique record by using a different random seed.\nAS person aliases the result of generate_person(random()) as person. This means that in the context of this query, the output of generate_person(random()) is referred to as person.\n\n\nFROM generate_series(1, 10000):\n\nThis part generates a series of numbers from 1 to 10,000. For each number in this series, the SELECT generate_person(random()) is executed. This means that the generate_person function is called 10,000 times, creating 10,000 unique records.\n\nThe Outer SELECT Statement:\n\nSELECT person.* FROM (...):\n\nThis outer SELECT statement selects all columns from each record generated by generate_person(random()) and arranges them into rows of the people table. The use of person.* indicates that all attributes of the person record should be included in the table.\n\n\n\nIn summary, this code is used to create a table named people in a DuckDB database and populate it with 10,000 unique rows of data, each row representing a person with various attributes. The data for each person is generated by the generate_person function, which appears to create diverse and random data for each person. This technique is especially useful for generating large sets of test data.\n\n\n\nWe will now employ various methods to verify and examine the data we have generated and stored in our database.\n\nquery = \"SELECT * FROM people ORDER BY RANDOM() LIMIT 5;\"\n\n\nresult = con.execute(query)\n\n\nrows = result.fetchall()\n\n\nfor row in rows:\n    print(row)\n\n('Brian Wolf', 'Waynehaven', 'Wisconsin', '08333', 'Cape Verde', 'dana74@example.net', 'Financial adviser', 'Russell LLC', '040-05-7683', datetime.date(2002, 8, 22), '240-447-8269')\n('Jerry Smith', 'Port Michael', 'Washington', '84743', 'Pakistan', 'melissa67@example.com', 'Programmer, systems', 'Moore, Foster and Mitchell', '374-58-8406', datetime.date(2011, 4, 5), '657-346-6548x310')\n('Tony Gonzales', 'New Heather', 'Illinois', '84194', 'Heard Island and McDonald Islands', 'ann77@example.com', 'Producer, television/film/video', 'Miller Group', '438-59-1081', datetime.date(1987, 9, 11), '+1-916-914-5582x8441')\n('David Hickman', 'North Eric', 'Louisiana', '55622', 'Argentina', 'matalarry@example.net', 'Dance movement psychotherapist', 'Rivera, Pittman and Orozco', '540-74-0081', datetime.date(2015, 12, 3), '313-341-7514x72204')\n('Nancy Clements', 'Stephaniemouth', 'Delaware', '00939', 'Gambia', 'sanderssarah@example.org', 'Arts administrator', 'Williams-Brown', '670-26-5114', datetime.date(1959, 8, 1), '759.858.5832x66035')\n\n\n\n#  Execute the First Query\nquery = \"SELECT COUNT(*) FROM people;\"\nrow_count = con.execute(query).fetchone()\n\n# Display the Result\nprint(\"Number of rows in 'people':\", row_count[0])\n\nNumber of rows in 'people': 10000\n\n\n\n# Execute the Second Query\nquery = \"SELECT * FROM people LIMIT 5;\"\nrows = con.execute(query).fetchall()\n\n# Display the Results\nfor row in rows:\n    print(row)\n\n('Stacey Pratt', 'Port Daniel', 'Arkansas', '85990', 'Syrian Arab Republic', 'uwarner@example.net', 'Therapist, speech and language', 'Sullivan-Thomas', '515-58-6703', datetime.date(2024, 2, 9), '+1-966-832-1511x417')\n('David Watkins', 'Skinnerborough', 'Maryland', '44041', 'Macao', 'sedwards@example.org', 'Engineer, manufacturing', 'Weaver LLC', '035-51-1158', datetime.date(1971, 3, 10), '827-546-2880')\n('Jonathan Sampson', 'Hannahtown', 'Nebraska', '67966', 'Tunisia', 'andreamcgee@example.com', 'Trade mark attorney', 'Fields-Henderson', '123-98-2248', datetime.date(2010, 7, 4), '3097651941')\n('Thomas Coleman', 'Sabrinashire', 'Oklahoma', '28417', 'Saudi Arabia', 'marieyang@example.org', 'Film/video editor', 'Tran, Anderson and Smith', '651-45-3497', datetime.date(1957, 5, 31), '001-565-609-3892')\n('Suzanne Jackson', 'Jessicaville', 'Minnesota', '95997', 'Antarctica (the territory South of 60 deg S)', 'kmadden@example.com', 'Air cabin crew', 'Hart-Wagner', '828-72-9535', datetime.date(2008, 12, 1), '875.786.6969x5960')\n\n\nAll appears to have gone smoothly.\nThat concludes this post. I trust you found as much pleasure in reading it as I did in writing it.\n\nThis insightful exploration into the functionalities of DuckDB, particularly the creation of dummy data using user-defined functions, is inspired by the work of Mark Needham. Mark is a prolific content creator in the data field, currently focusing on short-form content at ClickHouse. His expertise is evident in his engaging YouTube channel, LearnDataWithMark, where he simplifies complex data problems into concise, 5-minute tutorials. His previous experience at Neo4j, especially in graph analytics, further solidifies his stature in the community, as does his co-authorship of the O‚ÄôReilly Graph Algorithms Book with Amy Hodler. The original post that sparked this discussion can be found on Mark‚Äôs blog here. Mark‚Äôs clear and practical approach to data is not only enlightening but also instrumental in making advanced data concepts accessible to a wider audience."
  },
  {
    "objectID": "blog/2023-December-27-Data-Wrangling/Data_Wrangling.html",
    "href": "blog/2023-December-27-Data-Wrangling/Data_Wrangling.html",
    "title": "Data Wrangling",
    "section": "",
    "text": "Data wrangling, often also referred to as data munging, is the process of cleaning, structuring, and enriching raw data into a desired format for better decision-making. It‚Äôs a fundamental step in the data preparation process before analysis or processing. Data wrangling involves several tasks and can be quite complex depending on the state of the data and the desired outcome.\n\n\nData Cleaning: This involves handling missing or inconsistent data, correcting inaccuracies, and dealing with outliers. It might require filling missing values, smoothing noisy data, identifying or removing outliers, and resolving inconsistencies.\nData Transformation: This step is about converting data from one format or structure into another. This could involve aggregating data, normalizing or standardizing data (like bringing data to a common scale), or transforming data types.\nData Integration: In many cases, data needs to be combined from multiple sources. This could involve merging data from different databases, files, or formats, and aligning them into a single coherent data set.\nData Reduction: Large datasets are often reduced to a manageable size while maintaining their integrity. This can involve techniques like dimensionality reduction, binning, histograms, clustering, or principal component analysis (PCA).\nData Enrichment: Enhancing data by merging additional relevant information from external sources. This can provide more context for analysis or improve the accuracy of predictive models.\n\n\n\nQuality of Insights: Better data wrangling leads to higher-quality data, which in turn can produce more accurate and insightful analysis results.\nTime Efficiency: Although time-consuming, efficient data wrangling can save time in the long run by making data easier to work with and reducing errors in the analysis phase.\nImproved Decision Making: High-quality, well-structured data enables data scientists and businesses to make more informed and accurate decisions."
  },
  {
    "objectID": "blog/2023-December-27-Data-Wrangling/Data_Wrangling.html#introduction",
    "href": "blog/2023-December-27-Data-Wrangling/Data_Wrangling.html#introduction",
    "title": "Data Wrangling",
    "section": "",
    "text": "Data wrangling, often also referred to as data munging, is the process of cleaning, structuring, and enriching raw data into a desired format for better decision-making. It‚Äôs a fundamental step in the data preparation process before analysis or processing. Data wrangling involves several tasks and can be quite complex depending on the state of the data and the desired outcome.\n\n\nData Cleaning: This involves handling missing or inconsistent data, correcting inaccuracies, and dealing with outliers. It might require filling missing values, smoothing noisy data, identifying or removing outliers, and resolving inconsistencies.\nData Transformation: This step is about converting data from one format or structure into another. This could involve aggregating data, normalizing or standardizing data (like bringing data to a common scale), or transforming data types.\nData Integration: In many cases, data needs to be combined from multiple sources. This could involve merging data from different databases, files, or formats, and aligning them into a single coherent data set.\nData Reduction: Large datasets are often reduced to a manageable size while maintaining their integrity. This can involve techniques like dimensionality reduction, binning, histograms, clustering, or principal component analysis (PCA).\nData Enrichment: Enhancing data by merging additional relevant information from external sources. This can provide more context for analysis or improve the accuracy of predictive models.\n\n\n\nQuality of Insights: Better data wrangling leads to higher-quality data, which in turn can produce more accurate and insightful analysis results.\nTime Efficiency: Although time-consuming, efficient data wrangling can save time in the long run by making data easier to work with and reducing errors in the analysis phase.\nImproved Decision Making: High-quality, well-structured data enables data scientists and businesses to make more informed and accurate decisions."
  },
  {
    "objectID": "blog/2023-December-27-Data-Wrangling/Data_Wrangling.html#method-chaining-in-pandas",
    "href": "blog/2023-December-27-Data-Wrangling/Data_Wrangling.html#method-chaining-in-pandas",
    "title": "Data Wrangling",
    "section": "Method Chaining in Pandas",
    "text": "Method Chaining in Pandas\nMethod chaining in Pandas is a powerful feature that refers to the practice of executing multiple operations in a single, streamlined expression using method calls linked together. This is simply executing multiple operations in a sequence, where the output of one operation feeds directly into the input of the next. This stylistic feature in Python enhances readability and conciseness of code, and is particularly effective due to Pandas‚Äô method-based syntax.\n\nAdvantages of Chaining\n\nReadability and Clarity: Chaining enhances readability by presenting operations in a logical, top-to-bottom order.\nIntermediate Variable Reduction: It eliminates the need for intermediate variables, thus reducing workspace clutter and error risk.\nEncourages Functional Style: Promotes a functional programming style, leading to fewer side effects.\nEase of Debugging: Facilitates debugging through the insertion of debugging steps within the chain, such as using .pipe().\n\n\n\nTraditional Approach vs Chaining\nTraditional Approach:\ndf = pd.read_csv('data.csv')\ndf = df.dropna(subset=['column1'])\ndf['column2'] = df['column2'].apply(lambda x: x*2)\ndf = df[df['column3'] &gt; 0]\nChaining Approach:\ndf = (pd.read_csv('data.csv')\n      .dropna(subset=['column1'])\n      .assign(column2=lambda df: df['column2']*2)\n      .query('column3 &gt; 0'))\n\n\nAvoiding inplace=True\nIt‚Äôs recommended to avoid inplace=True in Pandas operations for clarity, predictability, and consistency, as well as to enable chaining.\n\n\nUtilizing Chaining Methods\n\n.assign()\nUsed for adding new columns in a chainable way.\nNew Way:\ndf = df.assign(new_column=lambda x: x['existing_column'] * 2)\n\n\n.pipe()\nAllows applying a function in the middle of a chain.\nNew Way:\ndf = (pd.read_csv('data.csv')\n      .pipe(process_data)\n      .query('column &gt; 0'))\n\n\n.query()\nProvides a more readable way to filter rows using a string expression.\nNew Way:\ndf = df.query('column &gt; 0')\n\n\n\nExample of Extended Chaining\ndf = (pd.read_csv('data.csv')\n      .fillna(...)\n      .query('some_condition')\n      .assign(new_column=lambda df: df.cut(...))\n      .pivot_table(...)\n      .rename(...))\n\n\nLambda functions in chaining\nNormal functions are defined using the def keyword and have a name. They are suitable for:\n\nRepeated Use: When the same functionality is needed in multiple places.\nComplex Logic: For operations that involve multiple steps or complex logic.\nReadability: Named functions can make code more readable by clearly stating their purpose.\nTesting: Easier to test and debug since they are standalone entities.\n\nExample of a normal function:\ndef double(x):\n    return x * 2\nLambda functions are small, one-line functions defined without a name. They are ideal for:\n\nSimplicity and Conciseness: When the function is simple enough to be expressed in a single line.\nOne-Time Use: Particularly useful for quick, throwaway functions that are not needed elsewhere.\nChaining: In Pandas, lambdas are often used in methods like .apply(), .map(), .assign(), and .filter() for inline operations.\n\nExample of an anonymous function:\nlambda x: x * 2\n\nUsing Anonymous Functions in Chaining\nAnonymous functions are highly useful in chaining for their brevity and inline nature. For instance:\ndf = (pd.read_csv('data.csv')\n      .assign(column2=lambda x: x['column1'] * 2)\n      .query(lambda x: x['column3'] &gt; 0))\n\n\nWhen to Use and When Not to Use Lambda Functions\nWhen to Use: - Short, Simple Operations: For short, one-off transformations that don‚Äôt need to be reused. - Inline Transformations: When working with Pandas chaining and the operation can be concisely expressed in a single line.\nWhen Not to Use: - Complex Operations: If the logic is too complex for a single line, a normal function is more suitable. - Readability Concerns: If using a lambda function makes the code hard to understand, a named function is preferable. - Reuse Across the Codebase: If the same functionality is needed in multiple places, define it once with a normal function.\nLet us clean with some real world data next. We are opting out for the bike rentals dataset from the UCI Machine Learning Repository, a world-famous data warehouse that is free to the public.\n\nimport pandas as pd\nimport datetime as dt\n\n\nurl = 'https://media.githubusercontent.com/media/theAfricanQuant/XGBoost4machinelearning/main/data/bike_rentals.csv'\n\nWhat we will do is to create a function that receives the url and prints out the dataframe.\n\ndef get_data(url):\n    return (\n        pd.read_csv(url)\n    )\n\ndf_bikes = get_data(url)\ndf_bikes.sample(n=5, random_state=43)\n\n\n\n\n\n\n\n\ninstant\ndteday\nseason\nyr\nmnth\nholiday\nweekday\nworkingday\nweathersit\ntemp\natemp\nhum\nwindspeed\ncasual\nregistered\ncnt\n\n\n\n\n335\n336\n2011-12-02\n4.0\n0.0\n12.0\n0.0\n5.0\n1.0\n1\n0.314167\n0.331433\n0.625833\n0.100754\n268\n3672\n3940\n\n\n631\n632\n2012-09-23\n4.0\n1.0\n9.0\n0.0\n0.0\n0.0\n1\n0.529167\n0.518933\n0.467083\n0.223258\n2454\n5453\n7907\n\n\n620\n621\n2012-09-12\n3.0\n1.0\n9.0\n0.0\n3.0\n1.0\n1\n0.599167\n0.570075\n0.577083\n0.131846\n1050\n6820\n7870\n\n\n722\n723\n2012-12-23\n1.0\n1.0\n12.0\n0.0\n0.0\n0.0\n1\n0.245833\n0.259471\n0.515417\n0.133083\n408\n1379\n1787\n\n\n388\n389\n2012-01-24\n1.0\n1.0\n1.0\n0.0\n2.0\n1.0\n1\n0.342500\n0.349108\nNaN\n0.123767\n439\n3900\n4339\n\n\n\n\n\n\n\n\ndf_bikes.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 731 entries, 0 to 730\nData columns (total 16 columns):\n #   Column      Non-Null Count  Dtype  \n---  ------      --------------  -----  \n 0   instant     731 non-null    int64  \n 1   dteday      731 non-null    object \n 2   season      731 non-null    float64\n 3   yr          730 non-null    float64\n 4   mnth        730 non-null    float64\n 5   holiday     731 non-null    float64\n 6   weekday     731 non-null    float64\n 7   workingday  731 non-null    float64\n 8   weathersit  731 non-null    int64  \n 9   temp        730 non-null    float64\n 10  atemp       730 non-null    float64\n 11  hum         728 non-null    float64\n 12  windspeed   726 non-null    float64\n 13  casual      731 non-null    int64  \n 14  registered  731 non-null    int64  \n 15  cnt         731 non-null    int64  \ndtypes: float64(10), int64(5), object(1)\nmemory usage: 91.5+ KB\n\n\nFrom the output above, we can see that even though we should have about 731 entries for all, some columns do not have up to that. We will use data wrangling to handle that using our pandas method chaining.\n\n(df_bikes\n .isna()\n .sum()\n .sum()\n)\n\n12\n\n\nThe output above tells us we have exactly 12 null values in the entire dataframe before us.\nWe will create a function called show_nulls that will only show us the rows with the missing values as we begin to wrangle through our data.\n\ndef show_nulls(df):\n    return (df[df\n            .isna()\n            .any(axis=1)]\n           )\n\nshow_nulls(df_bikes)\n\n\n\n\n\n\n\n\ninstant\ndteday\nseason\nyr\nmnth\nholiday\nweekday\nworkingday\nweathersit\ntemp\natemp\nhum\nwindspeed\ncasual\nregistered\ncnt\n\n\n\n\n56\n57\n2011-02-26\n1.0\n0.0\n2.0\n0.0\n6.0\n0.0\n1\n0.282500\n0.282192\n0.537917\nNaN\n424\n1545\n1969\n\n\n81\n82\n2011-03-23\n2.0\n0.0\n3.0\n0.0\n3.0\n1.0\n2\n0.346957\n0.337939\n0.839565\nNaN\n203\n1918\n2121\n\n\n128\n129\n2011-05-09\n2.0\n0.0\n5.0\n0.0\n1.0\n1.0\n1\n0.532500\n0.525246\n0.588750\nNaN\n664\n3698\n4362\n\n\n129\n130\n2011-05-10\n2.0\n0.0\n5.0\n0.0\n2.0\n1.0\n1\n0.532500\n0.522721\nNaN\n0.115671\n694\n4109\n4803\n\n\n213\n214\n2011-08-02\n3.0\n0.0\n8.0\n0.0\n2.0\n1.0\n1\n0.783333\n0.707071\nNaN\n0.205850\n801\n4044\n4845\n\n\n298\n299\n2011-10-26\n4.0\n0.0\n10.0\n0.0\n3.0\n1.0\n2\n0.484167\n0.472846\n0.720417\nNaN\n404\n3490\n3894\n\n\n388\n389\n2012-01-24\n1.0\n1.0\n1.0\n0.0\n2.0\n1.0\n1\n0.342500\n0.349108\nNaN\n0.123767\n439\n3900\n4339\n\n\n528\n529\n2012-06-12\n2.0\n1.0\n6.0\n0.0\n2.0\n1.0\n2\n0.653333\n0.597875\n0.833333\nNaN\n477\n4495\n4972\n\n\n701\n702\n2012-12-02\n4.0\n1.0\n12.0\n0.0\n0.0\n0.0\n2\nNaN\nNaN\n0.823333\n0.124379\n892\n3757\n4649\n\n\n730\n731\n2012-12-31\n1.0\nNaN\nNaN\n0.0\n1.0\n0.0\n2\n0.215833\n0.223487\n0.577500\n0.154846\n439\n2290\n2729\n\n\n\n\n\n\n\nWe will replace the null values in the windspeed column with the median. We choose to use the median over the mean because the median tends to guarantee that half the data is greater than the given value while the other half of the data is lower. The mean, by contrast, is vulnerable to outliers.\nThis is the begining of building our wrangle function and we will call it prep_data. We will continue to build it step by step until the end. We will now use the .assign method to start our chain.\n\ndef prep_data(data):\n    return (data\n            .assign(windspeed = data[\"windspeed\"]\n                    .fillna((data[\"windspeed\"]\n                             .median())))           \n           )\n\nbikes = prep_data(df_bikes)\nshow_nulls(bikes)\n\n\n\n\n\n\n\n\ninstant\ndteday\nseason\nyr\nmnth\nholiday\nweekday\nworkingday\nweathersit\ntemp\natemp\nhum\nwindspeed\ncasual\nregistered\ncnt\n\n\n\n\n129\n130\n2011-05-10\n2.0\n0.0\n5.0\n0.0\n2.0\n1.0\n1\n0.532500\n0.522721\nNaN\n0.115671\n694\n4109\n4803\n\n\n213\n214\n2011-08-02\n3.0\n0.0\n8.0\n0.0\n2.0\n1.0\n1\n0.783333\n0.707071\nNaN\n0.205850\n801\n4044\n4845\n\n\n388\n389\n2012-01-24\n1.0\n1.0\n1.0\n0.0\n2.0\n1.0\n1\n0.342500\n0.349108\nNaN\n0.123767\n439\n3900\n4339\n\n\n701\n702\n2012-12-02\n4.0\n1.0\n12.0\n0.0\n0.0\n0.0\n2\nNaN\nNaN\n0.823333\n0.124379\n892\n3757\n4649\n\n\n730\n731\n2012-12-31\n1.0\nNaN\nNaN\n0.0\n1.0\n0.0\n2\n0.215833\n0.223487\n0.577500\n0.154846\n439\n2290\n2729\n\n\n\n\n\n\n\nWe can now see that our output has shrunk a bit cos we have successfully eliminated the null values in the windspeed column.\nIt is possible to get more nuanced when correcting null values by using a groupby. A groupby organizes rows by shared values. Since there are four shared seasons spread out among the rows, a groupby of seasons results in a total of four rows, one for each season. But each season comes from many different rows with different values. We need a way to combine, or aggregate, the values. Choices for the aggregate include .sum(), .count(), .mean(), and .median().\nGrouping our dataframe by the season column with the .median(numeric_only=True) aggregate is shown below. numeric_only=True is a parameter that tells the method to ignore non-numeric columns and calculate the median only for numeric columns. If our DataFrame contains non-numeric columns (like strings or dates), they will not be included in the median calculation.\n\nbikes.groupby(['season']).median(numeric_only=True)\n\n\n\n\n\n\n\n\ninstant\nyr\nmnth\nholiday\nweekday\nworkingday\nweathersit\ntemp\natemp\nhum\nwindspeed\ncasual\nregistered\ncnt\n\n\nseason\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1.0\n366.0\n0.5\n2.0\n0.0\n3.0\n1.0\n1.0\n0.285833\n0.282821\n0.543750\n0.202750\n218.0\n1867.0\n2209.0\n\n\n2.0\n308.5\n0.5\n5.0\n0.0\n3.0\n1.0\n1.0\n0.562083\n0.538212\n0.646667\n0.191546\n867.0\n3844.0\n4941.5\n\n\n3.0\n401.5\n0.5\n8.0\n0.0\n3.0\n1.0\n1.0\n0.714583\n0.656575\n0.635833\n0.165115\n1050.5\n4110.5\n5353.5\n\n\n4.0\n493.0\n0.5\n11.0\n0.0\n3.0\n1.0\n1.0\n0.410000\n0.409708\n0.661042\n0.167918\n544.5\n3815.0\n4634.5\n\n\n\n\n\n\n\nTo correct the null values in the ‚Äòhum‚Äô column (humidity), we can take the median humidity by season.\nbikes['hum'] = bikes['hum'].fillna()\nThe code that goes inside fillna is the desired values. The values obtained from groupby require the transform method as follows:\nbikes.groupby('season')['hum'].transform('median')\nBringing everything together:\nbikes['hum'] = (bikes['hum']\n                   .fillna(bikes.groupby('season')['hum']\n                           .transform('median'))\n                  )\nHowever, to implement it we are going to use the method of chaining and the .assign method in our code.\n\ndef prep_data(data):\n    return (data\n            .assign(windspeed = data[\"windspeed\"]\n                    .fillna((data[\"windspeed\"]\n                             .median())),\n                    hum = (data['hum']\n                   .fillna(data.groupby('season')['hum']\n                           .transform('median'))\n                  )\n                   )           \n           )\n\nbikes = prep_data(df_bikes)\nshow_nulls(bikes)\n\n\n\n\n\n\n\n\ninstant\ndteday\nseason\nyr\nmnth\nholiday\nweekday\nworkingday\nweathersit\ntemp\natemp\nhum\nwindspeed\ncasual\nregistered\ncnt\n\n\n\n\n701\n702\n2012-12-02\n4.0\n1.0\n12.0\n0.0\n0.0\n0.0\n2\nNaN\nNaN\n0.823333\n0.124379\n892\n3757\n4649\n\n\n730\n731\n2012-12-31\n1.0\nNaN\nNaN\n0.0\n1.0\n0.0\n2\n0.215833\n0.223487\n0.577500\n0.154846\n439\n2290\n2729\n\n\n\n\n\n\n\nIn some cases, it may be advantageous to replace null values with data from specific rows.\nWhen correcting temperature, aside from consulting historical records, taking the mean temperature of the day before and the day after should give a good estimate.\nTo find null values of the ‚Äòtemp‚Äô column, enter the following code:\n\nbikes[bikes['temp'].isna()]\n\n\n\n\n\n\n\n\ninstant\ndteday\nseason\nyr\nmnth\nholiday\nweekday\nworkingday\nweathersit\ntemp\natemp\nhum\nwindspeed\ncasual\nregistered\ncnt\n\n\n\n\n701\n702\n2012-12-02\n4.0\n1.0\n12.0\n0.0\n0.0\n0.0\n2\nNaN\nNaN\n0.823333\n0.124379\n892\n3757\n4649\n\n\n\n\n\n\n\nFrom the output above, index 701 contains null values.\nWe will now find the mean temperature of the day before and the day after the 701 index.\nLet us sum the temperatures in rows 700 and 702 and divide by 2 for both the ‚Äòtemp‚Äô and ‚Äòatemp‚Äô columns. We will create a function to do that so we can pass that to the chain.\n\ndef mean_vals(df, idx1, idx2, col):\n    return (\n        (df.iloc[idx1][col] + \n        df.iloc[idx2][col])/2\n    )\n\nmean_vals(bikes, 700, 702, 'atemp')\n\n0.38634999999999997\n\n\n\ndef prep_data(data):\n    return (data\n            .assign(windspeed = data[\"windspeed\"]\n                    .fillna((data[\"windspeed\"]\n                             .median())),\n                    hum = (data['hum']\n                   .fillna(data.groupby('season')['hum']\n                           .transform('median'))),\n                    temp = (data['temp']\n                            .fillna(mean_vals(data, 700, 702, 'temp'))),\n                    atemp = (data['atemp']\n                            .fillna(mean_vals(data, 700, 702, 'atemp')))\n                   )           \n           )\n\nbikes = prep_data(df_bikes)\nshow_nulls(bikes)\n\n\n\n\n\n\n\n\ninstant\ndteday\nseason\nyr\nmnth\nholiday\nweekday\nworkingday\nweathersit\ntemp\natemp\nhum\nwindspeed\ncasual\nregistered\ncnt\n\n\n\n\n730\n731\n2012-12-31\n1.0\nNaN\nNaN\n0.0\n1.0\n0.0\n2\n0.215833\n0.223487\n0.5775\n0.154846\n439\n2290\n2729\n\n\n\n\n\n\n\nThe dteday is meant to be a date column but the .info we ran earlier revealed to us that it was an object or a string. Date objects such as years and months must be extrapolated from datetime types. Lets convert the column to a datetime.\n\ndef prep_data(data):\n    return (data\n            .assign(windspeed = data[\"windspeed\"]\n                    .fillna((data[\"windspeed\"]\n                             .median())),\n                    hum = (data['hum']\n                   .fillna(data.groupby('season')['hum']\n                           .transform('median'))),\n                    temp = (data['temp']\n                            .fillna(mean_vals(data, 700, 702, 'temp'))),\n                    atemp = (data['atemp']\n                            .fillna(mean_vals(data, 700, 702, 'atemp'))),\n                    dteday = pd.to_datetime(data['dteday'])\n                   )           \n           )\n\nbikes = (prep_data(df_bikes)\n        .info())\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 731 entries, 0 to 730\nData columns (total 16 columns):\n #   Column      Non-Null Count  Dtype         \n---  ------      --------------  -----         \n 0   instant     731 non-null    int64         \n 1   dteday      731 non-null    datetime64[ns]\n 2   season      731 non-null    float64       \n 3   yr          730 non-null    float64       \n 4   mnth        730 non-null    float64       \n 5   holiday     731 non-null    float64       \n 6   weekday     731 non-null    float64       \n 7   workingday  731 non-null    float64       \n 8   weathersit  731 non-null    int64         \n 9   temp        731 non-null    float64       \n 10  atemp       731 non-null    float64       \n 11  hum         731 non-null    float64       \n 12  windspeed   731 non-null    float64       \n 13  casual      731 non-null    int64         \n 14  registered  731 non-null    int64         \n 15  cnt         731 non-null    int64         \ndtypes: datetime64[ns](1), float64(10), int64(5)\nmemory usage: 91.5 KB\n\n\nThe output shows us that we have successfully converted the dteday column to a DateTime object.\nWe will convert the mnth column to the correct months extrpolated from the dteday column. We now introduce the use of the lambda function here or else we get an error. Without a lambda function, there‚Äôs a risk that the operation may refer to an outdated state of the DataFrame, especially if the referenced columns are being modified in the same .assign() method, which is actualyy our case here.\n\ndef prep_data(data):\n    return (data\n            .assign(windspeed = data[\"windspeed\"]\n                    .fillna((data[\"windspeed\"]\n                             .median())),\n                    hum = (data['hum']\n                   .fillna(data.groupby('season')['hum']\n                           .transform('median'))),\n                    temp = (data['temp']\n                            .fillna(mean_vals(data, 700, 702, 'temp'))),\n                    atemp = (data['atemp']\n                            .fillna(mean_vals(data, 700, 702, 'atemp'))),\n                    dteday = pd.to_datetime(data['dteday']),\n                    mnth = lambda x: x['dteday'].dt.month\n                   )           \n           )\n\nbikes = prep_data(df_bikes)\nshow_nulls(bikes)\n\n\n\n\n\n\n\n\ninstant\ndteday\nseason\nyr\nmnth\nholiday\nweekday\nworkingday\nweathersit\ntemp\natemp\nhum\nwindspeed\ncasual\nregistered\ncnt\n\n\n\n\n730\n731\n2012-12-31\n1.0\nNaN\n12\n0.0\n1.0\n0.0\n2\n0.215833\n0.223487\n0.5775\n0.154846\n439\n2290\n2729\n\n\n\n\n\n\n\nLet us check the last 5 values of the dataset we have worked on so far.\n\nbikes.tail(5)\n\n\n\n\n\n\n\n\ninstant\ndteday\nseason\nyr\nmnth\nholiday\nweekday\nworkingday\nweathersit\ntemp\natemp\nhum\nwindspeed\ncasual\nregistered\ncnt\n\n\n\n\n726\n727\n2012-12-27\n1.0\n1.0\n12\n0.0\n4.0\n1.0\n2\n0.254167\n0.226642\n0.652917\n0.350133\n247\n1867\n2114\n\n\n727\n728\n2012-12-28\n1.0\n1.0\n12\n0.0\n5.0\n1.0\n2\n0.253333\n0.255046\n0.590000\n0.155471\n644\n2451\n3095\n\n\n728\n729\n2012-12-29\n1.0\n1.0\n12\n0.0\n6.0\n0.0\n2\n0.253333\n0.242400\n0.752917\n0.124383\n159\n1182\n1341\n\n\n729\n730\n2012-12-30\n1.0\n1.0\n12\n0.0\n0.0\n0.0\n1\n0.255833\n0.231700\n0.483333\n0.350754\n364\n1432\n1796\n\n\n730\n731\n2012-12-31\n1.0\nNaN\n12\n0.0\n1.0\n0.0\n2\n0.215833\n0.223487\n0.577500\n0.154846\n439\n2290\n2729\n\n\n\n\n\n\n\nWe can see that even though the year value on the dteday column has 2012 all through, the value on the yr column is 1.0. It probably means that the values have been normalized, probably between 0 & 1. I think this was done because normalized data is often more efficient due to the fact that machine learning weights do not have to adjust for different ranges.\nWe will just use the forward fill for the null values here since the row with the null value is in the same month with the preceding row:\ndata['yr'].ffill()\n\ndef prep_data(data):\n    return (data\n            .assign(windspeed = data[\"windspeed\"]\n                    .fillna((data[\"windspeed\"]\n                             .median())),\n                    hum = (data['hum']\n                   .fillna(data.groupby('season')['hum']\n                           .transform('median'))),\n                    temp = (data['temp']\n                            .fillna(mean_vals(data, 700, 702, 'temp'))),\n                    atemp = (data['atemp']\n                            .fillna(mean_vals(data, 700, 702, 'atemp'))),\n                    dteday = pd.to_datetime(data['dteday']),\n                    mnth = lambda x: x['dteday'].dt.month,\n                    yr = data['yr'].ffill()\n                   )           \n           )\n\nbikes = prep_data(df_bikes)\nshow_nulls(bikes)\n\n\n\n\n\n\n\n\ninstant\ndteday\nseason\nyr\nmnth\nholiday\nweekday\nworkingday\nweathersit\ntemp\natemp\nhum\nwindspeed\ncasual\nregistered\ncnt\n\n\n\n\n\n\n\n\n\nLooks like we have no null values left in our cleaned dataframe. This is what we want.\nFor machine learning, all data columns should be numerical. According to .info(), the only column that is not numerical is dteday. Furthermore, it‚Äôs redundant since all date information exists in other columns.\n\ndef prep_data(data):\n    return (data\n            .assign(windspeed = data[\"windspeed\"]\n                    .fillna((data[\"windspeed\"]\n                             .median())),\n                    hum = (data['hum']\n                   .fillna(data.groupby('season')['hum']\n                           .transform('median'))),\n                    temp = (data['temp']\n                            .fillna(mean_vals(data, 700, 702, 'temp'))),\n                    atemp = (data['atemp']\n                            .fillna(mean_vals(data, 700, 702, 'atemp'))),\n                    dteday = pd.to_datetime(data['dteday']),\n                    mnth = lambda x: x['dteday'].dt.month,\n                    yr = data['yr'].ffill()\n                   )\n            .drop('dteday', axis=1)\n           )\n\nbikes = prep_data(df_bikes)\nbikes.sample(n=5, random_state=43)\n\n\n\n\n\n\n\n\ninstant\nseason\nyr\nmnth\nholiday\nweekday\nworkingday\nweathersit\ntemp\natemp\nhum\nwindspeed\ncasual\nregistered\ncnt\n\n\n\n\n335\n336\n4.0\n0.0\n12\n0.0\n5.0\n1.0\n1\n0.314167\n0.331433\n0.625833\n0.100754\n268\n3672\n3940\n\n\n631\n632\n4.0\n1.0\n9\n0.0\n0.0\n0.0\n1\n0.529167\n0.518933\n0.467083\n0.223258\n2454\n5453\n7907\n\n\n620\n621\n3.0\n1.0\n9\n0.0\n3.0\n1.0\n1\n0.599167\n0.570075\n0.577083\n0.131846\n1050\n6820\n7870\n\n\n722\n723\n1.0\n1.0\n12\n0.0\n0.0\n0.0\n1\n0.245833\n0.259471\n0.515417\n0.133083\n408\n1379\n1787\n\n\n388\n389\n1.0\n1.0\n1\n0.0\n2.0\n1.0\n1\n0.342500\n0.349108\n0.543750\n0.123767\n439\n3900\n4339\n\n\n\n\n\n\n\nThis is the beauty of method chaining. Now we have a function neatly done that will handle this dataset next time we come across it.\nThank you for reading this."
  },
  {
    "objectID": "blog/2024-May-15-Working-with-Dictionaries-and-DataFrames/WorkingWithDictionariesAndDataFrames.html",
    "href": "blog/2024-May-15-Working-with-Dictionaries-and-DataFrames/WorkingWithDictionariesAndDataFrames.html",
    "title": "Working with Dictionaries and DataFrames",
    "section": "",
    "text": "In the world of data science and programming, working with structured data is a fundamental skill. Python, being a versatile language, provides powerful tools and libraries to efficiently handle and manipulate data. In this notebook, we will explore two essential data structures in Python: dictionaries and DataFrames.\nDictionaries are a key-value pair data structure that allows for fast retrieval, updating, and deletion of elements using keys. They are widely used in various programming tasks and form the foundation for more complex data structures. On the other hand, DataFrames, which are part of the pandas library, are two-dimensional labeled data structures that can hold columns of different data types. They are particularly useful for handling structured data and offer a wide range of functions for data manipulation and analysis.\nTo enhance our understanding of these concepts, we will get into the weeds a little bit with practical examples and demonstrations. We will learn how to create dictionaries and DataFrames, manipulate their contents, and perform common operations. Furthermore, we will explore how to generate synthetic data using the faker library and the numpy random generator class. These tools will allow us to create realistic datasets for experimentation and testing purposes.\nBy the end of this notebook, you will have a solid understanding of dictionaries and DataFrames in Python, along with the skills to create, manipulate, and analyze structured data. Whether you are a beginner or an experienced programmer, this notebook aims to provide valuable insights and practical examples to enhance your data handling capabilities.\nSo, let‚Äôs dive in and explore the world of dictionaries and DataFrames in Python!\n\n\nA dictionary in programming is a data structure that stores pairs of elements‚Äîkeys and values‚Äîwhere each key is unique, and each value is associated with one key. This allows for fast retrieval, addition, and deletion of elements based on the key.\nIn Python, dictionaries are defined using curly braces {} with keys and values separated by colons :.\nmy_dict = {\n    'name': 'Alice',\n    'age': 25,\n    'is_student': False\n}\n\n# Accessing a value\nprint(my_dict['name'])  # Output: Alice\n\n# Adding a new key-value pair\nmy_dict['city'] = 'New York'\n\n# Output the updated dictionary\nprint(my_dict)\n\n\n\nA pandas DataFrame is a two-dimensional labeled data structure that consists of columns of potentially different types, similar to a spreadsheet or SQL table. It is a fundamental data structure in the pandas library for Python, widely used for data analysis and manipulation.\nComparison with Excel: - Like an Excel spreadsheet, a DataFrame has rows and columns. - Each column in a DataFrame can have a different data type (e.g., numeric, string, boolean), while in Excel, a column typically contains data of the same type. - DataFrames provide powerful functions for data manipulation, filtering, grouping, and merging, which are more advanced and flexible compared to Excel‚Äôs built-in functions.\nRelationship to R programming: - The pandas library in Python was heavily inspired by the data.frame in R. - Both pandas DataFrame and R‚Äôs data.frame are designed to handle structured, tabular data and provide similar functionality for data analysis and manipulation. - pandas DataFrame borrowed many concepts and functionalities from R‚Äôs data.frame, making it easier for R users to transition to Python for data analysis tasks.\nHere‚Äôs a simple example of creating a pandas DataFrame from lists to display fruits and their prices:\nimport pandas as pd\n\nfruits = ['Apple', 'Banana', 'Orange', 'Grapes', 'Mango']\nprices = [0.99, 0.50, 0.75, 2.99, 1.49]\n\ndf = pd.DataFrame({'Fruit': fruits, 'Price': prices})\n\nprint(df)\nOutput:\n    Fruit  Price\n0   Apple   0.99\n1  Banana   0.50\n2  Orange   0.75\n3  Grapes   2.99\n4   Mango   1.49\nLet us now get into generating a dictionary of fake datasets of landlord owners then create a pandas DataFrame from this information. First we will need to install the faker library if we don‚Äôt have it.\npip install faker --quiet\nThe --quiet flag is included to suppress the output of the install process and it is not needed for the installation to work.\n\nimport numpy as np\nimport pandas as pd\nfrom faker import Faker\n\nThe code is generating a dataset of approximately 500 fictional property listings in Germany, distributed across 10 randomly selected cities. Here‚Äôs a high-level overview:\n\nThe create_property_dict function generates a single property listing as a dictionary. It uses the Faker library to generate realistic fake data for the owner‚Äôs name and city, and NumPy‚Äôs random number generator to randomly select the property type, area, and price in euros.\nThe generate_property_data function generates a specified number of property listings by repeatedly calling create_property_dict. It randomly selects a city for each listing from the provided list of cities.\nThe code sets a fixed seed for both Faker and NumPy‚Äôs random number generator to ensure the generated data is reproducible.\nIt generates a list of 10 random German city names using Faker.\nIt calls generate_property_data to generate 500 property listings across the 10 cities.\nFinally, it converts the list of property dictionaries into a pandas DataFrame for convenient data analysis and manipulation.\n\n\ndef create_property_dict(city, state, faker_instance, rng_instance):\n    property_dict = {\n        'owners_name': faker_instance.name(),\n        'property_type': rng_instance.choice(['Wohnung', 'Haus']),\n        'city': city,\n        'state': state,\n        'area_m2': rng_instance.integers(50, 161),\n        'price_euros': round(rng_instance.uniform(100000, 1000000), 2)\n    }\n\n    return property_dict\n\ndef generate_property_data(num_entries, cities, states, faker_instance, rng_instance):\n    property_data = []\n    for i in range(num_entries):\n        city = rng_instance.choice(cities)\n        state = states[i % len(states)]  # Alternate between the states\n        property_dict = create_property_dict(city, state, faker_instance, rng_instance)\n        property_data.append(property_dict)\n    return property_data\n\n# Set the seed for Faker and NumPy\nFaker.seed(43)\nnp.random.seed(43)\n\n# Create Faker and NumPy random instances\nfake = Faker('de_DE')\nrng = np.random.default_rng()\n\n# Generate 10 random cities\ncities = [fake.city() for _ in range(10)]\n\n# Define 2 random states\nstates = ['Bavaria', 'Saxony']\n\n# Generate about 500 entries distributed among the cities\nnum_entries = 500\nproperty_data = generate_property_data(num_entries, cities, states, fake, rng)\n\n# Convert property_data to a DataFrame\ndf = pd.DataFrame(property_data)\n\ndf.head(10)\n\n\n\n\n\n\n\n\nowners_name\nproperty_type\ncity\nstate\narea_m2\nprice_euros\n\n\n\n\n0\nSahin Ebert\nWohnung\nMittweida\nBavaria\n110\n815563.33\n\n\n1\nCarmine Dietz-Fritsch\nHaus\nSchwerin\nSaxony\n136\n785586.70\n\n\n2\nDr. Patrizia Kraushaar B.Eng.\nHaus\nSchl√ºchtern\nBavaria\n51\n522034.28\n\n\n3\nDipl.-Ing. Anton Binner\nWohnung\nRockenhausen\nSaxony\n130\n972890.32\n\n\n4\nDomenico Bolander\nHaus\nMittweida\nBavaria\n89\n403418.10\n\n\n5\nIng. Freddy Buchholz\nWohnung\nBayreuth\nSaxony\n149\n985671.01\n\n\n6\nStanislav Sauer\nWohnung\nG√ºstrow\nBavaria\n71\n574618.72\n\n\n7\nBekir Scholtz\nWohnung\nBayreuth\nSaxony\n78\n121893.28\n\n\n8\nRamona Seifert B.Eng.\nHaus\nSchl√ºchtern\nBavaria\n107\n934978.00\n\n\n9\nKatarina Stadelmann\nWohnung\nMittweida\nSaxony\n68\n662183.34\n\n\n\n\n\n\n\n\n\n\nA correlation measures the relationship between two variables, indicating how changes in one variable are associated with changes in another. The correlation coefficient, typically denoted as ( r ), ranges from -1 to 1:\n\n( r = 1 ) indicates a perfect positive correlation (as one variable increases, the other also increases).\n( r = -1 ) indicates a perfect negative correlation (as one variable increases, the other decreases).\n( r = 0 ) indicates no correlation (no linear relationship between the variables).\n\nIn statistical terms:\n\nPositive correlation: Both variables move in the same direction.\nNegative correlation: Variables move in opposite directions.\nNo correlation: Variables do not show any linear relationship.\n\nHere‚Äôs how you can calculate the correlation coefficient in Python using the pandas library:\nimport pandas as pd\n\n# Sample data\ndata = {\n    'Variable1': [10, 20, 30, 40, 50],\n    'Variable2': [15, 25, 35, 45, 55],\n    'Variable3': [100, 200, 300, 400, 500]\n}\n\n# Create a DataFrame\ndf = pd.DataFrame(data)\n\n# Calculate the correlation matrix\ncorrelation_matrix = df.corr()\n\n\nWe will now create a dictionary called saxony_cities_corr with the names of all the cities in Saxony as keys and their corresponding correlation coefficients between area_m2 and price_euros as values.\nWe will do it manually first with so many steps of cut and paste.\nLet us first define a boolean mask then subset the entire dataframe with that mask.\n\nmask = df.state == 'Saxony'\ndf_saxony = df[mask]\n\ndf_saxony\n\n\n\n\n\n\n\n\nowners_name\nproperty_type\ncity\nstate\narea_m2\nprice_euros\n\n\n\n\n1\nCarmine Dietz-Fritsch\nHaus\nSchwerin\nSaxony\n136\n785586.70\n\n\n3\nDipl.-Ing. Anton Binner\nWohnung\nRockenhausen\nSaxony\n130\n972890.32\n\n\n5\nIng. Freddy Buchholz\nWohnung\nBayreuth\nSaxony\n149\n985671.01\n\n\n7\nBekir Scholtz\nWohnung\nBayreuth\nSaxony\n78\n121893.28\n\n\n9\nKatarina Stadelmann\nWohnung\nMittweida\nSaxony\n68\n662183.34\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n491\nIng. Knud M√ºhle\nWohnung\nRockenhausen\nSaxony\n64\n263566.53\n\n\n493\nMona Jacob\nHaus\nSchwerin\nSaxony\n72\n262477.49\n\n\n495\nKathi Kramer B.Sc.\nHaus\nRockenhausen\nSaxony\n100\n750277.81\n\n\n497\nJuan H√ºbel\nWohnung\nSchl√ºchtern\nSaxony\n156\n724366.15\n\n\n499\nDr. Stephanie Ullmann\nWohnung\nOschatz\nSaxony\n116\n965051.22\n\n\n\n\n250 rows √ó 6 columns\n\n\n\nThe code snippet provided performs a filtering operation on a pandas DataFrame named df to select rows where the state is ‚ÄòSaxony‚Äô. Here‚Äôs a breakdown of each step:\n\nCreating a Mask:\n\nmask = df.state == 'Saxony': This line creates a boolean mask where each element is True if the corresponding row‚Äôs state is ‚ÄòSaxony‚Äô, and False otherwise. This mask is essentially a series of boolean values that match the length of the DataFrame.\n\nApplying the Mask to the DataFrame:\n\ndf_saxony = df[mask]: This line applies the mask to the DataFrame df. By using the mask, it filters out the rows where the mask has a True value. The result is a new DataFrame, df_saxony, which contains only the rows from the original DataFrame where the state column has the value ‚ÄòSaxony‚Äô.\n\nResulting DataFrame:\n\nThe resulting df_saxony DataFrame includes only those entries from the original DataFrame that are located in ‚ÄòSaxony‚Äô. All columns are retained, but the number of rows may be reduced depending on how many entries meet the criteria.\n\n\nThis operation is useful for segmenting data based on specific criteria, in this case, geographic location (state). It‚Äôs commonly used in data analysis to focus on subsets of a dataset.\n\ndf_saxony.city.unique()\n\narray(['Schwerin', 'Rockenhausen', 'Bayreuth', 'Mittweida', 'Stollberg',\n       'K√∂tzting', 'Oschatz', 'G√ºstrow', 'Merseburg', 'Schl√ºchtern'],\n      dtype=object)\n\n\nWe create an empty dictionary and call it saxony_cities_corr.\n\nsaxony_cities_corr = {}\n\nWe will proceed to now manually calcualte the correlation for all the 10 cities found in the State of Saxony. Please note that even though the States and Cities listed in the DataFrame are real German States and Cities, they were all assigned randomly to each other as they are ‚Äòfake‚Äô.\n\nmask_bayreuth = df_saxony['city'] == 'Bayreuth'\ndf_bayreuth = df_saxony[mask_bayreuth]\nbayreuth_corr = df_bayreuth['area_m2'].corr(df_bayreuth['price_euros'])\nsaxony_cities_corr['Bayreuth'] = bayreuth_corr\nsaxony_cities_corr\n\n{'Bayreuth': 0.0770467229268453}\n\n\nThe code snippet filters the df_saxony DataFrame to include only properties in Bayreuth, calculates the correlation coefficient between the area and price of those properties, and adds the result to the saxony_cities_corr dictionary.\nWe will repeat the same process for the rest of the cities.\n\n# Calculate the correlation coefficient for K√∂tzting\nmask_koetzting = df_saxony['city'] == 'K√∂tzting'\ndf_koetzting = df_saxony[mask_koetzting]\nkoetzting_corr = df_koetzting['area_m2'].corr(df_koetzting['price_euros'])\nsaxony_cities_corr['K√∂tzting'] = koetzting_corr\nsaxony_cities_corr\n\n{'Bayreuth': 0.0770467229268453, 'K√∂tzting': 0.014957658503539707}\n\n\n\n# Calculate the correlation coefficient for Stollberg\nmask_stollberg = df_saxony['city'] == 'Stollberg'\ndf_stollberg = df_saxony[mask_stollberg]\nstollberg_corr = df_stollberg['area_m2'].corr(df_stollberg['price_euros'])\nsaxony_cities_corr['Stollberg'] = stollberg_corr\nsaxony_cities_corr\n\n{'Bayreuth': 0.0770467229268453,\n 'K√∂tzting': 0.014957658503539707,\n 'Stollberg': -0.29977732264028495}\n\n\n\n# Calculate the correlation coefficient for Schwerin\nmask_schwerin = df_saxony['city'] == 'Schwerin'\ndf_schwerin = df_saxony[mask_schwerin]\nschwerin_corr = df_schwerin['area_m2'].corr(df_schwerin['price_euros'])\nsaxony_cities_corr['Schwerin'] = schwerin_corr\nsaxony_cities_corr\n\n{'Bayreuth': 0.0770467229268453,\n 'K√∂tzting': 0.014957658503539707,\n 'Stollberg': -0.29977732264028495,\n 'Schwerin': -0.03456782460314455}\n\n\n\n# Calculate the correlation coefficient for Schl√ºchtern\nmask_schluechtern = df_saxony['city'] == 'Schl√ºchtern'\ndf_schluechtern = df_saxony[mask_schluechtern]\nschluechtern_corr = df_schluechtern['area_m2'].corr(df_schluechtern['price_euros'])\nsaxony_cities_corr['Schl√ºchtern'] = schluechtern_corr\nsaxony_cities_corr\n\n{'Bayreuth': 0.0770467229268453,\n 'K√∂tzting': 0.014957658503539707,\n 'Stollberg': -0.29977732264028495,\n 'Schwerin': -0.03456782460314455,\n 'Schl√ºchtern': -0.04393598070125694}\n\n\n\n# Calculate the correlation coefficient for Merseburg\nmask_merseburg = df_saxony['city'] == 'Merseburg'\ndf_merseburg = df_saxony[mask_merseburg]\nmerseburg_corr = df_merseburg['area_m2'].corr(df_merseburg['price_euros'])\nsaxony_cities_corr['Merseburg'] = merseburg_corr\nsaxony_cities_corr\n\n{'Bayreuth': 0.0770467229268453,\n 'K√∂tzting': 0.014957658503539707,\n 'Stollberg': -0.29977732264028495,\n 'Schwerin': -0.03456782460314455,\n 'Schl√ºchtern': -0.04393598070125694,\n 'Merseburg': 0.2805170371742092}\n\n\n\n# Calculate the correlation coefficient for Rockenhausen\nmask_rockenhausen = df_saxony['city'] == 'Rockenhausen'\ndf_rockenhausen = df_saxony[mask_rockenhausen]\nrockenhausen_corr = df_rockenhausen['area_m2'].corr(df_rockenhausen['price_euros'])\nsaxony_cities_corr['Rockenhausen'] = rockenhausen_corr\nsaxony_cities_corr\n\n{'Bayreuth': 0.0770467229268453,\n 'K√∂tzting': 0.014957658503539707,\n 'Stollberg': -0.29977732264028495,\n 'Schwerin': -0.03456782460314455,\n 'Schl√ºchtern': -0.04393598070125694,\n 'Merseburg': 0.2805170371742092,\n 'Rockenhausen': 0.1793142405949145}\n\n\n\n# Calculate the correlation coefficient for G√ºstrow\nmask_guestrow = df_saxony['city'] == 'G√ºstrow'\ndf_guestrow = df_saxony[mask_guestrow]\nguestrow_corr = df_guestrow['area_m2'].corr(df_guestrow['price_euros'])\nsaxony_cities_corr['G√ºstrow'] = guestrow_corr\nsaxony_cities_corr\n\n{'Bayreuth': 0.0770467229268453,\n 'K√∂tzting': 0.014957658503539707,\n 'Stollberg': -0.29977732264028495,\n 'Schwerin': -0.03456782460314455,\n 'Schl√ºchtern': -0.04393598070125694,\n 'Merseburg': 0.2805170371742092,\n 'Rockenhausen': 0.1793142405949145,\n 'G√ºstrow': 0.18345446151128425}\n\n\n\n# Calculate the correlation coefficient for Oschatz\nmask_oschatz = df_saxony['city'] == 'Oschatz'\ndf_oschatz = df_saxony[mask_oschatz]\noschatz_corr = df_oschatz['area_m2'].corr(df_oschatz['price_euros'])\nsaxony_cities_corr['Oschatz'] = oschatz_corr\nsaxony_cities_corr\n\n{'Bayreuth': 0.0770467229268453,\n 'K√∂tzting': 0.014957658503539707,\n 'Stollberg': -0.29977732264028495,\n 'Schwerin': -0.03456782460314455,\n 'Schl√ºchtern': -0.04393598070125694,\n 'Merseburg': 0.2805170371742092,\n 'Rockenhausen': 0.1793142405949145,\n 'G√ºstrow': 0.18345446151128425,\n 'Oschatz': 0.024082404872172}\n\n\n\n# Calculate the correlation coefficient for Mittweida\nmask_mittweida = df_saxony['city'] == 'Mittweida'\ndf_mittweida = df_saxony[mask_mittweida]\nmittweida_corr = df_mittweida['area_m2'].corr(df_mittweida['price_euros'])\nsaxony_cities_corr['Mittweida'] = mittweida_corr\nsaxony_cities_corr\n\n{'Bayreuth': 0.0770467229268453,\n 'K√∂tzting': 0.014957658503539707,\n 'Stollberg': -0.29977732264028495,\n 'Schwerin': -0.03456782460314455,\n 'Schl√ºchtern': -0.04393598070125694,\n 'Merseburg': 0.2805170371742092,\n 'Rockenhausen': 0.1793142405949145,\n 'G√ºstrow': 0.18345446151128425,\n 'Oschatz': 0.024082404872172,\n 'Mittweida': 0.11531078033267718}\n\n\nManually repeating the same code for each city was tedious and error-prone. However, this exercise helped me understand the process and laid the foundation for the next approach, which achieves the same result more efficiently using a for loop to iterate over the cities.\n\n\n\n\n# Provided list of cities\ncities = df_saxony.city.unique()\n\n# Initialize an empty dictionary to store the correlation coefficients\nsaxony_cities_corr = {}\n\n# Loop through each city and calculate the correlation coefficient\nfor city in cities:\n    mask_city = df_saxony['city'] == city\n    df_city = df_saxony[mask_city]\n    city_corr = df_city['area_m2'].corr(df_city['price_euros'])\n    saxony_cities_corr[city] = city_corr\n\n# Display the dictionary with correlation coefficients\nsaxony_cities_corr\n\n{'Schwerin': -0.03456782460314455,\n 'Rockenhausen': 0.1793142405949145,\n 'Bayreuth': 0.0770467229268453,\n 'Mittweida': 0.11531078033267718,\n 'Stollberg': -0.29977732264028495,\n 'K√∂tzting': 0.014957658503539707,\n 'Oschatz': 0.024082404872172,\n 'G√ºstrow': 0.18345446151128425,\n 'Merseburg': 0.2805170371742092,\n 'Schl√ºchtern': -0.04393598070125694}\n\n\nSame result using a more compact code.\n\nList of Cities: The list of cities is defined as provided.\nEmpty Dictionary: An empty dictionary saxony_cities_corr is initialized to store the correlation coefficients.\nFor Loop: The loop iterates over each city in the cities list.\n\nWithin the loop:\n\nA mask is created to filter rows for the current city.\nA DataFrame df_city is created containing only the rows for the current city.\nThe correlation coefficient between area_m2 and price_euros is calculated and stored in city_corr.\nThe correlation coefficient is then added to the dictionary saxony_cities_corr with the city name as the key.\n\n\nResult: The dictionary saxony_cities_corr is displayed, containing the correlation coefficients for all the cities.\n\nThis approach leverages the power of loops to reduce repetitive code and makes it easier to handle additional cities in the future.\nWe can even make it more compact with a dictionary comprehension.\n\n\n\n\n# Provided list of cities\ncities = df_saxony.city.unique()\n\n# Create a dictionary with correlation coefficients using dictionary comprehension\nsaxony_cities_corr = {\n    city: df_saxony[df_saxony['city'] == city]['area_m2'].corr(\n        df_saxony[df_saxony['city'] == city]['price_euros']\n    ) for city in cities\n}\n\n# Display the dictionary with correlation coefficients\nsaxony_cities_corr\n\n{'Schwerin': -0.03456782460314455,\n 'Rockenhausen': 0.1793142405949145,\n 'Bayreuth': 0.0770467229268453,\n 'Mittweida': 0.11531078033267718,\n 'Stollberg': -0.29977732264028495,\n 'K√∂tzting': 0.014957658503539707,\n 'Oschatz': 0.024082404872172,\n 'G√ºstrow': 0.18345446151128425,\n 'Merseburg': 0.2805170371742092,\n 'Schl√ºchtern': -0.04393598070125694}\n\n\n\nList of Cities: The list of cities is defined as provided.\nDictionary Comprehension: A dictionary comprehension is used to create the saxony_cities_corr dictionary.\n\nFor each city in the cities list:\n\nThe DataFrame df_saxony is filtered to include only rows where the city column matches the current city.\nThe correlation coefficient between area_m2 and price_euros for the filtered DataFrame is calculated.\nThe city name is used as the key, and the correlation coefficient is the value in the resulting dictionary.\n\n\n\nThis method is compact and leverages the power of dictionary comprehensions to achieve the same result with minimal code.\n\n\n\n\nMy desire to work on this notebook stemmed from interactions with my students who had questions about a similar exercise they encountered in class. This experience motivated me to explore and present a solution to their inquiries.\nThroughout this notebook, we delved into the faker library, which allows us to generate realistic fake data for various purposes. Additionally, we explored the numpy random generator class, specifically the numpy.random.Generator class introduced in NumPy version 1.17.0. This class provides a more flexible and efficient way to generate random numbers compared to the older numpy.random functions. It allows you to create multiple independent random number generators, each with its own state, which is useful for parallel computing and reproducibility.\nBy combining these libraries and applying the concepts of dictionaries and DataFrames in Python, we demonstrated how to create and manipulate datasets effectively. The examples and explanations provided aim to clarify the process and offer a solid foundation for tackling similar problems.\nIt is my sincere hope that you find this article informative and valuable. Whether you are a student, a data enthusiast, or a professional working with data, the techniques and insights shared here can be applied to a wide range of scenarios. Feel free to adapt and build upon the ideas presented to suit your specific needs, and don‚Äôt hesitate to explore the capabilities of the numpy.random.Generator class and the faker library further to generate random numbers and data, and also to perform random sampling in your own projects."
  },
  {
    "objectID": "blog/2024-May-15-Working-with-Dictionaries-and-DataFrames/WorkingWithDictionariesAndDataFrames.html#introduction",
    "href": "blog/2024-May-15-Working-with-Dictionaries-and-DataFrames/WorkingWithDictionariesAndDataFrames.html#introduction",
    "title": "Working with Dictionaries and DataFrames",
    "section": "",
    "text": "In the world of data science and programming, working with structured data is a fundamental skill. Python, being a versatile language, provides powerful tools and libraries to efficiently handle and manipulate data. In this notebook, we will explore two essential data structures in Python: dictionaries and DataFrames.\nDictionaries are a key-value pair data structure that allows for fast retrieval, updating, and deletion of elements using keys. They are widely used in various programming tasks and form the foundation for more complex data structures. On the other hand, DataFrames, which are part of the pandas library, are two-dimensional labeled data structures that can hold columns of different data types. They are particularly useful for handling structured data and offer a wide range of functions for data manipulation and analysis.\nTo enhance our understanding of these concepts, we will get into the weeds a little bit with practical examples and demonstrations. We will learn how to create dictionaries and DataFrames, manipulate their contents, and perform common operations. Furthermore, we will explore how to generate synthetic data using the faker library and the numpy random generator class. These tools will allow us to create realistic datasets for experimentation and testing purposes.\nBy the end of this notebook, you will have a solid understanding of dictionaries and DataFrames in Python, along with the skills to create, manipulate, and analyze structured data. Whether you are a beginner or an experienced programmer, this notebook aims to provide valuable insights and practical examples to enhance your data handling capabilities.\nSo, let‚Äôs dive in and explore the world of dictionaries and DataFrames in Python!\n\n\nA dictionary in programming is a data structure that stores pairs of elements‚Äîkeys and values‚Äîwhere each key is unique, and each value is associated with one key. This allows for fast retrieval, addition, and deletion of elements based on the key.\nIn Python, dictionaries are defined using curly braces {} with keys and values separated by colons :.\nmy_dict = {\n    'name': 'Alice',\n    'age': 25,\n    'is_student': False\n}\n\n# Accessing a value\nprint(my_dict['name'])  # Output: Alice\n\n# Adding a new key-value pair\nmy_dict['city'] = 'New York'\n\n# Output the updated dictionary\nprint(my_dict)\n\n\n\nA pandas DataFrame is a two-dimensional labeled data structure that consists of columns of potentially different types, similar to a spreadsheet or SQL table. It is a fundamental data structure in the pandas library for Python, widely used for data analysis and manipulation.\nComparison with Excel: - Like an Excel spreadsheet, a DataFrame has rows and columns. - Each column in a DataFrame can have a different data type (e.g., numeric, string, boolean), while in Excel, a column typically contains data of the same type. - DataFrames provide powerful functions for data manipulation, filtering, grouping, and merging, which are more advanced and flexible compared to Excel‚Äôs built-in functions.\nRelationship to R programming: - The pandas library in Python was heavily inspired by the data.frame in R. - Both pandas DataFrame and R‚Äôs data.frame are designed to handle structured, tabular data and provide similar functionality for data analysis and manipulation. - pandas DataFrame borrowed many concepts and functionalities from R‚Äôs data.frame, making it easier for R users to transition to Python for data analysis tasks.\nHere‚Äôs a simple example of creating a pandas DataFrame from lists to display fruits and their prices:\nimport pandas as pd\n\nfruits = ['Apple', 'Banana', 'Orange', 'Grapes', 'Mango']\nprices = [0.99, 0.50, 0.75, 2.99, 1.49]\n\ndf = pd.DataFrame({'Fruit': fruits, 'Price': prices})\n\nprint(df)\nOutput:\n    Fruit  Price\n0   Apple   0.99\n1  Banana   0.50\n2  Orange   0.75\n3  Grapes   2.99\n4   Mango   1.49\nLet us now get into generating a dictionary of fake datasets of landlord owners then create a pandas DataFrame from this information. First we will need to install the faker library if we don‚Äôt have it.\npip install faker --quiet\nThe --quiet flag is included to suppress the output of the install process and it is not needed for the installation to work.\n\nimport numpy as np\nimport pandas as pd\nfrom faker import Faker\n\nThe code is generating a dataset of approximately 500 fictional property listings in Germany, distributed across 10 randomly selected cities. Here‚Äôs a high-level overview:\n\nThe create_property_dict function generates a single property listing as a dictionary. It uses the Faker library to generate realistic fake data for the owner‚Äôs name and city, and NumPy‚Äôs random number generator to randomly select the property type, area, and price in euros.\nThe generate_property_data function generates a specified number of property listings by repeatedly calling create_property_dict. It randomly selects a city for each listing from the provided list of cities.\nThe code sets a fixed seed for both Faker and NumPy‚Äôs random number generator to ensure the generated data is reproducible.\nIt generates a list of 10 random German city names using Faker.\nIt calls generate_property_data to generate 500 property listings across the 10 cities.\nFinally, it converts the list of property dictionaries into a pandas DataFrame for convenient data analysis and manipulation.\n\n\ndef create_property_dict(city, state, faker_instance, rng_instance):\n    property_dict = {\n        'owners_name': faker_instance.name(),\n        'property_type': rng_instance.choice(['Wohnung', 'Haus']),\n        'city': city,\n        'state': state,\n        'area_m2': rng_instance.integers(50, 161),\n        'price_euros': round(rng_instance.uniform(100000, 1000000), 2)\n    }\n\n    return property_dict\n\ndef generate_property_data(num_entries, cities, states, faker_instance, rng_instance):\n    property_data = []\n    for i in range(num_entries):\n        city = rng_instance.choice(cities)\n        state = states[i % len(states)]  # Alternate between the states\n        property_dict = create_property_dict(city, state, faker_instance, rng_instance)\n        property_data.append(property_dict)\n    return property_data\n\n# Set the seed for Faker and NumPy\nFaker.seed(43)\nnp.random.seed(43)\n\n# Create Faker and NumPy random instances\nfake = Faker('de_DE')\nrng = np.random.default_rng()\n\n# Generate 10 random cities\ncities = [fake.city() for _ in range(10)]\n\n# Define 2 random states\nstates = ['Bavaria', 'Saxony']\n\n# Generate about 500 entries distributed among the cities\nnum_entries = 500\nproperty_data = generate_property_data(num_entries, cities, states, fake, rng)\n\n# Convert property_data to a DataFrame\ndf = pd.DataFrame(property_data)\n\ndf.head(10)\n\n\n\n\n\n\n\n\nowners_name\nproperty_type\ncity\nstate\narea_m2\nprice_euros\n\n\n\n\n0\nSahin Ebert\nWohnung\nMittweida\nBavaria\n110\n815563.33\n\n\n1\nCarmine Dietz-Fritsch\nHaus\nSchwerin\nSaxony\n136\n785586.70\n\n\n2\nDr. Patrizia Kraushaar B.Eng.\nHaus\nSchl√ºchtern\nBavaria\n51\n522034.28\n\n\n3\nDipl.-Ing. Anton Binner\nWohnung\nRockenhausen\nSaxony\n130\n972890.32\n\n\n4\nDomenico Bolander\nHaus\nMittweida\nBavaria\n89\n403418.10\n\n\n5\nIng. Freddy Buchholz\nWohnung\nBayreuth\nSaxony\n149\n985671.01\n\n\n6\nStanislav Sauer\nWohnung\nG√ºstrow\nBavaria\n71\n574618.72\n\n\n7\nBekir Scholtz\nWohnung\nBayreuth\nSaxony\n78\n121893.28\n\n\n8\nRamona Seifert B.Eng.\nHaus\nSchl√ºchtern\nBavaria\n107\n934978.00\n\n\n9\nKatarina Stadelmann\nWohnung\nMittweida\nSaxony\n68\n662183.34\n\n\n\n\n\n\n\n\n\n\nA correlation measures the relationship between two variables, indicating how changes in one variable are associated with changes in another. The correlation coefficient, typically denoted as ( r ), ranges from -1 to 1:\n\n( r = 1 ) indicates a perfect positive correlation (as one variable increases, the other also increases).\n( r = -1 ) indicates a perfect negative correlation (as one variable increases, the other decreases).\n( r = 0 ) indicates no correlation (no linear relationship between the variables).\n\nIn statistical terms:\n\nPositive correlation: Both variables move in the same direction.\nNegative correlation: Variables move in opposite directions.\nNo correlation: Variables do not show any linear relationship.\n\nHere‚Äôs how you can calculate the correlation coefficient in Python using the pandas library:\nimport pandas as pd\n\n# Sample data\ndata = {\n    'Variable1': [10, 20, 30, 40, 50],\n    'Variable2': [15, 25, 35, 45, 55],\n    'Variable3': [100, 200, 300, 400, 500]\n}\n\n# Create a DataFrame\ndf = pd.DataFrame(data)\n\n# Calculate the correlation matrix\ncorrelation_matrix = df.corr()\n\n\nWe will now create a dictionary called saxony_cities_corr with the names of all the cities in Saxony as keys and their corresponding correlation coefficients between area_m2 and price_euros as values.\nWe will do it manually first with so many steps of cut and paste.\nLet us first define a boolean mask then subset the entire dataframe with that mask.\n\nmask = df.state == 'Saxony'\ndf_saxony = df[mask]\n\ndf_saxony\n\n\n\n\n\n\n\n\nowners_name\nproperty_type\ncity\nstate\narea_m2\nprice_euros\n\n\n\n\n1\nCarmine Dietz-Fritsch\nHaus\nSchwerin\nSaxony\n136\n785586.70\n\n\n3\nDipl.-Ing. Anton Binner\nWohnung\nRockenhausen\nSaxony\n130\n972890.32\n\n\n5\nIng. Freddy Buchholz\nWohnung\nBayreuth\nSaxony\n149\n985671.01\n\n\n7\nBekir Scholtz\nWohnung\nBayreuth\nSaxony\n78\n121893.28\n\n\n9\nKatarina Stadelmann\nWohnung\nMittweida\nSaxony\n68\n662183.34\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n491\nIng. Knud M√ºhle\nWohnung\nRockenhausen\nSaxony\n64\n263566.53\n\n\n493\nMona Jacob\nHaus\nSchwerin\nSaxony\n72\n262477.49\n\n\n495\nKathi Kramer B.Sc.\nHaus\nRockenhausen\nSaxony\n100\n750277.81\n\n\n497\nJuan H√ºbel\nWohnung\nSchl√ºchtern\nSaxony\n156\n724366.15\n\n\n499\nDr. Stephanie Ullmann\nWohnung\nOschatz\nSaxony\n116\n965051.22\n\n\n\n\n250 rows √ó 6 columns\n\n\n\nThe code snippet provided performs a filtering operation on a pandas DataFrame named df to select rows where the state is ‚ÄòSaxony‚Äô. Here‚Äôs a breakdown of each step:\n\nCreating a Mask:\n\nmask = df.state == 'Saxony': This line creates a boolean mask where each element is True if the corresponding row‚Äôs state is ‚ÄòSaxony‚Äô, and False otherwise. This mask is essentially a series of boolean values that match the length of the DataFrame.\n\nApplying the Mask to the DataFrame:\n\ndf_saxony = df[mask]: This line applies the mask to the DataFrame df. By using the mask, it filters out the rows where the mask has a True value. The result is a new DataFrame, df_saxony, which contains only the rows from the original DataFrame where the state column has the value ‚ÄòSaxony‚Äô.\n\nResulting DataFrame:\n\nThe resulting df_saxony DataFrame includes only those entries from the original DataFrame that are located in ‚ÄòSaxony‚Äô. All columns are retained, but the number of rows may be reduced depending on how many entries meet the criteria.\n\n\nThis operation is useful for segmenting data based on specific criteria, in this case, geographic location (state). It‚Äôs commonly used in data analysis to focus on subsets of a dataset.\n\ndf_saxony.city.unique()\n\narray(['Schwerin', 'Rockenhausen', 'Bayreuth', 'Mittweida', 'Stollberg',\n       'K√∂tzting', 'Oschatz', 'G√ºstrow', 'Merseburg', 'Schl√ºchtern'],\n      dtype=object)\n\n\nWe create an empty dictionary and call it saxony_cities_corr.\n\nsaxony_cities_corr = {}\n\nWe will proceed to now manually calcualte the correlation for all the 10 cities found in the State of Saxony. Please note that even though the States and Cities listed in the DataFrame are real German States and Cities, they were all assigned randomly to each other as they are ‚Äòfake‚Äô.\n\nmask_bayreuth = df_saxony['city'] == 'Bayreuth'\ndf_bayreuth = df_saxony[mask_bayreuth]\nbayreuth_corr = df_bayreuth['area_m2'].corr(df_bayreuth['price_euros'])\nsaxony_cities_corr['Bayreuth'] = bayreuth_corr\nsaxony_cities_corr\n\n{'Bayreuth': 0.0770467229268453}\n\n\nThe code snippet filters the df_saxony DataFrame to include only properties in Bayreuth, calculates the correlation coefficient between the area and price of those properties, and adds the result to the saxony_cities_corr dictionary.\nWe will repeat the same process for the rest of the cities.\n\n# Calculate the correlation coefficient for K√∂tzting\nmask_koetzting = df_saxony['city'] == 'K√∂tzting'\ndf_koetzting = df_saxony[mask_koetzting]\nkoetzting_corr = df_koetzting['area_m2'].corr(df_koetzting['price_euros'])\nsaxony_cities_corr['K√∂tzting'] = koetzting_corr\nsaxony_cities_corr\n\n{'Bayreuth': 0.0770467229268453, 'K√∂tzting': 0.014957658503539707}\n\n\n\n# Calculate the correlation coefficient for Stollberg\nmask_stollberg = df_saxony['city'] == 'Stollberg'\ndf_stollberg = df_saxony[mask_stollberg]\nstollberg_corr = df_stollberg['area_m2'].corr(df_stollberg['price_euros'])\nsaxony_cities_corr['Stollberg'] = stollberg_corr\nsaxony_cities_corr\n\n{'Bayreuth': 0.0770467229268453,\n 'K√∂tzting': 0.014957658503539707,\n 'Stollberg': -0.29977732264028495}\n\n\n\n# Calculate the correlation coefficient for Schwerin\nmask_schwerin = df_saxony['city'] == 'Schwerin'\ndf_schwerin = df_saxony[mask_schwerin]\nschwerin_corr = df_schwerin['area_m2'].corr(df_schwerin['price_euros'])\nsaxony_cities_corr['Schwerin'] = schwerin_corr\nsaxony_cities_corr\n\n{'Bayreuth': 0.0770467229268453,\n 'K√∂tzting': 0.014957658503539707,\n 'Stollberg': -0.29977732264028495,\n 'Schwerin': -0.03456782460314455}\n\n\n\n# Calculate the correlation coefficient for Schl√ºchtern\nmask_schluechtern = df_saxony['city'] == 'Schl√ºchtern'\ndf_schluechtern = df_saxony[mask_schluechtern]\nschluechtern_corr = df_schluechtern['area_m2'].corr(df_schluechtern['price_euros'])\nsaxony_cities_corr['Schl√ºchtern'] = schluechtern_corr\nsaxony_cities_corr\n\n{'Bayreuth': 0.0770467229268453,\n 'K√∂tzting': 0.014957658503539707,\n 'Stollberg': -0.29977732264028495,\n 'Schwerin': -0.03456782460314455,\n 'Schl√ºchtern': -0.04393598070125694}\n\n\n\n# Calculate the correlation coefficient for Merseburg\nmask_merseburg = df_saxony['city'] == 'Merseburg'\ndf_merseburg = df_saxony[mask_merseburg]\nmerseburg_corr = df_merseburg['area_m2'].corr(df_merseburg['price_euros'])\nsaxony_cities_corr['Merseburg'] = merseburg_corr\nsaxony_cities_corr\n\n{'Bayreuth': 0.0770467229268453,\n 'K√∂tzting': 0.014957658503539707,\n 'Stollberg': -0.29977732264028495,\n 'Schwerin': -0.03456782460314455,\n 'Schl√ºchtern': -0.04393598070125694,\n 'Merseburg': 0.2805170371742092}\n\n\n\n# Calculate the correlation coefficient for Rockenhausen\nmask_rockenhausen = df_saxony['city'] == 'Rockenhausen'\ndf_rockenhausen = df_saxony[mask_rockenhausen]\nrockenhausen_corr = df_rockenhausen['area_m2'].corr(df_rockenhausen['price_euros'])\nsaxony_cities_corr['Rockenhausen'] = rockenhausen_corr\nsaxony_cities_corr\n\n{'Bayreuth': 0.0770467229268453,\n 'K√∂tzting': 0.014957658503539707,\n 'Stollberg': -0.29977732264028495,\n 'Schwerin': -0.03456782460314455,\n 'Schl√ºchtern': -0.04393598070125694,\n 'Merseburg': 0.2805170371742092,\n 'Rockenhausen': 0.1793142405949145}\n\n\n\n# Calculate the correlation coefficient for G√ºstrow\nmask_guestrow = df_saxony['city'] == 'G√ºstrow'\ndf_guestrow = df_saxony[mask_guestrow]\nguestrow_corr = df_guestrow['area_m2'].corr(df_guestrow['price_euros'])\nsaxony_cities_corr['G√ºstrow'] = guestrow_corr\nsaxony_cities_corr\n\n{'Bayreuth': 0.0770467229268453,\n 'K√∂tzting': 0.014957658503539707,\n 'Stollberg': -0.29977732264028495,\n 'Schwerin': -0.03456782460314455,\n 'Schl√ºchtern': -0.04393598070125694,\n 'Merseburg': 0.2805170371742092,\n 'Rockenhausen': 0.1793142405949145,\n 'G√ºstrow': 0.18345446151128425}\n\n\n\n# Calculate the correlation coefficient for Oschatz\nmask_oschatz = df_saxony['city'] == 'Oschatz'\ndf_oschatz = df_saxony[mask_oschatz]\noschatz_corr = df_oschatz['area_m2'].corr(df_oschatz['price_euros'])\nsaxony_cities_corr['Oschatz'] = oschatz_corr\nsaxony_cities_corr\n\n{'Bayreuth': 0.0770467229268453,\n 'K√∂tzting': 0.014957658503539707,\n 'Stollberg': -0.29977732264028495,\n 'Schwerin': -0.03456782460314455,\n 'Schl√ºchtern': -0.04393598070125694,\n 'Merseburg': 0.2805170371742092,\n 'Rockenhausen': 0.1793142405949145,\n 'G√ºstrow': 0.18345446151128425,\n 'Oschatz': 0.024082404872172}\n\n\n\n# Calculate the correlation coefficient for Mittweida\nmask_mittweida = df_saxony['city'] == 'Mittweida'\ndf_mittweida = df_saxony[mask_mittweida]\nmittweida_corr = df_mittweida['area_m2'].corr(df_mittweida['price_euros'])\nsaxony_cities_corr['Mittweida'] = mittweida_corr\nsaxony_cities_corr\n\n{'Bayreuth': 0.0770467229268453,\n 'K√∂tzting': 0.014957658503539707,\n 'Stollberg': -0.29977732264028495,\n 'Schwerin': -0.03456782460314455,\n 'Schl√ºchtern': -0.04393598070125694,\n 'Merseburg': 0.2805170371742092,\n 'Rockenhausen': 0.1793142405949145,\n 'G√ºstrow': 0.18345446151128425,\n 'Oschatz': 0.024082404872172,\n 'Mittweida': 0.11531078033267718}\n\n\nManually repeating the same code for each city was tedious and error-prone. However, this exercise helped me understand the process and laid the foundation for the next approach, which achieves the same result more efficiently using a for loop to iterate over the cities.\n\n\n\n\n# Provided list of cities\ncities = df_saxony.city.unique()\n\n# Initialize an empty dictionary to store the correlation coefficients\nsaxony_cities_corr = {}\n\n# Loop through each city and calculate the correlation coefficient\nfor city in cities:\n    mask_city = df_saxony['city'] == city\n    df_city = df_saxony[mask_city]\n    city_corr = df_city['area_m2'].corr(df_city['price_euros'])\n    saxony_cities_corr[city] = city_corr\n\n# Display the dictionary with correlation coefficients\nsaxony_cities_corr\n\n{'Schwerin': -0.03456782460314455,\n 'Rockenhausen': 0.1793142405949145,\n 'Bayreuth': 0.0770467229268453,\n 'Mittweida': 0.11531078033267718,\n 'Stollberg': -0.29977732264028495,\n 'K√∂tzting': 0.014957658503539707,\n 'Oschatz': 0.024082404872172,\n 'G√ºstrow': 0.18345446151128425,\n 'Merseburg': 0.2805170371742092,\n 'Schl√ºchtern': -0.04393598070125694}\n\n\nSame result using a more compact code.\n\nList of Cities: The list of cities is defined as provided.\nEmpty Dictionary: An empty dictionary saxony_cities_corr is initialized to store the correlation coefficients.\nFor Loop: The loop iterates over each city in the cities list.\n\nWithin the loop:\n\nA mask is created to filter rows for the current city.\nA DataFrame df_city is created containing only the rows for the current city.\nThe correlation coefficient between area_m2 and price_euros is calculated and stored in city_corr.\nThe correlation coefficient is then added to the dictionary saxony_cities_corr with the city name as the key.\n\n\nResult: The dictionary saxony_cities_corr is displayed, containing the correlation coefficients for all the cities.\n\nThis approach leverages the power of loops to reduce repetitive code and makes it easier to handle additional cities in the future.\nWe can even make it more compact with a dictionary comprehension.\n\n\n\n\n# Provided list of cities\ncities = df_saxony.city.unique()\n\n# Create a dictionary with correlation coefficients using dictionary comprehension\nsaxony_cities_corr = {\n    city: df_saxony[df_saxony['city'] == city]['area_m2'].corr(\n        df_saxony[df_saxony['city'] == city]['price_euros']\n    ) for city in cities\n}\n\n# Display the dictionary with correlation coefficients\nsaxony_cities_corr\n\n{'Schwerin': -0.03456782460314455,\n 'Rockenhausen': 0.1793142405949145,\n 'Bayreuth': 0.0770467229268453,\n 'Mittweida': 0.11531078033267718,\n 'Stollberg': -0.29977732264028495,\n 'K√∂tzting': 0.014957658503539707,\n 'Oschatz': 0.024082404872172,\n 'G√ºstrow': 0.18345446151128425,\n 'Merseburg': 0.2805170371742092,\n 'Schl√ºchtern': -0.04393598070125694}\n\n\n\nList of Cities: The list of cities is defined as provided.\nDictionary Comprehension: A dictionary comprehension is used to create the saxony_cities_corr dictionary.\n\nFor each city in the cities list:\n\nThe DataFrame df_saxony is filtered to include only rows where the city column matches the current city.\nThe correlation coefficient between area_m2 and price_euros for the filtered DataFrame is calculated.\nThe city name is used as the key, and the correlation coefficient is the value in the resulting dictionary.\n\n\n\nThis method is compact and leverages the power of dictionary comprehensions to achieve the same result with minimal code.\n\n\n\n\nMy desire to work on this notebook stemmed from interactions with my students who had questions about a similar exercise they encountered in class. This experience motivated me to explore and present a solution to their inquiries.\nThroughout this notebook, we delved into the faker library, which allows us to generate realistic fake data for various purposes. Additionally, we explored the numpy random generator class, specifically the numpy.random.Generator class introduced in NumPy version 1.17.0. This class provides a more flexible and efficient way to generate random numbers compared to the older numpy.random functions. It allows you to create multiple independent random number generators, each with its own state, which is useful for parallel computing and reproducibility.\nBy combining these libraries and applying the concepts of dictionaries and DataFrames in Python, we demonstrated how to create and manipulate datasets effectively. The examples and explanations provided aim to clarify the process and offer a solid foundation for tackling similar problems.\nIt is my sincere hope that you find this article informative and valuable. Whether you are a student, a data enthusiast, or a professional working with data, the techniques and insights shared here can be applied to a wide range of scenarios. Feel free to adapt and build upon the ideas presented to suit your specific needs, and don‚Äôt hesitate to explore the capabilities of the numpy.random.Generator class and the faker library further to generate random numbers and data, and also to perform random sampling in your own projects."
  },
  {
    "objectID": "blog/2024-May-25-Lambda-Functions/lambda_functions.html",
    "href": "blog/2024-May-25-Lambda-Functions/lambda_functions.html",
    "title": "Lambda Functions: How I tend to use them",
    "section": "",
    "text": "In programming, you often find yourself repeating the same logic multiple times. When this happens, it‚Äôs usually a good idea to use a function. Functions allow us to encapsulate commonly or repeatedly executed tasks, so instead of writing the same code over and over, we can simply call a function with different inputs, thereby reusing the code efficiently.\nBefore diving into lambda functions, let‚Äôs first understand regular functions.\n\n\nIn Python, a function is defined using the def keyword, followed by the function name and parentheses (), which may include parameters, a body of code to be executed, and a return statement that specifies the value to be returned. The general syntax is:\ndef function_name(parameters):\n    # function body\n    return output\nFor instance, suppose you frequently need to sum two values in your code. You could write a function to handle this task:\ndef add_values(value1, value2):\n    return value1 + value2\nThe function add_values takes two arguments, value1 and value2, and returns their sum. Here‚Äôs a breakdown of how the function works:\n\nFunction Definition: python     def add_values(value1, value2):\n\nThe function add_values is defined with two parameters: value1 and value2.\n\nSumming the Values: python     return value1 + value2\n\nThe function returns the result of adding value1 and value2.\n\n\nHere is an example of how you might use this function:\nresult = add_values(3, 5)\nprint(result)  # Output: 8\nWhen the function add_values is called with the arguments 3 and 5, it computes and returns their sum, which is 8.\nIn add_values, the function name is add_values, and it takes two parameters: value1 and value2.\n\n\n\nParameters are variables listed inside the parentheses in the function definition.\nArguments are the values that you pass into the function when you call it.\n\nIn add_values, value1 and value2 are parameters, and when you call the function like add_values(3, 5), 3 and 5 are the arguments.\n\n\n\nThe return statement ends the function execution and specifies what value should be returned to the caller:\nreturn value1 + value2\nThe function returns the sum of value1 and value2.\n\n\n\n\nLambda functions, also known as anonymous functions, are a shorter way to write functions in Python. They are defined using the lambda keyword instead of def. These functions are quick, simple, and used for short-term tasks. They are particularly useful in scenarios where a full function definition would be overkill.\nHere is how you can write the same add_values function using a lambda function:\n# regular function\ndef add_values(value1, value2): return value1 + value2\n\n# lambda function\nadd_values = lambda value1, value2: value1 + value2\nBoth the regular function and the lambda function accomplish the same task: summing any two numbers passed to them. The main differences are that lambda functions use the lambda keyword, do not have a name (unless assigned to a variable like we did above), and do not require a return statement‚Äîthe result is implicitly returned.\nRegular functions and lambda functions each have their own use cases. Regular functions are more versatile and easier to read, especially for complex operations. Lambda functions are more concise and are often used for short-term tasks and in places where a small function is required temporarily, such as in method chaining or as arguments to higher-order functions.\nIn this article, we will focus on how lambda functions can be utilized effectively in your code, particularly in the context of method chaining in pandas.\n\n\n\nIn pandas, chaining methods is often preferred over using operators for manipulating data because it enhances readability and maintains a functional style. Since most pandas methods return a new object rather than modifying the original data in place, you can continuously chain method calls on the returned objects. This approach leads to cleaner and more understandable code. Although chaining with operators is possible, it usually requires wrapping operations in parentheses.\n\n\nMethods and functions serve similar purposes in programming. Both encapsulate reusable blocks of code, but methods are functions that belong to objects, allowing them to operate on the data contained within those objects. Functions, on the other hand, are standalone units that can be called independently."
  },
  {
    "objectID": "blog/2024-May-25-Lambda-Functions/lambda_functions.html#introduction",
    "href": "blog/2024-May-25-Lambda-Functions/lambda_functions.html#introduction",
    "title": "Lambda Functions: How I tend to use them",
    "section": "",
    "text": "In programming, you often find yourself repeating the same logic multiple times. When this happens, it‚Äôs usually a good idea to use a function. Functions allow us to encapsulate commonly or repeatedly executed tasks, so instead of writing the same code over and over, we can simply call a function with different inputs, thereby reusing the code efficiently.\nBefore diving into lambda functions, let‚Äôs first understand regular functions.\n\n\nIn Python, a function is defined using the def keyword, followed by the function name and parentheses (), which may include parameters, a body of code to be executed, and a return statement that specifies the value to be returned. The general syntax is:\ndef function_name(parameters):\n    # function body\n    return output\nFor instance, suppose you frequently need to sum two values in your code. You could write a function to handle this task:\ndef add_values(value1, value2):\n    return value1 + value2\nThe function add_values takes two arguments, value1 and value2, and returns their sum. Here‚Äôs a breakdown of how the function works:\n\nFunction Definition: python     def add_values(value1, value2):\n\nThe function add_values is defined with two parameters: value1 and value2.\n\nSumming the Values: python     return value1 + value2\n\nThe function returns the result of adding value1 and value2.\n\n\nHere is an example of how you might use this function:\nresult = add_values(3, 5)\nprint(result)  # Output: 8\nWhen the function add_values is called with the arguments 3 and 5, it computes and returns their sum, which is 8.\nIn add_values, the function name is add_values, and it takes two parameters: value1 and value2.\n\n\n\nParameters are variables listed inside the parentheses in the function definition.\nArguments are the values that you pass into the function when you call it.\n\nIn add_values, value1 and value2 are parameters, and when you call the function like add_values(3, 5), 3 and 5 are the arguments.\n\n\n\nThe return statement ends the function execution and specifies what value should be returned to the caller:\nreturn value1 + value2\nThe function returns the sum of value1 and value2.\n\n\n\n\nLambda functions, also known as anonymous functions, are a shorter way to write functions in Python. They are defined using the lambda keyword instead of def. These functions are quick, simple, and used for short-term tasks. They are particularly useful in scenarios where a full function definition would be overkill.\nHere is how you can write the same add_values function using a lambda function:\n# regular function\ndef add_values(value1, value2): return value1 + value2\n\n# lambda function\nadd_values = lambda value1, value2: value1 + value2\nBoth the regular function and the lambda function accomplish the same task: summing any two numbers passed to them. The main differences are that lambda functions use the lambda keyword, do not have a name (unless assigned to a variable like we did above), and do not require a return statement‚Äîthe result is implicitly returned.\nRegular functions and lambda functions each have their own use cases. Regular functions are more versatile and easier to read, especially for complex operations. Lambda functions are more concise and are often used for short-term tasks and in places where a small function is required temporarily, such as in method chaining or as arguments to higher-order functions.\nIn this article, we will focus on how lambda functions can be utilized effectively in your code, particularly in the context of method chaining in pandas.\n\n\n\nIn pandas, chaining methods is often preferred over using operators for manipulating data because it enhances readability and maintains a functional style. Since most pandas methods return a new object rather than modifying the original data in place, you can continuously chain method calls on the returned objects. This approach leads to cleaner and more understandable code. Although chaining with operators is possible, it usually requires wrapping operations in parentheses.\n\n\nMethods and functions serve similar purposes in programming. Both encapsulate reusable blocks of code, but methods are functions that belong to objects, allowing them to operate on the data contained within those objects. Functions, on the other hand, are standalone units that can be called independently."
  },
  {
    "objectID": "blog/2024-May-25-Lambda-Functions/lambda_functions.html#a-deeper-dive",
    "href": "blog/2024-May-25-Lambda-Functions/lambda_functions.html#a-deeper-dive",
    "title": "Lambda Functions: How I tend to use them",
    "section": "A Deeper dive",
    "text": "A Deeper dive\nWhat we are going to do here is to play around with a couple of datasets and deploy what w have been talking about in code.\nWe have a stock data we created (also called syntehtic data) and we will take the time to prepare it. First we will load it with pandas and display its information.\n\nimport pandas as pd\n\n\nurl = 'synthetic_stock_data.csv'\ndata = pd.read_csv(url)\ndata.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 347 entries, 0 to 346\nData columns (total 2 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   Date    347 non-null    object \n 1   Close   347 non-null    float64\ndtypes: float64(1), object(1)\nmemory usage: 5.5+ KB\n\n\nThe output above reveals that the Date column has a Dtype of object, indicating that Python will treat the values in this column as strings. If we attempt to create a new column to display the name of the month based on the Date column, Python will not be able to interpret the string values correctly.\nTo handle this situation effectively, we will adopt a step-by-step approach. The initial step involves converting the Date column from its current object type to a Datetime type. This conversion will enable Python to recognize and work with the dates properly, facilitating the extraction of relevant information such as the month name.\nBy transforming the Date column to a Datetime type, we lay the foundation for performing meaningful operations and analyses on the date values. This step is crucial in ensuring that Python can correctly interpret and manipulate the dates, allowing us to create new columns or derive insights based on the temporal information contained within the Date column.\n\ndata['Date'] = pd.to_datetime(data['Date'])\ndata.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 347 entries, 0 to 346\nData columns (total 2 columns):\n #   Column  Non-Null Count  Dtype         \n---  ------  --------------  -----         \n 0   Date    347 non-null    datetime64[ns]\n 1   Close   347 non-null    float64       \ndtypes: datetime64[ns](1), float64(1)\nmemory usage: 5.5 KB\n\n\nAfter converting the Date column to datetime, you can access various attributes and methods to extract information from the dates. For example:\n# Extract the month name\ndf['Month'] = df['Date'].dt.month_name()\n\n# Extract the year\ndf['Year'] = df['Date'].dt.year\n\n# Extract the day of the month\ndf['Day'] = df['Date'].dt.day\nLet‚Äôs add a new column to the DataFrame that contains the month names. To ensure reproducibility when using the sample() method in pandas, we‚Äôll set a seed by passing the random_state parameter.\nBy setting a seed value, we guarantee that the sample() method will always select the same set of random rows each time the code is executed. This is particularly useful when you want others to be able to reproduce the same results as you, especially in a learning or collaborative environment.\nIf having consistent random selections is not important for your specific use case, feel free to modify the random_state value or remove it entirely.\n\ndata['Month'] = data['Date'].dt.month_name()\ndata.sample(5, random_state=43)\n\n\n\n\n\n\n\n\nDate\nClose\nMonth\n\n\n\n\n217\n2023-11-01\n3123.753804\nNovember\n\n\n258\n2023-12-28\n3066.558857\nDecember\n\n\n15\n2023-01-23\n3227.238722\nJanuary\n\n\n294\n2024-02-16\n3843.574952\nFebruary\n\n\n17\n2023-01-25\n3063.817256\nJanuary\n\n\n\n\n\n\n\nIn this code, we create a new column called Month using data['Date'].dt.month_name(), which extracts the month name from each date in the Date column.\nThen, we use data.sample(5, random_state=43) to randomly select 5 rows from the data DataFrame. The random_state=43 parameter sets the seed value to 43, ensuring that the same set of random rows will be displayed each time the code is run.\nBy setting the seed, anyone following along with this code will see the same randomly selected rows as you do, facilitating consistency and reproducibility in a learning or collaborative setting.\nWe all know the drill up to this point. We will round up here by creating a function that we can call anytime that will do all we have done above in one go.\n\ndef tweak_data(url):\n    \"\"\"\n    Reads a CSV file from a given URL, performs data modifications, and returns the updated DataFrame.\n\n    Args:\n        url (str): The URL of the CSV file to be read.\n\n    Returns:\n        pd.DataFrame: The modified DataFrame with a new 'Month' column and converted 'Date' column.\n    \"\"\"\n    # Read the CSV file from the specified URL into a DataFrame\n    data = pd.read_csv(url)\n    \n    # Convert the 'Date' column to datetime format\n    data['Date'] = pd.to_datetime(data['Date'])\n    \n    # Create a new 'Month' column by extracting the month name from the 'Date' column\n    \n    data['Month'] = data['Date'].dt.month_name()\n    \n    # Return the modified DataFrame\n    return data\n\n\ndata = tweak_data(url)\ndata.sample(5, random_state=43)\n\n\n\n\n\n\n\n\nDate\nClose\nMonth\n\n\n\n\n217\n2023-11-01\n3123.753804\nNovember\n\n\n258\n2023-12-28\n3066.558857\nDecember\n\n\n15\n2023-01-23\n3227.238722\nJanuary\n\n\n294\n2024-02-16\n3843.574952\nFebruary\n\n\n17\n2023-01-25\n3063.817256\nJanuary\n\n\n\n\n\n\n\nBasic stuff thus far."
  },
  {
    "objectID": "blog/2024-May-25-Lambda-Functions/lambda_functions.html#chaining",
    "href": "blog/2024-May-25-Lambda-Functions/lambda_functions.html#chaining",
    "title": "Lambda Functions: How I tend to use them",
    "section": "Chaining",
    "text": "Chaining\nLet us now use the Method chaining we have been alluding to right from the begining and see how lambda functions help. We will start from the beginning and build slowly.\n\nurl = 'synthetic_stock_data.csv'\ndata = pd.read_csv(url)\ndata.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 347 entries, 0 to 346\nData columns (total 2 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   Date    347 non-null    object \n 1   Close   347 non-null    float64\ndtypes: float64(1), object(1)\nmemory usage: 5.5+ KB\n\n\nThis is the same step as before. The next steps are what will make the process wonderful. We want to change the Date column to Datetime using the assign method in pandas.\n\n(data\n .assign(Date = pd.to_datetime(data['Date']))\n).info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 347 entries, 0 to 346\nData columns (total 2 columns):\n #   Column  Non-Null Count  Dtype         \n---  ------  --------------  -----         \n 0   Date    347 non-null    datetime64[ns]\n 1   Close   347 non-null    float64       \ndtypes: datetime64[ns](1), float64(1)\nmemory usage: 5.5 KB\n\n\n\nThe .assign() method creates a new DataFrame with the specified modifications without altering the original DataFrame. In this case, it converts the Date column to datetime format and returns a new DataFrame with the updated Date column.\nThe original data DataFrame is not modified because the .assign() method is used within the context of the method chain (data.assign(...)).info(). The modifications are applied to a temporary DataFrame returned by .assign(), and the .info() method is called on that temporary DataFrame.\nAfter the .info() method is called, the temporary DataFrame is discarded, and the original ‚Äòdata‚Äô DataFrame remains unchanged.\n\nTo verify that the data DataFrame is still in its original state, you can execute data.info() separately after running the code snippet. You will observe that the ‚ÄòDate‚Äô column in the ‚Äòdata‚Äô DataFrame has not been converted to datetime format.\nWe are next going to create a new column like we did above. The .assign method allows you to multiple columns to be modified or created with one call. Ideally this would be our code:\n(data\n .assign(Date = pd.to_datetime(data['Date']),\n Month = data['Date'].dt.month_name())\n)\nThis code will give us an error however. This error (AttributeError in this case) occurs because we need the Date column to be in Datetime format which is not the case since the last operation we performed in the chain did not modify the data DataFrame in place, and therefore the output of that operation is not available for this new line to use. Instead, the second assignment still sees the original Date, which is not yet in datetime format.\nLambda Functions to the rescue\nLambda functions can be used to ensure that each assignment within .assign() is evaluated in the correct order. By using lambda functions, you can ensure that the Date column is first converted to datetime before the Month column tries to access it.\n\n(data\n .assign(Date = pd.to_datetime(data['Date']),\n Month = lambda x: x['Date'].dt.month_name())\n).sample(5, random_state=43)\n\n\n\n\n\n\n\n\nDate\nClose\nMonth\n\n\n\n\n217\n2023-11-01\n3123.753804\nNovember\n\n\n258\n2023-12-28\n3066.558857\nDecember\n\n\n15\n2023-01-23\n3227.238722\nJanuary\n\n\n294\n2024-02-16\n3843.574952\nFebruary\n\n\n17\n2023-01-25\n3063.817256\nJanuary\n\n\n\n\n\n\n\nThe word lambda is introduced to create an anonymous function. The variable data is changed to x (or any other placeholder) to represent the DataFrame within the lambda function.\nWe will next create a function that we can call anytime to process a similar dataset.\n\ndef tweak_data():\n    return (data\n             .assign(Date = pd.to_datetime(data['Date']),\n             Month = lambda x: x['Date'].dt.month_name())\n            )\n\n\ndata = tweak_data()\ndata.sample(5, random_state=43)\n\n\n\n\n\n\n\n\nDate\nClose\nMonth\n\n\n\n\n217\n2023-11-01\n3123.753804\nNovember\n\n\n258\n2023-12-28\n3066.558857\nDecember\n\n\n15\n2023-01-23\n3227.238722\nJanuary\n\n\n294\n2024-02-16\n3843.574952\nFebruary\n\n\n17\n2023-01-25\n3063.817256\nJanuary\n\n\n\n\n\n\n\nI want to rewrite this function so it comes out like the first function we created above by reading the CSV in the function itself. In this case since we won‚Äôt have the data DataFrame, we will have to use the lambda function both places.\n\nurl = 'synthetic_stock_data.csv'\n\ndef tweak_data(url):\n    \"\"\"\n    Reads a CSV file from a given URL, performs data modifications, and returns the updated DataFrame.\n\n    Args:\n        url (str): The URL of the CSV file to be read.\n\n    Returns:\n        pd.DataFrame: The modified DataFrame with a new 'Month' column and converted 'Date' column.\n    \"\"\"\n    return (pd.read_csv(url)\n            # Convert the 'Date' column to datetime format\n            .assign(Date=lambda x: pd.to_datetime(x['Date']),\n            # Create a new 'Month' column by extracting the month name from the 'Date' column\n                    Month=lambda x: x['Date'].dt.month_name())\n           )\n\n\ndata = tweak_data(url)\ndata.sample(5, random_state=43)\n\n\n\n\n\n\n\n\nDate\nClose\nMonth\n\n\n\n\n217\n2023-11-01\n3123.753804\nNovember\n\n\n258\n2023-12-28\n3066.558857\nDecember\n\n\n15\n2023-01-23\n3227.238722\nJanuary\n\n\n294\n2024-02-16\n3843.574952\nFebruary\n\n\n17\n2023-01-25\n3063.817256\nJanuary\n\n\n\n\n\n\n\nMethod chaining offers several benefits: it allows for multiple operations in a single, compact statement, making the code concise, and for those familiar with the technique, it enhances readability by presenting a clear, linear sequence of transformations. It promotes a functional programming style, where each method call is a transformation, avoiding the need for intermediate variables and reducing the risk of variable conflicts and cognitive load. Developers who prefer method chaining appreciate its efficiency in expressing multiple operations in one line, providing a streamlined coding experience with consistent and intuitive flow, and less boilerplate code, making the codebase cleaner and easier to maintain.\nHowever, the step-by-step approach may be preferred by beginners or those less familiar with method chaining, as it breaks down each operation into distinct steps that are easier to follow and debug. This approach also provides explicitness, making each line of code clear, which can be beneficial for readability and maintenance, especially in complex transformations. Ultimately, method chaining is favored for its concise, readable, and functional approach, while the step-by-step method is valued for its clarity and ease of understanding, particularly for beginners and debugging purposes. The choice between the two depends on the developer‚Äôs familiarity with the techniques and the specific context of the code."
  },
  {
    "objectID": "blog/2024-May-25-Lambda-Functions/lambda_functions.html#aggregation",
    "href": "blog/2024-May-25-Lambda-Functions/lambda_functions.html#aggregation",
    "title": "Lambda Functions: How I tend to use them",
    "section": "Aggregation",
    "text": "Aggregation\nAggregation is like gathering and summarizing data. Imagine you have a list of your friends‚Äô scores from different games. Instead of looking at all the scores one by one, you can add them up to see the total, find the highest score, the average score, or even count how many games each friend played. Aggregation helps you quickly understand and summarize large amounts of information by looking at key details.\nLets explain further using the .groupby method in the pandas library DataFrames and Series objects. We will use a dataset from the gapminder library. If you don‚Äôt have it you can install it with pip:\npip install gapminder --quiet\nThe --quiet flag is optional. Its primary purpose is to reduce the amount of output shown during installation. The installation will still work correctly without this flag; and you will see more detailed output about the progress and steps being taken by pip.\n\nfrom gapminder import gapminder\n\ndf = gapminder\ndf.sample(5, random_state=43)\n\n\n\n\n\n\n\n\ncountry\ncontinent\nyear\nlifeExp\npop\ngdpPercap\n\n\n\n\n920\nMadagascar\nAfrica\n1992\n52.214\n12210395\n1040.676190\n\n\n1506\nTaiwan\nAsia\n1982\n72.160\n18501390\n7426.354774\n\n\n1361\nSingapore\nAsia\n1977\n70.795\n2325300\n11210.089480\n\n\n1216\nPhilippines\nAsia\n1972\n58.065\n40850141\n1989.374070\n\n\n824\nKenya\nAfrica\n1992\n59.285\n25020539\n1341.921721\n\n\n\n\n\n\n\nThe gapminder dataset from the gapminder library provides data on various countries over time, focusing on key indicators such as life expectancy, population, and GDP per capita. This dataset is useful for analyzing global development trends across different countries and continents. For example, a sample from the dataset includes records from countries like Madagascar, Taiwan, Singapore, the Philippines, and Kenya, showing life expectancy, population, and economic productivity for different years. This data is often utilized for educational purposes, data analysis, and visualizations to understand and compare the progress of nations over time.\nPerforming a groupby operation on the lifeExp (life expectancy) column in the gapminder dataset and then calculating the mean will give you the average life expectancy for each group. Typically, you would group the data by a categorical column such as country, continent, or year. We will do this by country.\n\n(df\n .groupby('country')\n ['lifeExp']\n .mean()\n)\n\ncountry\nAfghanistan           37.478833\nAlbania               68.432917\nAlgeria               59.030167\nAngola                37.883500\nArgentina             69.060417\n                        ...    \nVietnam               57.479500\nWest Bank and Gaza    60.328667\nYemen, Rep.           46.780417\nZambia                45.996333\nZimbabwe              52.663167\nName: lifeExp, Length: 142, dtype: float64\n\n\nGroupby First, we group the data by a specific column (country in our case) This means you put all data from each country Africa together in preparation of the next step.\nAggregation: Next, we summarize each group. In the case above we calculate the average life expectancy for each country.\nBut what if we want to perform a more complex and custom aggregation that is not directly available as built-in methods? Enter the .aggregate method (or the .agg method. They are essentially the same!). We will use the lambda function to create a function that will show only the 25th percentile of the gdpPercap column for each country.\n\nimport numpy as np\n# Group by country and calculate the 25th percentile of GDP per capita\n(df\n .groupby('country')\n ['gdpPercap']\n .agg(lambda x: np.percentile(x, 25))\n)\n\ncountry\nAfghanistan            736.669343\nAlbania               2451.300665\nAlgeria               3188.737834\nAngola                2724.676675\nArgentina             7823.006272\n                         ...     \nVietnam                693.697595\nWest Bank and Gaza    2537.025333\nYemen, Rep.            853.237410\nZambia                1195.010682\nZimbabwe               525.145203\nName: gdpPercap, Length: 142, dtype: float64"
  },
  {
    "objectID": "blog/2024-May-25-Lambda-Functions/lambda_functions.html#conclusion",
    "href": "blog/2024-May-25-Lambda-Functions/lambda_functions.html#conclusion",
    "title": "Lambda Functions: How I tend to use them",
    "section": "Conclusion",
    "text": "Conclusion\nLambda functions, or anonymous functions, are a powerful feature in Python that allow for creating small, unnamed functions at runtime. Their concise syntax makes them ideal for scenarios where a simple function is needed temporarily. Beyond their use in data aggregation with pandas, lambda functions are widely utilized in various contexts to enhance code efficiency and readability.\nThey are frequently used in sorting operations to define custom sort keys, in filtering sequences to specify conditions, and in mapping functions to apply transformations to elements. Lambda functions also shine in reduction operations to cumulatively apply functions to sequences, and in functional programming for composing and combining functions on the fly. Additionally, they are valuable in defining inline callbacks in event-driven programming, like GUI development.\nIf you haven‚Äôt used lambda functions before, this could be an excellent opportunity to explore and incorporate them into your programming practice. Their flexibility and brevity can streamline your code and open up new possibilities for concise and efficient function definitions\nLet me know in the comments below what you think about them and if you have been using them or intend to use them in future."
  },
  {
    "objectID": "resume/index.html",
    "href": "resume/index.html",
    "title": "R√âSUM√â",
    "section": "",
    "text": "Download my current R√©sum√©"
  }
]